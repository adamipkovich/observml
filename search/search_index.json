{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to ObservML Documentation ObservML is a comprehensive machine learning framework designed for process monitoring, anomaly detection, fault isolation, and time series analysis. Built with a plugin-based architecture, it provides easy-to-use microservices with integrated MLOps, tracking, and deployment capabilities. What is ObservML? ObservML (Observable Machine Learning) is a modular framework that focuses on industrial process monitoring and anomaly detection. The project evolved from MMLW (Modular Machine Learning Workflows) to provide a more robust, scalable, and production-ready solution for machine learning in industrial environments. Key Features ExperimentHub Architecture : Central management system for multiple experiments Plugin System : Extensible architecture with MLOps, DataStream, and custom plugins Configuration-Driven : YAML-based configuration for easy setup and deployment REST API : FastAPI-based API with automatic documentation Real-time Processing : RabbitMQ integration for streaming data and predictions MLOps Integration : Built-in MLflow support for experiment tracking and model registry Prerequisites Before getting started with ObservML, ensure you have the following: System Requirements Computer : At least 16GB RAM (32GB+ recommended for production), 32-64GB free disk space Operating System : Linux, macOS, or Windows with WSL2 Python : Version 3.11 or higher Docker : Docker Engine and Docker Compose (for containerized deployment) Git : For cloning the repository Development Tools (Optional) IDE : VSCode, PyCharm, or any preferred IDE Poetry : For dependency management (recommended) Make : For using Makefile commands (optional) Quick Installation 1. Clone the Repository git clone https://github.com/adamipkovich/observml.git cd observml 2. Choose Installation Method Option A: Poetry (Recommended) # Install Poetry if not already installed curl -sSL https://install.python-poetry.org | python3 - # Install dependencies poetry install # Activate virtual environment poetry shell Option B: pip # Create virtual environment python -m venv venv source venv/bin/activate # On Windows: venv\\Scripts\\activate # Install dependencies pip install -r requirements.txt 3. Start Infrastructure Services # Start MLflow and RabbitMQ using Docker docker-compose up -d mlflow rabbitmq 4. Configure ObservML # Copy example configuration cp hub_config.yaml hub_config.yaml # Edit configuration as needed nano hub_config.yaml 5. Start the API Server python ExperimentHubAPI.py The API will be available at: - API Server : http://localhost:8010 - API Documentation : http://localhost:8010/docs - MLflow UI : http://localhost:5000 - RabbitMQ Management : http://localhost:15672 (guest/guest) Architecture Overview ObservML is built around several core components: ExperimentHub The central component that manages all experiments, plugins, and configurations. It provides: - Experiment lifecycle management (create, train, predict, save, load) - Plugin coordination and health monitoring - Configuration management - API endpoint handling Plugin System ObservML uses a plugin architecture for extensibility: MLOps Plugin : Handles experiment tracking and model registry (MLflow) DataStream Plugin : Manages real-time data streaming (RabbitMQ) Custom Plugins : Extensible system for adding new functionality Experiment Types Modular experiment implementations for different use cases: - Time Series Analysis : Forecasting and anomaly detection in time series data - Fault Detection : Anomaly detection in sensor data - Fault Isolation : Classification and root cause analysis - Process Mining : Business process analysis and optimization Basic Workflow ObservML follows a simple workflow for machine learning experiments: graph TD A[Configure System] --> B[Create Experiment] B --> C[Send Data via RabbitMQ] C --> D[Train Model] D --> E[Save to MLflow] E --> F[Make Predictions] F --> G[Visualize Results] G --> H[Monitor Performance] H --> I{Retrain Needed?} I -->|Yes| D I -->|No| F Step-by-Step Process Configuration : Set up plugins and experiment types in hub_config.yaml Data Preparation : Send training data through RabbitMQ queues Experiment Creation : Create and configure experiments via API Training : Train models with automatic MLflow tracking Prediction : Make real-time predictions on streaming data Monitoring : Track model performance and trigger retraining when needed Communication Architecture ObservML uses a microservices architecture with message-based communication: graph LR A[Data Source] --> B[RabbitMQ] B --> C[ExperimentHub] C --> D[MLflow] C --> E[Model Storage] F[API Client] --> G[FastAPI] G --> C C --> H[Predictions] H --> I[Visualization] Key Benefits Decoupled Architecture : Services can be scaled independently Asynchronous Processing : Non-blocking operations for better performance Fault Tolerance : Message queues provide reliability and retry mechanisms Scalability : Easy to add more workers or services as needed Getting Help Documentation Structure This documentation is organized into several sections: Local Development : Setting up a development environment Deployment : Production deployment with Docker Client Usage : Using the API and client libraries API Reference : Complete API documentation REST API Reference : Comprehensive REST API guide with framework architecture Configuration : Configuration options and examples Plugin System : Extending ObservML with plugins Models : Available models and algorithms Experiments : Experiment types and configurations Support Channels GitHub Issues : Report bugs and request features GitHub Discussions : Ask questions and share ideas Documentation : Comprehensive guides and API reference Examples : Sample configurations and use cases Contributing ObservML is an open-source project and welcomes contributions: Fork the repository Create a feature branch Make your changes Add tests for new functionality Submit a pull request Next Steps Now that you have ObservML installed, you can: Set up local development for experimentation Deploy to production using Docker Learn the API for programmatic access Explore examples to understand common use cases Configure experiments for your specific needs Project History ObservML evolved from the MMLW (Modular Machine Learning Workflows) project, which was part of research project 2020-1.1.2-PIACI-KFI-2020-00062. The project has been redesigned with: Modern plugin architecture Improved scalability and performance Better separation of concerns Enhanced configuration management Production-ready deployment options The focus remains on providing accessible machine learning tools for industrial process monitoring and anomaly detection, but with a more robust and extensible foundation.","title":"Home"},{"location":"#welcome-to-observml-documentation","text":"ObservML is a comprehensive machine learning framework designed for process monitoring, anomaly detection, fault isolation, and time series analysis. Built with a plugin-based architecture, it provides easy-to-use microservices with integrated MLOps, tracking, and deployment capabilities.","title":"Welcome to ObservML Documentation"},{"location":"#what-is-observml","text":"ObservML (Observable Machine Learning) is a modular framework that focuses on industrial process monitoring and anomaly detection. The project evolved from MMLW (Modular Machine Learning Workflows) to provide a more robust, scalable, and production-ready solution for machine learning in industrial environments.","title":"What is ObservML?"},{"location":"#key-features","text":"ExperimentHub Architecture : Central management system for multiple experiments Plugin System : Extensible architecture with MLOps, DataStream, and custom plugins Configuration-Driven : YAML-based configuration for easy setup and deployment REST API : FastAPI-based API with automatic documentation Real-time Processing : RabbitMQ integration for streaming data and predictions MLOps Integration : Built-in MLflow support for experiment tracking and model registry","title":"Key Features"},{"location":"#prerequisites","text":"Before getting started with ObservML, ensure you have the following:","title":"Prerequisites"},{"location":"#system-requirements","text":"Computer : At least 16GB RAM (32GB+ recommended for production), 32-64GB free disk space Operating System : Linux, macOS, or Windows with WSL2 Python : Version 3.11 or higher Docker : Docker Engine and Docker Compose (for containerized deployment) Git : For cloning the repository","title":"System Requirements"},{"location":"#development-tools-optional","text":"IDE : VSCode, PyCharm, or any preferred IDE Poetry : For dependency management (recommended) Make : For using Makefile commands (optional)","title":"Development Tools (Optional)"},{"location":"#quick-installation","text":"","title":"Quick Installation"},{"location":"#1-clone-the-repository","text":"git clone https://github.com/adamipkovich/observml.git cd observml","title":"1. Clone the Repository"},{"location":"#2-choose-installation-method","text":"","title":"2. Choose Installation Method"},{"location":"#option-a-poetry-recommended","text":"# Install Poetry if not already installed curl -sSL https://install.python-poetry.org | python3 - # Install dependencies poetry install # Activate virtual environment poetry shell","title":"Option A: Poetry (Recommended)"},{"location":"#option-b-pip","text":"# Create virtual environment python -m venv venv source venv/bin/activate # On Windows: venv\\Scripts\\activate # Install dependencies pip install -r requirements.txt","title":"Option B: pip"},{"location":"#3-start-infrastructure-services","text":"# Start MLflow and RabbitMQ using Docker docker-compose up -d mlflow rabbitmq","title":"3. Start Infrastructure Services"},{"location":"#4-configure-observml","text":"# Copy example configuration cp hub_config.yaml hub_config.yaml # Edit configuration as needed nano hub_config.yaml","title":"4. Configure ObservML"},{"location":"#5-start-the-api-server","text":"python ExperimentHubAPI.py The API will be available at: - API Server : http://localhost:8010 - API Documentation : http://localhost:8010/docs - MLflow UI : http://localhost:5000 - RabbitMQ Management : http://localhost:15672 (guest/guest)","title":"5. Start the API Server"},{"location":"#architecture-overview","text":"ObservML is built around several core components:","title":"Architecture Overview"},{"location":"#experimenthub","text":"The central component that manages all experiments, plugins, and configurations. It provides: - Experiment lifecycle management (create, train, predict, save, load) - Plugin coordination and health monitoring - Configuration management - API endpoint handling","title":"ExperimentHub"},{"location":"#plugin-system","text":"ObservML uses a plugin architecture for extensibility: MLOps Plugin : Handles experiment tracking and model registry (MLflow) DataStream Plugin : Manages real-time data streaming (RabbitMQ) Custom Plugins : Extensible system for adding new functionality","title":"Plugin System"},{"location":"#experiment-types","text":"Modular experiment implementations for different use cases: - Time Series Analysis : Forecasting and anomaly detection in time series data - Fault Detection : Anomaly detection in sensor data - Fault Isolation : Classification and root cause analysis - Process Mining : Business process analysis and optimization","title":"Experiment Types"},{"location":"#basic-workflow","text":"ObservML follows a simple workflow for machine learning experiments: graph TD A[Configure System] --> B[Create Experiment] B --> C[Send Data via RabbitMQ] C --> D[Train Model] D --> E[Save to MLflow] E --> F[Make Predictions] F --> G[Visualize Results] G --> H[Monitor Performance] H --> I{Retrain Needed?} I -->|Yes| D I -->|No| F","title":"Basic Workflow"},{"location":"#step-by-step-process","text":"Configuration : Set up plugins and experiment types in hub_config.yaml Data Preparation : Send training data through RabbitMQ queues Experiment Creation : Create and configure experiments via API Training : Train models with automatic MLflow tracking Prediction : Make real-time predictions on streaming data Monitoring : Track model performance and trigger retraining when needed","title":"Step-by-Step Process"},{"location":"#communication-architecture","text":"ObservML uses a microservices architecture with message-based communication: graph LR A[Data Source] --> B[RabbitMQ] B --> C[ExperimentHub] C --> D[MLflow] C --> E[Model Storage] F[API Client] --> G[FastAPI] G --> C C --> H[Predictions] H --> I[Visualization]","title":"Communication Architecture"},{"location":"#key-benefits","text":"Decoupled Architecture : Services can be scaled independently Asynchronous Processing : Non-blocking operations for better performance Fault Tolerance : Message queues provide reliability and retry mechanisms Scalability : Easy to add more workers or services as needed","title":"Key Benefits"},{"location":"#getting-help","text":"","title":"Getting Help"},{"location":"#documentation-structure","text":"This documentation is organized into several sections: Local Development : Setting up a development environment Deployment : Production deployment with Docker Client Usage : Using the API and client libraries API Reference : Complete API documentation REST API Reference : Comprehensive REST API guide with framework architecture Configuration : Configuration options and examples Plugin System : Extending ObservML with plugins Models : Available models and algorithms Experiments : Experiment types and configurations","title":"Documentation Structure"},{"location":"#support-channels","text":"GitHub Issues : Report bugs and request features GitHub Discussions : Ask questions and share ideas Documentation : Comprehensive guides and API reference Examples : Sample configurations and use cases","title":"Support Channels"},{"location":"#contributing","text":"ObservML is an open-source project and welcomes contributions: Fork the repository Create a feature branch Make your changes Add tests for new functionality Submit a pull request","title":"Contributing"},{"location":"#next-steps","text":"Now that you have ObservML installed, you can: Set up local development for experimentation Deploy to production using Docker Learn the API for programmatic access Explore examples to understand common use cases Configure experiments for your specific needs","title":"Next Steps"},{"location":"#project-history","text":"ObservML evolved from the MMLW (Modular Machine Learning Workflows) project, which was part of research project 2020-1.1.2-PIACI-KFI-2020-00062. The project has been redesigned with: Modern plugin architecture Improved scalability and performance Better separation of concerns Enhanced configuration management Production-ready deployment options The focus remains on providing accessible machine learning tools for industrial process monitoring and anomaly detection, but with a more robust and extensible foundation.","title":"Project History"},{"location":"api/","text":"ExperimentHub API The ExperimentHub API provides a RESTful interface for interacting with the ExperimentHub. This document explains the available endpoints, their parameters, and example usage. API Overview The ExperimentHub API is built using FastAPI and provides endpoints for: Health Checks : Check the health of all plugins Experiment Management : Create, load, save, and kill experiments Training and Prediction : Train models and make predictions Visualization : Get plots and figures from experiments Base URL The API is typically available at: http://localhost:8010 Authentication The API currently does not implement authentication. If you need to secure the API, consider using an API gateway or implementing authentication middleware. Endpoints Health and Status GET /health Check the health of all plugins. Response : { \"mlops\": { \"healthy\": true, \"details\": { \"status\": \"connected\", \"uri\": \"http://localhost:5000\" } }, \"datastream\": { \"healthy\": true, \"details\": { \"status\": \"connected\", \"host\": \"localhost\", \"port\": 5672 } } } GET /available_experiments Get available experiment types. Response : { \"time_series\": { \"module\": \"framework.TimeSeriesAnalysis\", \"class\": \"TimeSeriesExperiment\" }, \"fault_detection\": { \"module\": \"framework.FaultDetection\", \"class\": \"FaultDetectionExperiment\" } } Experiment Management POST /create_experiment/{name}/{experiment_type} Create a new experiment of the specified type. Path Parameters : name : Name of the experiment experiment_type : Type of experiment (must be registered) Request Body : Configuration for the experiment (JSON). Example : curl -X POST \"http://localhost:8010/create_experiment/my_experiment/time_series\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"setup\": { \"target\": \"value\", \"ds\": \"ds\" }, \"create_model\": { \"model_type\": \"Prophet\", \"params\": { \"seasonality_mode\": \"multiplicative\", \"daily_seasonality\": true } } }' Response : { \"message\": \"Creating experiment 'my_experiment' of type 'time_series'\" } POST /{name}/train Train an experiment. Path Parameters : name : Name of the experiment Request Body : Configuration for the experiment (JSON). Example : curl -X POST \"http://localhost:8010/my_experiment/train\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"setup\": { \"target\": \"value\", \"ds\": \"ds\" }, \"create_model\": { \"model_type\": \"Prophet\", \"params\": { \"seasonality_mode\": \"multiplicative\", \"daily_seasonality\": true } } }' Response : { \"message\": \"Training started based on sent parameters.\" } POST /{name}/load Load an experiment. Path Parameters : name : Name of the experiment Example : curl -X POST \"http://localhost:8010/my_experiment/load\" Response : Experiment loaded POST /{name}/load/{run_id} Load an experiment with a specific run ID. Path Parameters : name : Name of the experiment run_id : MLflow run ID Example : curl -X POST \"http://localhost:8010/my_experiment/load/abcdef123456\" Response : Experiment loaded POST /{name}/save Save an experiment. Path Parameters : name : Name of the experiment Example : curl -X POST \"http://localhost:8010/my_experiment/save\" Response : Experiment saved POST /{name}/stop_training Stop the training process for an experiment without removing it. Path Parameters : name : Name of the experiment Example : curl -X POST \"http://localhost:8010/my_experiment/stop_training\" Response : { \"status\": \"success\", \"message\": \"Training for experiment my_experiment stopped successfully\" } POST /{name}/delete Remove an experiment from memory. Path Parameters : name : Name of the experiment Example : curl -X POST \"http://localhost:8010/my_experiment/delete\" Response : { \"status\": \"success\", \"message\": \"Experiment my_experiment deleted successfully\" } POST /{name}/kill [DEPRECATED] Legacy endpoint. Use /stop_training instead. Path Parameters : name : Name of the experiment Example : curl -X POST \"http://localhost:8010/my_experiment/kill\" Response : { \"status\": \"success\", \"message\": \"Experiment my_experiment training stopped successfully\" } Training and Prediction POST /{name}/predict Make predictions with an experiment. Path Parameters : name : Name of the experiment Example : curl -X POST \"http://localhost:8010/my_experiment/predict\" Response : Prediction called POST /{name}/retrain Retrain an experiment. Path Parameters : name : Name of the experiment Example : curl -X POST \"http://localhost:8010/my_experiment/retrain\" Response : Retraining started as requested. Visualization GET /{name}/plot/{plot_name} Get a plot from an experiment. Path Parameters : name : Name of the experiment plot_name : Name of the plot Example : curl -X GET \"http://localhost:8010/my_experiment/plot/forecast\" Response : Plotly JSON figure. GET /{name}/plot_eda/{plot_name} Get an EDA figure from an experiment. Path Parameters : name : Name of the experiment plot_name : Name of the figure Example : curl -X GET \"http://localhost:8010/my_experiment/plot_eda/correlation\" Response : Plotly JSON figure. Data Management POST /flush/{queue} Flush a queue. Path Parameters : queue : Name of the queue Example : curl -X POST \"http://localhost:8010/flush/my_experiment\" Response : Rabbit flushed GET /{name}/train_data Get training data from an experiment. Path Parameters : name : Name of the experiment Example : curl -X GET \"http://localhost:8010/my_experiment/train_data\" Response : JSON data. GET /{name}/cfg Get configuration of an experiment. Path Parameters : name : Name of the experiment Example : curl -X GET \"http://localhost:8010/my_experiment/cfg\" Response : JSON configuration. Metadata GET /experiments Get experiment names and figure names. Example : curl -X GET \"http://localhost:8010/experiments\" Response : { \"my_experiment\": [ [\"forecast\", \"components\"], [\"correlation\", \"histogram\"] ] } GET /{name}/run_id Get run ID of an experiment. Path Parameters : name : Name of the experiment Example : curl -X GET \"http://localhost:8010/my_experiment/run_id\" Response : abcdef123456 GET /{name}/exp_id Get experiment ID of an experiment. Path Parameters : name : Name of the experiment Example : curl -X GET \"http://localhost:8010/my_experiment/exp_id\" Response : 123 Error Handling The API returns appropriate HTTP status codes for different error conditions: 200 OK : The request was successful 204 No Content : The request was successful, but there is no content to return 400 Bad Request : The request was invalid 404 Not Found : The requested resource was not found 500 Internal Server Error : An error occurred on the server Error responses include a detail message explaining the error: { \"detail\": \"Unknown experiment type: invalid_type\" } API Client Here's an example of a simple Python client for the ExperimentHub API: import requests import json class ExperimentHubClient: def __init__(self, base_url=\"http://localhost:8010\"): self.base_url = base_url def check_health(self): \"\"\"Check the health of all plugins\"\"\" response = requests.get(f\"{self.base_url}/health\") return response.json() def get_available_experiments(self): \"\"\"Get available experiment types\"\"\" response = requests.get(f\"{self.base_url}/available_experiments\") return response.json() def create_experiment(self, name, experiment_type, config): \"\"\"Create a new experiment\"\"\" response = requests.post( f\"{self.base_url}/create_experiment/{name}/{experiment_type}\", json=config ) return response.json() def train(self, name, config): \"\"\"Train an experiment\"\"\" response = requests.post( f\"{self.base_url}/{name}/train\", json=config ) return response.text def predict(self, name): \"\"\"Make predictions with an experiment\"\"\" response = requests.post(f\"{self.base_url}/{name}/predict\") return response.text def load(self, name, run_id=None): \"\"\"Load an experiment\"\"\" if run_id: response = requests.post(f\"{self.base_url}/{name}/load/{run_id}\") else: response = requests.post(f\"{self.base_url}/{name}/load\") return response.text def save(self, name): \"\"\"Save an experiment\"\"\" response = requests.post(f\"{self.base_url}/{name}/save\") return response.text def get_plot(self, name, plot_name): \"\"\"Get a plot from an experiment\"\"\" response = requests.get(f\"{self.base_url}/{name}/plot/{plot_name}\") return response.json() def get_eda_plot(self, name, plot_name): \"\"\"Get an EDA figure from an experiment\"\"\" response = requests.get(f\"{self.base_url}/{name}/plot_eda/{plot_name}\") return response.json() def get_experiments(self): \"\"\"Get experiment names and figure names\"\"\" response = requests.get(f\"{self.base_url}/experiments\") return response.json() Example Usage # Create client client = ExperimentHubClient() # Check health health = client.check_health() print(f\"Health: {json.dumps(health, indent=2)}\") # Get available experiments experiments = client.get_available_experiments() print(f\"Available experiments: {json.dumps(experiments, indent=2)}\") # Create and train experiment config = { \"setup\": { \"target\": \"value\", \"ds\": \"ds\" }, \"create_model\": { \"model_type\": \"Prophet\", \"params\": { \"seasonality_mode\": \"multiplicative\", \"daily_seasonality\": True } } } result = client.create_experiment(\"my_experiment\", \"time_series\", config) print(f\"Create result: {result}\") # Make predictions prediction = client.predict(\"my_experiment\") print(f\"Prediction result: {prediction}\") # Get plots plot = client.get_plot(\"my_experiment\", \"forecast\") print(f\"Plot: {plot}\") Troubleshooting Connection Refused If you get a \"Connection refused\" error, make sure the API server is running and accessible at the specified URL. Experiment Not Found If you get a \"Experiment not found\" error, make sure the experiment exists and is loaded. Plugin Health Issues If the health check shows that a plugin is unhealthy, check the plugin configuration and make sure the required services (MLflow, RabbitMQ, etc.) are running.","title":"API Reference"},{"location":"api/#experimenthub-api","text":"The ExperimentHub API provides a RESTful interface for interacting with the ExperimentHub. This document explains the available endpoints, their parameters, and example usage.","title":"ExperimentHub API"},{"location":"api/#api-overview","text":"The ExperimentHub API is built using FastAPI and provides endpoints for: Health Checks : Check the health of all plugins Experiment Management : Create, load, save, and kill experiments Training and Prediction : Train models and make predictions Visualization : Get plots and figures from experiments","title":"API Overview"},{"location":"api/#base-url","text":"The API is typically available at: http://localhost:8010","title":"Base URL"},{"location":"api/#authentication","text":"The API currently does not implement authentication. If you need to secure the API, consider using an API gateway or implementing authentication middleware.","title":"Authentication"},{"location":"api/#endpoints","text":"","title":"Endpoints"},{"location":"api/#health-and-status","text":"","title":"Health and Status"},{"location":"api/#get-health","text":"Check the health of all plugins. Response : { \"mlops\": { \"healthy\": true, \"details\": { \"status\": \"connected\", \"uri\": \"http://localhost:5000\" } }, \"datastream\": { \"healthy\": true, \"details\": { \"status\": \"connected\", \"host\": \"localhost\", \"port\": 5672 } } }","title":"GET /health"},{"location":"api/#get-available_experiments","text":"Get available experiment types. Response : { \"time_series\": { \"module\": \"framework.TimeSeriesAnalysis\", \"class\": \"TimeSeriesExperiment\" }, \"fault_detection\": { \"module\": \"framework.FaultDetection\", \"class\": \"FaultDetectionExperiment\" } }","title":"GET /available_experiments"},{"location":"api/#experiment-management","text":"","title":"Experiment Management"},{"location":"api/#post-create_experimentnameexperiment_type","text":"Create a new experiment of the specified type. Path Parameters : name : Name of the experiment experiment_type : Type of experiment (must be registered) Request Body : Configuration for the experiment (JSON). Example : curl -X POST \"http://localhost:8010/create_experiment/my_experiment/time_series\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"setup\": { \"target\": \"value\", \"ds\": \"ds\" }, \"create_model\": { \"model_type\": \"Prophet\", \"params\": { \"seasonality_mode\": \"multiplicative\", \"daily_seasonality\": true } } }' Response : { \"message\": \"Creating experiment 'my_experiment' of type 'time_series'\" }","title":"POST /create_experiment/{name}/{experiment_type}"},{"location":"api/#post-nametrain","text":"Train an experiment. Path Parameters : name : Name of the experiment Request Body : Configuration for the experiment (JSON). Example : curl -X POST \"http://localhost:8010/my_experiment/train\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"setup\": { \"target\": \"value\", \"ds\": \"ds\" }, \"create_model\": { \"model_type\": \"Prophet\", \"params\": { \"seasonality_mode\": \"multiplicative\", \"daily_seasonality\": true } } }' Response : { \"message\": \"Training started based on sent parameters.\" }","title":"POST /{name}/train"},{"location":"api/#post-nameload","text":"Load an experiment. Path Parameters : name : Name of the experiment Example : curl -X POST \"http://localhost:8010/my_experiment/load\" Response : Experiment loaded","title":"POST /{name}/load"},{"location":"api/#post-nameloadrun_id","text":"Load an experiment with a specific run ID. Path Parameters : name : Name of the experiment run_id : MLflow run ID Example : curl -X POST \"http://localhost:8010/my_experiment/load/abcdef123456\" Response : Experiment loaded","title":"POST /{name}/load/{run_id}"},{"location":"api/#post-namesave","text":"Save an experiment. Path Parameters : name : Name of the experiment Example : curl -X POST \"http://localhost:8010/my_experiment/save\" Response : Experiment saved","title":"POST /{name}/save"},{"location":"api/#post-namestop_training","text":"Stop the training process for an experiment without removing it. Path Parameters : name : Name of the experiment Example : curl -X POST \"http://localhost:8010/my_experiment/stop_training\" Response : { \"status\": \"success\", \"message\": \"Training for experiment my_experiment stopped successfully\" }","title":"POST /{name}/stop_training"},{"location":"api/#post-namedelete","text":"Remove an experiment from memory. Path Parameters : name : Name of the experiment Example : curl -X POST \"http://localhost:8010/my_experiment/delete\" Response : { \"status\": \"success\", \"message\": \"Experiment my_experiment deleted successfully\" }","title":"POST /{name}/delete"},{"location":"api/#post-namekill","text":"[DEPRECATED] Legacy endpoint. Use /stop_training instead. Path Parameters : name : Name of the experiment Example : curl -X POST \"http://localhost:8010/my_experiment/kill\" Response : { \"status\": \"success\", \"message\": \"Experiment my_experiment training stopped successfully\" }","title":"POST /{name}/kill"},{"location":"api/#training-and-prediction","text":"","title":"Training and Prediction"},{"location":"api/#post-namepredict","text":"Make predictions with an experiment. Path Parameters : name : Name of the experiment Example : curl -X POST \"http://localhost:8010/my_experiment/predict\" Response : Prediction called","title":"POST /{name}/predict"},{"location":"api/#post-nameretrain","text":"Retrain an experiment. Path Parameters : name : Name of the experiment Example : curl -X POST \"http://localhost:8010/my_experiment/retrain\" Response : Retraining started as requested.","title":"POST /{name}/retrain"},{"location":"api/#visualization","text":"","title":"Visualization"},{"location":"api/#get-nameplotplot_name","text":"Get a plot from an experiment. Path Parameters : name : Name of the experiment plot_name : Name of the plot Example : curl -X GET \"http://localhost:8010/my_experiment/plot/forecast\" Response : Plotly JSON figure.","title":"GET /{name}/plot/{plot_name}"},{"location":"api/#get-nameplot_edaplot_name","text":"Get an EDA figure from an experiment. Path Parameters : name : Name of the experiment plot_name : Name of the figure Example : curl -X GET \"http://localhost:8010/my_experiment/plot_eda/correlation\" Response : Plotly JSON figure.","title":"GET /{name}/plot_eda/{plot_name}"},{"location":"api/#data-management","text":"","title":"Data Management"},{"location":"api/#post-flushqueue","text":"Flush a queue. Path Parameters : queue : Name of the queue Example : curl -X POST \"http://localhost:8010/flush/my_experiment\" Response : Rabbit flushed","title":"POST /flush/{queue}"},{"location":"api/#get-nametrain_data","text":"Get training data from an experiment. Path Parameters : name : Name of the experiment Example : curl -X GET \"http://localhost:8010/my_experiment/train_data\" Response : JSON data.","title":"GET /{name}/train_data"},{"location":"api/#get-namecfg","text":"Get configuration of an experiment. Path Parameters : name : Name of the experiment Example : curl -X GET \"http://localhost:8010/my_experiment/cfg\" Response : JSON configuration.","title":"GET /{name}/cfg"},{"location":"api/#metadata","text":"","title":"Metadata"},{"location":"api/#get-experiments","text":"Get experiment names and figure names. Example : curl -X GET \"http://localhost:8010/experiments\" Response : { \"my_experiment\": [ [\"forecast\", \"components\"], [\"correlation\", \"histogram\"] ] }","title":"GET /experiments"},{"location":"api/#get-namerun_id","text":"Get run ID of an experiment. Path Parameters : name : Name of the experiment Example : curl -X GET \"http://localhost:8010/my_experiment/run_id\" Response : abcdef123456","title":"GET /{name}/run_id"},{"location":"api/#get-nameexp_id","text":"Get experiment ID of an experiment. Path Parameters : name : Name of the experiment Example : curl -X GET \"http://localhost:8010/my_experiment/exp_id\" Response : 123","title":"GET /{name}/exp_id"},{"location":"api/#error-handling","text":"The API returns appropriate HTTP status codes for different error conditions: 200 OK : The request was successful 204 No Content : The request was successful, but there is no content to return 400 Bad Request : The request was invalid 404 Not Found : The requested resource was not found 500 Internal Server Error : An error occurred on the server Error responses include a detail message explaining the error: { \"detail\": \"Unknown experiment type: invalid_type\" }","title":"Error Handling"},{"location":"api/#api-client","text":"Here's an example of a simple Python client for the ExperimentHub API: import requests import json class ExperimentHubClient: def __init__(self, base_url=\"http://localhost:8010\"): self.base_url = base_url def check_health(self): \"\"\"Check the health of all plugins\"\"\" response = requests.get(f\"{self.base_url}/health\") return response.json() def get_available_experiments(self): \"\"\"Get available experiment types\"\"\" response = requests.get(f\"{self.base_url}/available_experiments\") return response.json() def create_experiment(self, name, experiment_type, config): \"\"\"Create a new experiment\"\"\" response = requests.post( f\"{self.base_url}/create_experiment/{name}/{experiment_type}\", json=config ) return response.json() def train(self, name, config): \"\"\"Train an experiment\"\"\" response = requests.post( f\"{self.base_url}/{name}/train\", json=config ) return response.text def predict(self, name): \"\"\"Make predictions with an experiment\"\"\" response = requests.post(f\"{self.base_url}/{name}/predict\") return response.text def load(self, name, run_id=None): \"\"\"Load an experiment\"\"\" if run_id: response = requests.post(f\"{self.base_url}/{name}/load/{run_id}\") else: response = requests.post(f\"{self.base_url}/{name}/load\") return response.text def save(self, name): \"\"\"Save an experiment\"\"\" response = requests.post(f\"{self.base_url}/{name}/save\") return response.text def get_plot(self, name, plot_name): \"\"\"Get a plot from an experiment\"\"\" response = requests.get(f\"{self.base_url}/{name}/plot/{plot_name}\") return response.json() def get_eda_plot(self, name, plot_name): \"\"\"Get an EDA figure from an experiment\"\"\" response = requests.get(f\"{self.base_url}/{name}/plot_eda/{plot_name}\") return response.json() def get_experiments(self): \"\"\"Get experiment names and figure names\"\"\" response = requests.get(f\"{self.base_url}/experiments\") return response.json()","title":"API Client"},{"location":"api/#example-usage","text":"# Create client client = ExperimentHubClient() # Check health health = client.check_health() print(f\"Health: {json.dumps(health, indent=2)}\") # Get available experiments experiments = client.get_available_experiments() print(f\"Available experiments: {json.dumps(experiments, indent=2)}\") # Create and train experiment config = { \"setup\": { \"target\": \"value\", \"ds\": \"ds\" }, \"create_model\": { \"model_type\": \"Prophet\", \"params\": { \"seasonality_mode\": \"multiplicative\", \"daily_seasonality\": True } } } result = client.create_experiment(\"my_experiment\", \"time_series\", config) print(f\"Create result: {result}\") # Make predictions prediction = client.predict(\"my_experiment\") print(f\"Prediction result: {prediction}\") # Get plots plot = client.get_plot(\"my_experiment\", \"forecast\") print(f\"Plot: {plot}\")","title":"Example Usage"},{"location":"api/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"api/#connection-refused","text":"If you get a \"Connection refused\" error, make sure the API server is running and accessible at the specified URL.","title":"Connection Refused"},{"location":"api/#experiment-not-found","text":"If you get a \"Experiment not found\" error, make sure the experiment exists and is loaded.","title":"Experiment Not Found"},{"location":"api/#plugin-health-issues","text":"If the health check shows that a plugin is unhealthy, check the plugin configuration and make sure the required services (MLflow, RabbitMQ, etc.) are running.","title":"Plugin Health Issues"},{"location":"client/","text":"Client-side Use This section explains how to communicate with a running ObServML application, which includes a short description on how to compile instructions for training sessions, then how predict and loading/saving functions are used. Since the developer environment is not required, first a short client-side setup is included. Client-side environment setup. First download Python and an IDE (We recommend VSC or Pycharm). Pull from repo https://github.com/adamipkovich/observml.git, branch \"main\". This repo is currently public and available for cloning. Install packages if only the client-side code is used: pip install -r mindeps.txt Example project In the \"main\" branch, you can find the files relevant to client-side use: Hydra template config directory -- configs folder folders that represent an Experiment-ID and a rabbit-queue -> these folders include .yaml files that are experiment specifications. __init__.py -- a must have or hydra cannot build the configuration file config.yaml -- this yaml file contains what experiments should be trained with which configuration (adding here key/value pairs will train an experiment with the name ${key}, with model configuration located in ${value} - which are .yaml files. Removing a key/value pair means that it will not train that experiment again. ) Template Experiment specifications are provided under the Models section. api_commands.py - precompiled set of commands. A must have. data (e.g. ./data/) - testing data to be posted on rabbit. Each is stored in .xlsx format, but the system can only receive .json formatted data! NOTE: Recent developments contain a formatter for json files. data.yaml - defines local paths to data, if not passed as objects. (used by train_stript_local.py) example files: train_script_local.py, train_script_obj.py, test_script.py Data folder and .yaml is not necessary, data can be passed as an object as well. We provide 2 examples for training, one of which automatically reads in data from the project, defined in the .yaml file. The other reads the .xlsx-s in to pass it as an object in memory (as an example for memory-based handling of data). docker-compose.yaml which will be the heartbeat of the application. An example for project structure looks as follows: data example.data.xlsx configs rabbit_queue_name_1 ml_model_template.yaml rabbit_queue_name_2 ml_model_template.yaml __init__.py config.yaml api_commands.py data.yaml docker-compose.yaml NOTE: In this form, the project is only capable of sending instructions. The hydra structure has a main configuration file, which is called config.yaml. Here, keywords represent the folder names, and the values the .yaml file that should be used for the key (hydra dynamically builds the configuration files, but one keyword can only have one .yaml injected). Here it would look like: defaults: rabbit_queue_name_1 : experiment_template.yaml rabbit_queue_name_2 : experiment_template.yaml .yaml files will be appended together in config.yaml with keys rabbit_queue_name_1 and rabbit_queue_name_2 as a dict. The system will open up channels with names: rabbit_queue_name_1 and rabbit_queue_name_2. Let us suppose that we want to train a PCA model on the pump dataset. The main config.yaml file should look as: defaults: pump : pca.yaml where pca.yaml is: load_object : ## this is a must - should not remove - # unique dynamic import of class (dynamic package download is not yet supported) module: framework.FaultDetection # framework folder, FaultDetection.py name: FaultDetectionExperiment # import this class setup: # this is also a must have. each experiment requires different parameters. # MUST NOT HAVE A \"data\" KEYWORD! datetime_column : \"ds\" #-> index, column datetime_format : \"ms\" # if saved integer, then provide timedelta e.g. milisecond, # if string then the format must be defined such as %y-%m-%d. eda: # from this key, inclusion is arbitrary create_model : #the key name is a function in the experiment, and the value is the parameter of the function. model : \"pca\" # here you can change the model for other models that is loaded in FaultDetectionExperiment class # e.g. \"ee\" #\"dbscan\" #\"pca\" # etc params: ## pca does not require tuning. ## if you want to change the model so that only #60% of the variance is explained, then: n_components : 0.6 ... # can add new functionalities to an experiment by overloading the class' functions. #In the run function, each key, e.g. \"function_name\" function in the class # is called with the parameters defined below the key. function_name : # define \"function_name\" az a class function param1 : 'a' # a parameter of \"function_name\" param2 : 'b' # another parameter of \"function_name\" General rules of writing the configuration files: eda can be turned of by removing its key (e.g. commenting out the line) if you do not want to change parameters, please do not declare them/include them under the params: keyword. The system is robust, but configuration files may or may not include must-have -s. This will be shown in model specification. Cannot read complex data structures, only basic types + lists/dicts for setup, datetime_column can be left out, and will use default index, except for time series analysis, where it is a must-have . Models section contains relevant parameters. If not, then the model's (in ./models/{experiment}/{model_name}) __init__ function contains modifiable parameters. If naming a target/datetime variable, and it is not existent, the system will throw an error. It is often just a \"ill-defined\" configuration. For the string datetime formating definitions, click here . The system facilitates data formating and auto-retraining function. Additional configuration is required. For FaultIsolation and TimeSeriesAnalysis, Retraining function is available. setup: ## Fault Isolation setup ... predict_window : 1000 ## does not work for PCA, controls how many points there should be in the prediction figure. retrain : retrain_window : 5000 ## if none, retrain on all data. Otherwise retrain on the last 5000 samples. # If the amount of data is less than 5000, it will train on all data. metric : \"Accuracy\" ## depending on experiment -> Accuracy, F1, Precision, Recall for Fault Isolation metric_threshold : 0.9 ## threshold that will cause retraining higher_better : True ## less or more better? -> depends on the measure. There is also a formating function for incoming non-xlsx sourced .json file. This currently works on one template asked for by a partner: setup: ## Fault Isolation setup ... format : ## settings for data reading... name : \"pivot\" # mode id : \"tsdata\" # what variable max_level : 1 columns : \"target\" index : \"date\" values : \"value\" Which can be used for the following template: { \"tsdata\": [ { \"target\": \"xy sensor1\", \"date\": \"2024-08-07T00:00:00\", \"value\": 0.12 }, { \"target\": \"xy sensor1\", \"date\": \"2024-08-07T00:00:10\", \"value\": 0.11 }, { \"target\": \"xy sensor2\", \"date\": \"2024-08-07T00:00:00\", \"value\": 231 }, { \"target\": \"xy sensor2\", \"date\": \"2024-08-07T00:00:10\", \"value\": 223 } ] } NOTE: For Text mining methods, a query format may be added to generate required files that are often is a very specific format. IT IS NOT YET IMPLEMENTED. Training Compiling training instructions is straightforward, api_commands.py has a function called compile_config , whose output can be used to request the mmlw:backend to start building specific models. compile_config Compiles the config file into a dictionary from several other .yaml configurations with hydra. Parameters: conf_path ( str ) \u2013 Path to the configuration files. conf_name ( str ) \u2013 Name of the configuration file. Returns: dict \u2013 Configuration dictionary Source code in api_commands.py 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 def compile_config ( conf_path , conf_name ): \"\"\"Compiles the config file into a dictionary from several other .yaml configurations with hydra. Parameters: conf_path (str): Path to the configuration files. conf_name (str): Name of the configuration file. Returns: dict: Configuration dictionary \"\"\" with initialize ( config_path = conf_path ): cfg = compose ( config_name = conf_name ) conf = OmegaConf . to_container ( cfg , resolve = True ) return conf In the config.yaml one can define any amount of \"queue\"-s -- These represent not only a rabbit queue for data flow, but an ID for the training models. You can also overwrite model IDs. Models are wrapped in an Experiment object, which handles specific utilities for the models. PLEASE NOTE THAT PREDICTION DATA IS NOT SAVED HERE, AND SHOULD BE DIRECTED TO AN OUTSIDE DATABASE. Here is a model configuration: load_object : ## this is a must have! This loads the experiment dinamically. module: framework.FaultIsolation ## Module name: FaultIsolationExperiment #x class name setup: ## this is outside of dynamic loop #- this must be present. It must assign the data for the experiment in a way it can handle. datetime_column : ds # will set column ds index # NOTE: different experiment handle function parameters differently! target : \"Output (S)\" # what to classify - target variable create_model : model : \"dt\" #\"ee\" #\"dbscan\" #\"pca\" # params: Some configuration designate a target variable - and must be given to use (e.g. fault isolation, time series analysis), otherwise the service will refuse to train the model (throws an error). create_model function's params keyword represent the model's arguments. Therefore, if you have e.g. a pca, variance_explained argument will be given here, provided it has the same name as defined in the model init () . If the template is finished, one should send it to the application. The communication system currently works similarly to the Remote Procedure Call (RPC) principle, which means that a request is sent to the backend, which will in turn pull data out of rabbit, and sends back a response. This is how we check on whether the data has been sent and used. The example script train_script_obj.py shows how to wrap data json objects to be sent to training. It uses a function named train extensively. Data is loaded in the script with pandas, but it can extended with anything. api_commands.train() Sends only one training request to the backend. Parameters: cfg ( dict ) \u2013 Configuration dictionary. url ( str , default: 'http://localhost:8010' ) \u2013 URL of the backend. model_name ( str , default: 'model' ) \u2013 Name of the model. data ( str , default: None ) \u2013 Data in JSON format. (must be converted to json, if not json) rabbit_host ( str , default: 'localhost' ) \u2013 RabbitMQ host. rabbit_port ( str , default: '5100' ) \u2013 RabbitMQ port. user ( str , default: 'guest' ) \u2013 RabbitMQ user. password ( str , default: 'guest' ) \u2013 RabbitMQ password. Returns: response \u2013 Response from the backend. Source code in api_commands.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def train ( cfg : dict , url = \"http://localhost:8010\" , model_name = \"model\" , data = None , rabbit_host = \"localhost\" , rabbit_port = \"5100\" , user = \"guest\" , password = \"guest\" ): \"\"\"Sends only one training request to the backend. Parameters: cfg (dict): Configuration dictionary. url (str): URL of the backend. model_name (str): Name of the model. data (str): Data in JSON format. (must be converted to json, if not json) rabbit_host (str): RabbitMQ host. rabbit_port (str): RabbitMQ port. user (str): RabbitMQ user. password (str): RabbitMQ password. Returns: response: Response from the backend. \"\"\" post_data ( data , url = url , queue_name = model_name , host = rabbit_host , port = rabbit_port , user = user , password = password ) json_object = json . dumps ( cfg , indent = 4 ) response = requests . post ( url = url + f \"/ { model_name } \" + \"/train\" , data = json_object ) return response In train_script_local.py , api_commands.train_models is used -- which reads data from the harddrive. The path to the data is defined in data.yaml. api_commands.train_models() Sends project template and related data to the backend. For each key in the config, it will initiate a training request and post the relevant data to the relevant queue. Parameters: conf_path ( str , default: './configs' ) \u2013 Path to the configuration files. conf_name ( str , default: 'config' ) \u2013 Name of the configuration file. data_path ( str , default: 'data.yaml' ) \u2013 Path to the data file. Requires the same keys as the configuration file. url ( str , default: 'http://localhost:8010' ) \u2013 URL of the backend. rabbit_host ( str , default: 'localhost' ) \u2013 RabbitMQ host. rabbit_port ( str , default: '5100' ) \u2013 RabbitMQ port. user ( str , default: 'guest' ) \u2013 RabbitMQ user. password ( str , default: 'guest' ) \u2013 RabbitMQ password. Returns: response \u2013 Response from the backend. Source code in api_commands.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def train_models ( conf_path : str = \"./configs\" , conf_name : str = \"config\" , data_path : str = \"data.yaml\" , url : str = \"http://localhost:8010\" , rabbit_host : str = \"localhost\" , rabbit_port : str = \"5100\" , user : str = \"guest\" , password : str = \"guest\" ): \"\"\"Sends project template and related data to the backend. For each key in the config, it will initiate a training request and post the relevant data to the relevant queue. Parameters: conf_path (str): Path to the configuration files. conf_name (str): Name of the configuration file. data_path (str): Path to the data file. Requires the same keys as the configuration file. url (str): URL of the backend. rabbit_host (str): RabbitMQ host. rabbit_port (str): RabbitMQ port. user (str): RabbitMQ user. password (str): RabbitMQ password. Returns: response: Response from the backend. \"\"\" cfg = compile_config ( conf_path , conf_name ) with open ( data_path , 'r' ) as f : data_cfg = yaml . load ( f , Loader = yaml . SafeLoader ) for k in cfg . keys (): ## read data - xlsx, csv, json, pickle if data_cfg [ k ] . endswith ( \".xlsx\" ): #endswith X = pd . read_excel ( data_cfg [ k ]) X = X . to_json () elif data_cfg [ k ] . endswith ( \".csv\" ): X = pd . read_csv ( data_cfg [ k ]) X = X . to_json () elif data_cfg [ k ] . endswith ( \".txt\" ): X = pd . read_csv ( data_cfg [ k ]) X = X . to_json () elif data_cfg [ k ] . endswith ( \".json\" ): with open ( data_cfg [ k ], 'r' ) as f : X = json . dumps (( json . load ( f ))) elif data_cfg [ k ] . endswith ( \".pkl\" ): with open ( data_cfg [ k ], \"rb\" ) as f : X = pickle . load ( f ) X = pd . DataFrame ( X ) . to_json () else : print ( f \"Data file { data_path [ k ] } does not exist, or cannot load. Please ensure the file is in the correct format (xlsx, csv, txt, json, pkl).\" ) return flush_rabbit ( url , k ) rep = train ( cfg [ k ], url = url , model_name = k , data = X , rabbit_host = rabbit_host , rabbit_port = rabbit_port , user = user , password = password ) print ( f \"Response: { rep . content } \" ) To try out the example file, type to the terminal (run it in console): python train_script_local.py The frontend asks for the figures every 5 seconds and will automatically refresh the figures, if any models are trained and available, it will drop the error, and start showing the figures. Here, the detect experiment was used with DecisionTree.yaml. However, the predict figures remain empty. If the backend is busy, the frontend will not be able to load. If multiple models were trained, one can also select which to monitor on the left hand side, under \"Experiments\" One can also inspect the logs in Docker Desktop's log tab: Predict and Monitoring Prediction will impact the frontend's prediction figure(s). It currently works similar to the train, except that no configuration is required to use predict. If there is no model with the name the predict asks for, then it will throw an error. Let's test the prediction function of the decision tree we trained in the previous section. The test_script.py employs the api_commands.predict function: api_commands.predict Sends a prediction request to the backend. Post data to RabbitMQ. Parameters: url ( str , default: 'http://localhost:8010' ) \u2013 URL of the backend. data ( str , default: None ) \u2013 Data in JSON format. (must be converted to json, if not json) model_name ( str , default: 'model' ) \u2013 Name of the model. rhost ( str , default: 'localhost' ) \u2013 RabbitMQ host. rport ( str , default: 5672 ) \u2013 RabbitMQ port. user ( str , default: 'guest' ) \u2013 RabbitMQ user. password ( str , default: 'guest' ) \u2013 RabbitMQ password. Returns: response \u2013 Response from the backend. Source code in api_commands.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 def predict ( url = \"http://localhost:8010\" , data = None , model_name = \"model\" , rhost = \"localhost\" , rport = 5672 , user = \"guest\" , password = \"guest\" ): \"\"\"Sends a prediction request to the backend. Post data to RabbitMQ. Parameters: url (str): URL of the backend. data (str): Data in JSON format. (must be converted to json, if not json) model_name (str): Name of the model. rhost (str): RabbitMQ host. rport (str): RabbitMQ port. user (str): RabbitMQ user. password (str): RabbitMQ password. Returns: response: Response from the backend. \"\"\" if data is None : data = json . dumps ({}) post_data ( data , url = url , queue_name = model_name , host = rhost , port = rport , user = user , password = password ) response = requests . post ( f \" { url } / { model_name } /predict\" ) return response Change mode_name arg for queue and id selection. test_script.py contains: from api_commands import predict # get predict import pandas as pd # pandas read in xlsx for i in range(8): ## test dataset is separated into 8 different xlsx files. data = pd.read_excel(f\"./data/detect_test_{i}.xlsx\") # read data.reset_index(inplace=True, drop=True) data = data.to_json() # to json predict(url=\"http://localhost:8010\", model_name = \"detect\", data = data, rhost = \"localhost\", rport = \"5100\") ##simple way to post predict request. #predict sends data to \"detect\" queue, and send a train request to the url provided. Processing the data will update the figures, and so: PLEASE NOTE THAT ONLY JSON FILES CAN BE SENT! Loading Experiment Loading a model is necessary if the same model is to be used with the same sensors. It saves computing power, and time spent with parameterizing. api_commands comes with a function called load_experiment. This will only load the state of the model it was saved in, most likely after training. api_commands.load_experiment() Sends a load request to the backend. Parameters: address ( str , default: 'http://localhost:8010' ) \u2013 URL of the backend. model_name ( str , default: 'model' ) \u2013 Name of the model. run_id ( str , default: None ) \u2013 Run ID of the model. Returns: response \u2013 Response from the backend. Source code in api_commands.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def load_experiment ( address : str = \"http://localhost:8010\" , model_name = \"model\" , run_id : str = None ): \"\"\"Sends a load request to the backend. Parameters: address (str): URL of the backend. model_name (str): Name of the model. run_id (str): Run ID of the model. Returns: response: Response from the backend. \"\"\" if run_id is None : resp = requests . post ( f \" { address } / { model_name } /load\" ) else : resp = requests . post ( f \" { address } / { model_name } /load/ { run_id } \" ) return resp . text To load a model, you have to find the wanted model's run_id, if it has not been initialized (no models with ID). If the model ID was previously used, it will load back that model. Run ID can be read from the mlflow ui. e.g. for an LSTM model, you have to find the experiment initialized as the rabbit queue name, select a sympathetic run and copy its ID: load_script.py will load a run id if given. For local testing, train a model, restart service, and use the ID of the trained model. The ID can be found in the MLFLOW repository. We gave the keyword \"load_try\", which did not override my previous model. It is an instance of the trained model saved on mlflow before. We also saved the trained/predicted model to the MLFLOW repository. python save_script.py which can be loaded back with load_script.py if the mlflow run id is given! Notes Cannot yet compare models programatically. Retraining autmatically is not yet included PCA package has problems with indexing, so shouldn't be fed with data that contains duplicate indices Bug with BayesNet - only works locally as the environment could not yet make the necessary C++ compilations -- It works but only locally. Some models do not have \"proper\" predict functions -- process mining is a train only model which repeats train function under prediction function, without saving the results. If you notice any bugs, please save the error message, and send it to the developer.","title":"Client Usage and Testing"},{"location":"client/#client-side-use","text":"This section explains how to communicate with a running ObServML application, which includes a short description on how to compile instructions for training sessions, then how predict and loading/saving functions are used. Since the developer environment is not required, first a short client-side setup is included.","title":"Client-side Use"},{"location":"client/#client-side-environment-setup","text":"First download Python and an IDE (We recommend VSC or Pycharm). Pull from repo https://github.com/adamipkovich/observml.git, branch \"main\". This repo is currently public and available for cloning. Install packages if only the client-side code is used: pip install -r mindeps.txt","title":"Client-side environment setup."},{"location":"client/#example-project","text":"In the \"main\" branch, you can find the files relevant to client-side use: Hydra template config directory -- configs folder folders that represent an Experiment-ID and a rabbit-queue -> these folders include .yaml files that are experiment specifications. __init__.py -- a must have or hydra cannot build the configuration file config.yaml -- this yaml file contains what experiments should be trained with which configuration (adding here key/value pairs will train an experiment with the name ${key}, with model configuration located in ${value} - which are .yaml files. Removing a key/value pair means that it will not train that experiment again. ) Template Experiment specifications are provided under the Models section. api_commands.py - precompiled set of commands. A must have. data (e.g. ./data/) - testing data to be posted on rabbit. Each is stored in .xlsx format, but the system can only receive .json formatted data! NOTE: Recent developments contain a formatter for json files. data.yaml - defines local paths to data, if not passed as objects. (used by train_stript_local.py) example files: train_script_local.py, train_script_obj.py, test_script.py Data folder and .yaml is not necessary, data can be passed as an object as well. We provide 2 examples for training, one of which automatically reads in data from the project, defined in the .yaml file. The other reads the .xlsx-s in to pass it as an object in memory (as an example for memory-based handling of data). docker-compose.yaml which will be the heartbeat of the application. An example for project structure looks as follows: data example.data.xlsx configs rabbit_queue_name_1 ml_model_template.yaml rabbit_queue_name_2 ml_model_template.yaml __init__.py config.yaml api_commands.py data.yaml docker-compose.yaml NOTE: In this form, the project is only capable of sending instructions. The hydra structure has a main configuration file, which is called config.yaml. Here, keywords represent the folder names, and the values the .yaml file that should be used for the key (hydra dynamically builds the configuration files, but one keyword can only have one .yaml injected). Here it would look like: defaults: rabbit_queue_name_1 : experiment_template.yaml rabbit_queue_name_2 : experiment_template.yaml .yaml files will be appended together in config.yaml with keys rabbit_queue_name_1 and rabbit_queue_name_2 as a dict. The system will open up channels with names: rabbit_queue_name_1 and rabbit_queue_name_2. Let us suppose that we want to train a PCA model on the pump dataset. The main config.yaml file should look as: defaults: pump : pca.yaml where pca.yaml is: load_object : ## this is a must - should not remove - # unique dynamic import of class (dynamic package download is not yet supported) module: framework.FaultDetection # framework folder, FaultDetection.py name: FaultDetectionExperiment # import this class setup: # this is also a must have. each experiment requires different parameters. # MUST NOT HAVE A \"data\" KEYWORD! datetime_column : \"ds\" #-> index, column datetime_format : \"ms\" # if saved integer, then provide timedelta e.g. milisecond, # if string then the format must be defined such as %y-%m-%d. eda: # from this key, inclusion is arbitrary create_model : #the key name is a function in the experiment, and the value is the parameter of the function. model : \"pca\" # here you can change the model for other models that is loaded in FaultDetectionExperiment class # e.g. \"ee\" #\"dbscan\" #\"pca\" # etc params: ## pca does not require tuning. ## if you want to change the model so that only #60% of the variance is explained, then: n_components : 0.6 ... # can add new functionalities to an experiment by overloading the class' functions. #In the run function, each key, e.g. \"function_name\" function in the class # is called with the parameters defined below the key. function_name : # define \"function_name\" az a class function param1 : 'a' # a parameter of \"function_name\" param2 : 'b' # another parameter of \"function_name\" General rules of writing the configuration files: eda can be turned of by removing its key (e.g. commenting out the line) if you do not want to change parameters, please do not declare them/include them under the params: keyword. The system is robust, but configuration files may or may not include must-have -s. This will be shown in model specification. Cannot read complex data structures, only basic types + lists/dicts for setup, datetime_column can be left out, and will use default index, except for time series analysis, where it is a must-have . Models section contains relevant parameters. If not, then the model's (in ./models/{experiment}/{model_name}) __init__ function contains modifiable parameters. If naming a target/datetime variable, and it is not existent, the system will throw an error. It is often just a \"ill-defined\" configuration. For the string datetime formating definitions, click here . The system facilitates data formating and auto-retraining function. Additional configuration is required. For FaultIsolation and TimeSeriesAnalysis, Retraining function is available. setup: ## Fault Isolation setup ... predict_window : 1000 ## does not work for PCA, controls how many points there should be in the prediction figure. retrain : retrain_window : 5000 ## if none, retrain on all data. Otherwise retrain on the last 5000 samples. # If the amount of data is less than 5000, it will train on all data. metric : \"Accuracy\" ## depending on experiment -> Accuracy, F1, Precision, Recall for Fault Isolation metric_threshold : 0.9 ## threshold that will cause retraining higher_better : True ## less or more better? -> depends on the measure. There is also a formating function for incoming non-xlsx sourced .json file. This currently works on one template asked for by a partner: setup: ## Fault Isolation setup ... format : ## settings for data reading... name : \"pivot\" # mode id : \"tsdata\" # what variable max_level : 1 columns : \"target\" index : \"date\" values : \"value\" Which can be used for the following template: { \"tsdata\": [ { \"target\": \"xy sensor1\", \"date\": \"2024-08-07T00:00:00\", \"value\": 0.12 }, { \"target\": \"xy sensor1\", \"date\": \"2024-08-07T00:00:10\", \"value\": 0.11 }, { \"target\": \"xy sensor2\", \"date\": \"2024-08-07T00:00:00\", \"value\": 231 }, { \"target\": \"xy sensor2\", \"date\": \"2024-08-07T00:00:10\", \"value\": 223 } ] } NOTE: For Text mining methods, a query format may be added to generate required files that are often is a very specific format. IT IS NOT YET IMPLEMENTED.","title":"Example project"},{"location":"client/#training","text":"Compiling training instructions is straightforward, api_commands.py has a function called compile_config , whose output can be used to request the mmlw:backend to start building specific models.","title":"Training"},{"location":"client/#compile_config","text":"Compiles the config file into a dictionary from several other .yaml configurations with hydra. Parameters: conf_path ( str ) \u2013 Path to the configuration files. conf_name ( str ) \u2013 Name of the configuration file. Returns: dict \u2013 Configuration dictionary Source code in api_commands.py 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 def compile_config ( conf_path , conf_name ): \"\"\"Compiles the config file into a dictionary from several other .yaml configurations with hydra. Parameters: conf_path (str): Path to the configuration files. conf_name (str): Name of the configuration file. Returns: dict: Configuration dictionary \"\"\" with initialize ( config_path = conf_path ): cfg = compose ( config_name = conf_name ) conf = OmegaConf . to_container ( cfg , resolve = True ) return conf In the config.yaml one can define any amount of \"queue\"-s -- These represent not only a rabbit queue for data flow, but an ID for the training models. You can also overwrite model IDs. Models are wrapped in an Experiment object, which handles specific utilities for the models. PLEASE NOTE THAT PREDICTION DATA IS NOT SAVED HERE, AND SHOULD BE DIRECTED TO AN OUTSIDE DATABASE. Here is a model configuration: load_object : ## this is a must have! This loads the experiment dinamically. module: framework.FaultIsolation ## Module name: FaultIsolationExperiment #x class name setup: ## this is outside of dynamic loop #- this must be present. It must assign the data for the experiment in a way it can handle. datetime_column : ds # will set column ds index # NOTE: different experiment handle function parameters differently! target : \"Output (S)\" # what to classify - target variable create_model : model : \"dt\" #\"ee\" #\"dbscan\" #\"pca\" # params: Some configuration designate a target variable - and must be given to use (e.g. fault isolation, time series analysis), otherwise the service will refuse to train the model (throws an error). create_model function's params keyword represent the model's arguments. Therefore, if you have e.g. a pca, variance_explained argument will be given here, provided it has the same name as defined in the model init () . If the template is finished, one should send it to the application. The communication system currently works similarly to the Remote Procedure Call (RPC) principle, which means that a request is sent to the backend, which will in turn pull data out of rabbit, and sends back a response. This is how we check on whether the data has been sent and used. The example script train_script_obj.py shows how to wrap data json objects to be sent to training. It uses a function named train extensively. Data is loaded in the script with pandas, but it can extended with anything. api_commands.train() Sends only one training request to the backend. Parameters: cfg ( dict ) \u2013 Configuration dictionary. url ( str , default: 'http://localhost:8010' ) \u2013 URL of the backend. model_name ( str , default: 'model' ) \u2013 Name of the model. data ( str , default: None ) \u2013 Data in JSON format. (must be converted to json, if not json) rabbit_host ( str , default: 'localhost' ) \u2013 RabbitMQ host. rabbit_port ( str , default: '5100' ) \u2013 RabbitMQ port. user ( str , default: 'guest' ) \u2013 RabbitMQ user. password ( str , default: 'guest' ) \u2013 RabbitMQ password. Returns: response \u2013 Response from the backend. Source code in api_commands.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def train ( cfg : dict , url = \"http://localhost:8010\" , model_name = \"model\" , data = None , rabbit_host = \"localhost\" , rabbit_port = \"5100\" , user = \"guest\" , password = \"guest\" ): \"\"\"Sends only one training request to the backend. Parameters: cfg (dict): Configuration dictionary. url (str): URL of the backend. model_name (str): Name of the model. data (str): Data in JSON format. (must be converted to json, if not json) rabbit_host (str): RabbitMQ host. rabbit_port (str): RabbitMQ port. user (str): RabbitMQ user. password (str): RabbitMQ password. Returns: response: Response from the backend. \"\"\" post_data ( data , url = url , queue_name = model_name , host = rabbit_host , port = rabbit_port , user = user , password = password ) json_object = json . dumps ( cfg , indent = 4 ) response = requests . post ( url = url + f \"/ { model_name } \" + \"/train\" , data = json_object ) return response In train_script_local.py , api_commands.train_models is used -- which reads data from the harddrive. The path to the data is defined in data.yaml. api_commands.train_models() Sends project template and related data to the backend. For each key in the config, it will initiate a training request and post the relevant data to the relevant queue. Parameters: conf_path ( str , default: './configs' ) \u2013 Path to the configuration files. conf_name ( str , default: 'config' ) \u2013 Name of the configuration file. data_path ( str , default: 'data.yaml' ) \u2013 Path to the data file. Requires the same keys as the configuration file. url ( str , default: 'http://localhost:8010' ) \u2013 URL of the backend. rabbit_host ( str , default: 'localhost' ) \u2013 RabbitMQ host. rabbit_port ( str , default: '5100' ) \u2013 RabbitMQ port. user ( str , default: 'guest' ) \u2013 RabbitMQ user. password ( str , default: 'guest' ) \u2013 RabbitMQ password. Returns: response \u2013 Response from the backend. Source code in api_commands.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def train_models ( conf_path : str = \"./configs\" , conf_name : str = \"config\" , data_path : str = \"data.yaml\" , url : str = \"http://localhost:8010\" , rabbit_host : str = \"localhost\" , rabbit_port : str = \"5100\" , user : str = \"guest\" , password : str = \"guest\" ): \"\"\"Sends project template and related data to the backend. For each key in the config, it will initiate a training request and post the relevant data to the relevant queue. Parameters: conf_path (str): Path to the configuration files. conf_name (str): Name of the configuration file. data_path (str): Path to the data file. Requires the same keys as the configuration file. url (str): URL of the backend. rabbit_host (str): RabbitMQ host. rabbit_port (str): RabbitMQ port. user (str): RabbitMQ user. password (str): RabbitMQ password. Returns: response: Response from the backend. \"\"\" cfg = compile_config ( conf_path , conf_name ) with open ( data_path , 'r' ) as f : data_cfg = yaml . load ( f , Loader = yaml . SafeLoader ) for k in cfg . keys (): ## read data - xlsx, csv, json, pickle if data_cfg [ k ] . endswith ( \".xlsx\" ): #endswith X = pd . read_excel ( data_cfg [ k ]) X = X . to_json () elif data_cfg [ k ] . endswith ( \".csv\" ): X = pd . read_csv ( data_cfg [ k ]) X = X . to_json () elif data_cfg [ k ] . endswith ( \".txt\" ): X = pd . read_csv ( data_cfg [ k ]) X = X . to_json () elif data_cfg [ k ] . endswith ( \".json\" ): with open ( data_cfg [ k ], 'r' ) as f : X = json . dumps (( json . load ( f ))) elif data_cfg [ k ] . endswith ( \".pkl\" ): with open ( data_cfg [ k ], \"rb\" ) as f : X = pickle . load ( f ) X = pd . DataFrame ( X ) . to_json () else : print ( f \"Data file { data_path [ k ] } does not exist, or cannot load. Please ensure the file is in the correct format (xlsx, csv, txt, json, pkl).\" ) return flush_rabbit ( url , k ) rep = train ( cfg [ k ], url = url , model_name = k , data = X , rabbit_host = rabbit_host , rabbit_port = rabbit_port , user = user , password = password ) print ( f \"Response: { rep . content } \" ) To try out the example file, type to the terminal (run it in console): python train_script_local.py The frontend asks for the figures every 5 seconds and will automatically refresh the figures, if any models are trained and available, it will drop the error, and start showing the figures. Here, the detect experiment was used with DecisionTree.yaml. However, the predict figures remain empty. If the backend is busy, the frontend will not be able to load. If multiple models were trained, one can also select which to monitor on the left hand side, under \"Experiments\" One can also inspect the logs in Docker Desktop's log tab:","title":"compile_config"},{"location":"client/#predict-and-monitoring","text":"Prediction will impact the frontend's prediction figure(s). It currently works similar to the train, except that no configuration is required to use predict. If there is no model with the name the predict asks for, then it will throw an error. Let's test the prediction function of the decision tree we trained in the previous section. The test_script.py employs the api_commands.predict function:","title":"Predict and Monitoring"},{"location":"client/#api_commandspredict","text":"Sends a prediction request to the backend. Post data to RabbitMQ. Parameters: url ( str , default: 'http://localhost:8010' ) \u2013 URL of the backend. data ( str , default: None ) \u2013 Data in JSON format. (must be converted to json, if not json) model_name ( str , default: 'model' ) \u2013 Name of the model. rhost ( str , default: 'localhost' ) \u2013 RabbitMQ host. rport ( str , default: 5672 ) \u2013 RabbitMQ port. user ( str , default: 'guest' ) \u2013 RabbitMQ user. password ( str , default: 'guest' ) \u2013 RabbitMQ password. Returns: response \u2013 Response from the backend. Source code in api_commands.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 def predict ( url = \"http://localhost:8010\" , data = None , model_name = \"model\" , rhost = \"localhost\" , rport = 5672 , user = \"guest\" , password = \"guest\" ): \"\"\"Sends a prediction request to the backend. Post data to RabbitMQ. Parameters: url (str): URL of the backend. data (str): Data in JSON format. (must be converted to json, if not json) model_name (str): Name of the model. rhost (str): RabbitMQ host. rport (str): RabbitMQ port. user (str): RabbitMQ user. password (str): RabbitMQ password. Returns: response: Response from the backend. \"\"\" if data is None : data = json . dumps ({}) post_data ( data , url = url , queue_name = model_name , host = rhost , port = rport , user = user , password = password ) response = requests . post ( f \" { url } / { model_name } /predict\" ) return response Change mode_name arg for queue and id selection. test_script.py contains: from api_commands import predict # get predict import pandas as pd # pandas read in xlsx for i in range(8): ## test dataset is separated into 8 different xlsx files. data = pd.read_excel(f\"./data/detect_test_{i}.xlsx\") # read data.reset_index(inplace=True, drop=True) data = data.to_json() # to json predict(url=\"http://localhost:8010\", model_name = \"detect\", data = data, rhost = \"localhost\", rport = \"5100\") ##simple way to post predict request. #predict sends data to \"detect\" queue, and send a train request to the url provided. Processing the data will update the figures, and so: PLEASE NOTE THAT ONLY JSON FILES CAN BE SENT!","title":"api_commands.predict"},{"location":"client/#loading-experiment","text":"Loading a model is necessary if the same model is to be used with the same sensors. It saves computing power, and time spent with parameterizing. api_commands comes with a function called load_experiment. This will only load the state of the model it was saved in, most likely after training. api_commands.load_experiment() Sends a load request to the backend. Parameters: address ( str , default: 'http://localhost:8010' ) \u2013 URL of the backend. model_name ( str , default: 'model' ) \u2013 Name of the model. run_id ( str , default: None ) \u2013 Run ID of the model. Returns: response \u2013 Response from the backend. Source code in api_commands.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def load_experiment ( address : str = \"http://localhost:8010\" , model_name = \"model\" , run_id : str = None ): \"\"\"Sends a load request to the backend. Parameters: address (str): URL of the backend. model_name (str): Name of the model. run_id (str): Run ID of the model. Returns: response: Response from the backend. \"\"\" if run_id is None : resp = requests . post ( f \" { address } / { model_name } /load\" ) else : resp = requests . post ( f \" { address } / { model_name } /load/ { run_id } \" ) return resp . text To load a model, you have to find the wanted model's run_id, if it has not been initialized (no models with ID). If the model ID was previously used, it will load back that model. Run ID can be read from the mlflow ui. e.g. for an LSTM model, you have to find the experiment initialized as the rabbit queue name, select a sympathetic run and copy its ID: load_script.py will load a run id if given. For local testing, train a model, restart service, and use the ID of the trained model. The ID can be found in the MLFLOW repository. We gave the keyword \"load_try\", which did not override my previous model. It is an instance of the trained model saved on mlflow before. We also saved the trained/predicted model to the MLFLOW repository. python save_script.py which can be loaded back with load_script.py if the mlflow run id is given!","title":"Loading Experiment"},{"location":"client/#notes","text":"Cannot yet compare models programatically. Retraining autmatically is not yet included PCA package has problems with indexing, so shouldn't be fed with data that contains duplicate indices Bug with BayesNet - only works locally as the environment could not yet make the necessary C++ compilations -- It works but only locally. Some models do not have \"proper\" predict functions -- process mining is a train only model which repeats train function under prediction function, without saving the results. If you notice any bugs, please save the error message, and send it to the developer.","title":"Notes"},{"location":"configuration/","text":"ExperimentHub Configuration System The ExperimentHub configuration system provides a flexible way to configure the ExperimentHub, its plugins, and available experiment types. This document explains the configuration file structure, how to load configuration, and best practices for configuration management. Configuration File Structure The ExperimentHub is configured using a YAML configuration file ( hub_config.yaml ). The file has the following structure: # hub_config.yaml version: \"1.0\" # Global settings global: log_level: \"info\" environment: \"development\" # Plugin configurations plugins: # MLOps plugin configuration mlops: enabled: true type: \"mlflow\" # Which implementation to use config: mlflow_uri: \"http://localhost:5000\" # Data stream plugin configuration datastream: enabled: true type: \"rabbitmq\" config: host: \"localhost\" port: 5672 username: \"guest\" password: \"guest\" # Task queue plugin configuration - REMOVED # The task queue functionality has been removed from ObservML # Training and prediction operations are now handled synchronously # Available experiment types experiments: - name: \"time_series\" module: \"framework.TimeSeriesAnalysis\" class: \"TimeSeriesExperiment\" enabled: true - name: \"fault_detection\" module: \"framework.FaultDetection\" class: \"FaultDetectionExperiment\" enabled: true - name: \"fault_isolation\" module: \"framework.FaultIsolation\" class: \"FaultIsolationExperiment\" enabled: true - name: \"process_mining\" module: \"framework.ProcessMining\" class: \"ProcessMiningExperiment\" enabled: true Global Settings The global section contains global settings for the ExperimentHub: log_level : The logging level (e.g., \"debug\", \"info\", \"warning\", \"error\") environment : The environment (e.g., \"development\", \"testing\", \"production\") Plugin Configurations The plugins section contains configurations for each plugin type: mlops : Configuration for the MLOps plugin datastream : Configuration for the DataStream plugin taskqueue : Configuration for the TaskQueue plugin Each plugin configuration has the following structure: enabled : Whether the plugin is enabled type : The type of plugin implementation to use config : Configuration parameters specific to the plugin implementation Experiment Types The experiments section contains a list of available experiment types: name : The name of the experiment type module : The Python module containing the experiment class class : The name of the experiment class enabled : Whether the experiment type is enabled Loading Configuration The ExperimentHub can be initialized from a configuration file using the from_config class method: from framework.ExperimentHub import ExperimentHub # Initialize from configuration file hub = ExperimentHub.from_config(\"hub_config.yaml\") The from_config method performs the following steps: Loads the configuration file Creates an ExperimentHub instance Initializes plugins based on the configuration Registers available experiment types Returns the initialized ExperimentHub Environment Variables The ExperimentHubAPI also supports configuration through environment variables: HUB_CONFIG_PATH : Path to the configuration file (default: \"hub_config.yaml\") MLFLOW_URI : URI for MLflow tracking server RABBIT_HOST : Hostname for RabbitMQ server RABBIT_PORT : Port for RabbitMQ server RABBIT_USER : Username for RabbitMQ server RABBIT_PASSWORD : Password for RabbitMQ server If the configuration file specified by HUB_CONFIG_PATH exists, it will be used. Otherwise, the ExperimentHub will be initialized using environment variables. Configuration Best Practices Separate Configuration by Environment Create separate configuration files for different environments: hub_config.dev.yaml : Development environment hub_config.test.yaml : Testing environment hub_config.prod.yaml : Production environment Use Environment Variables for Sensitive Information Use environment variables for sensitive information like passwords and API keys: plugins: datastream: enabled: true type: \"rabbitmq\" config: host: \"localhost\" port: 5672 username: \"${RABBIT_USER}\" password: \"${RABBIT_PASSWORD}\" Document Configuration Parameters Document all configuration parameters, including their purpose, allowed values, and default values. Validate Configuration Validate the configuration file before using it: def validate_config(config): \"\"\"Validate the configuration file\"\"\" # Check required sections required_sections = [\"version\", \"plugins\", \"experiments\"] for section in required_sections: if section not in config: raise ValueError(f\"Missing required section: {section}\") # Check plugin configurations for plugin_type, plugin_config in config[\"plugins\"].items(): if \"enabled\" not in plugin_config: raise ValueError(f\"Missing 'enabled' field in {plugin_type} plugin configuration\") if plugin_config[\"enabled\"] and \"type\" not in plugin_config: raise ValueError(f\"Missing 'type' field in {plugin_type} plugin configuration\") if plugin_config[\"enabled\"] and \"config\" not in plugin_config: raise ValueError(f\"Missing 'config' field in {plugin_type} plugin configuration\") # Check experiment configurations for i, exp_config in enumerate(config[\"experiments\"]): if \"name\" not in exp_config: raise ValueError(f\"Missing 'name' field in experiment configuration at index {i}\") if \"module\" not in exp_config: raise ValueError(f\"Missing 'module' field in experiment configuration at index {i}\") if \"class\" not in exp_config: raise ValueError(f\"Missing 'class' field in experiment configuration at index {i}\") Use Default Values Provide default values for optional configuration parameters: def get_config_value(config, path, default=None): \"\"\"Get a value from the configuration, with a default value\"\"\" parts = path.split(\".\") current = config for part in parts: if part not in current: return default current = current[part] return current Example Configurations Development Configuration # hub_config.dev.yaml version: \"1.0\" global: log_level: \"debug\" environment: \"development\" plugins: mlops: enabled: true type: \"mlflow\" config: mlflow_uri: \"http://localhost:5000\" datastream: enabled: true type: \"rabbitmq\" config: host: \"localhost\" port: 5672 username: \"guest\" password: \"guest\" experiments: - name: \"time_series\" module: \"framework.TimeSeriesAnalysis\" class: \"TimeSeriesExperiment\" enabled: true - name: \"fault_detection\" module: \"framework.FaultDetection\" class: \"FaultDetectionExperiment\" enabled: true Production Configuration # hub_config.prod.yaml version: \"1.0\" global: log_level: \"info\" environment: \"production\" plugins: mlops: enabled: true type: \"mlflow\" config: mlflow_uri: \"http://mlflow.example.com\" datastream: enabled: true type: \"rabbitmq\" config: host: \"rabbitmq.example.com\" port: 5672 username: \"${RABBIT_USER}\" password: \"${RABBIT_PASSWORD}\" experiments: - name: \"time_series\" module: \"framework.TimeSeriesAnalysis\" class: \"TimeSeriesExperiment\" enabled: true - name: \"fault_detection\" module: \"framework.FaultDetection\" class: \"FaultDetectionExperiment\" enabled: true - name: \"fault_isolation\" module: \"framework.FaultIsolation\" class: \"FaultIsolationExperiment\" enabled: true - name: \"process_mining\" module: \"framework.ProcessMining\" class: \"ProcessMiningExperiment\" enabled: true Troubleshooting Configuration File Not Found If the configuration file is not found, the ExperimentHub will fall back to using environment variables. Make sure the configuration file exists and is accessible. Invalid Configuration If the configuration file is invalid, the ExperimentHub will raise an error. Check the error message for details on what's wrong with the configuration. Plugin Initialization Errors If a plugin fails to initialize, check the plugin configuration and make sure all required parameters are provided.","title":"Configuration"},{"location":"configuration/#experimenthub-configuration-system","text":"The ExperimentHub configuration system provides a flexible way to configure the ExperimentHub, its plugins, and available experiment types. This document explains the configuration file structure, how to load configuration, and best practices for configuration management.","title":"ExperimentHub Configuration System"},{"location":"configuration/#configuration-file-structure","text":"The ExperimentHub is configured using a YAML configuration file ( hub_config.yaml ). The file has the following structure: # hub_config.yaml version: \"1.0\" # Global settings global: log_level: \"info\" environment: \"development\" # Plugin configurations plugins: # MLOps plugin configuration mlops: enabled: true type: \"mlflow\" # Which implementation to use config: mlflow_uri: \"http://localhost:5000\" # Data stream plugin configuration datastream: enabled: true type: \"rabbitmq\" config: host: \"localhost\" port: 5672 username: \"guest\" password: \"guest\" # Task queue plugin configuration - REMOVED # The task queue functionality has been removed from ObservML # Training and prediction operations are now handled synchronously # Available experiment types experiments: - name: \"time_series\" module: \"framework.TimeSeriesAnalysis\" class: \"TimeSeriesExperiment\" enabled: true - name: \"fault_detection\" module: \"framework.FaultDetection\" class: \"FaultDetectionExperiment\" enabled: true - name: \"fault_isolation\" module: \"framework.FaultIsolation\" class: \"FaultIsolationExperiment\" enabled: true - name: \"process_mining\" module: \"framework.ProcessMining\" class: \"ProcessMiningExperiment\" enabled: true","title":"Configuration File Structure"},{"location":"configuration/#global-settings","text":"The global section contains global settings for the ExperimentHub: log_level : The logging level (e.g., \"debug\", \"info\", \"warning\", \"error\") environment : The environment (e.g., \"development\", \"testing\", \"production\")","title":"Global Settings"},{"location":"configuration/#plugin-configurations","text":"The plugins section contains configurations for each plugin type: mlops : Configuration for the MLOps plugin datastream : Configuration for the DataStream plugin taskqueue : Configuration for the TaskQueue plugin Each plugin configuration has the following structure: enabled : Whether the plugin is enabled type : The type of plugin implementation to use config : Configuration parameters specific to the plugin implementation","title":"Plugin Configurations"},{"location":"configuration/#experiment-types","text":"The experiments section contains a list of available experiment types: name : The name of the experiment type module : The Python module containing the experiment class class : The name of the experiment class enabled : Whether the experiment type is enabled","title":"Experiment Types"},{"location":"configuration/#loading-configuration","text":"The ExperimentHub can be initialized from a configuration file using the from_config class method: from framework.ExperimentHub import ExperimentHub # Initialize from configuration file hub = ExperimentHub.from_config(\"hub_config.yaml\") The from_config method performs the following steps: Loads the configuration file Creates an ExperimentHub instance Initializes plugins based on the configuration Registers available experiment types Returns the initialized ExperimentHub","title":"Loading Configuration"},{"location":"configuration/#environment-variables","text":"The ExperimentHubAPI also supports configuration through environment variables: HUB_CONFIG_PATH : Path to the configuration file (default: \"hub_config.yaml\") MLFLOW_URI : URI for MLflow tracking server RABBIT_HOST : Hostname for RabbitMQ server RABBIT_PORT : Port for RabbitMQ server RABBIT_USER : Username for RabbitMQ server RABBIT_PASSWORD : Password for RabbitMQ server If the configuration file specified by HUB_CONFIG_PATH exists, it will be used. Otherwise, the ExperimentHub will be initialized using environment variables.","title":"Environment Variables"},{"location":"configuration/#configuration-best-practices","text":"","title":"Configuration Best Practices"},{"location":"configuration/#separate-configuration-by-environment","text":"Create separate configuration files for different environments: hub_config.dev.yaml : Development environment hub_config.test.yaml : Testing environment hub_config.prod.yaml : Production environment","title":"Separate Configuration by Environment"},{"location":"configuration/#use-environment-variables-for-sensitive-information","text":"Use environment variables for sensitive information like passwords and API keys: plugins: datastream: enabled: true type: \"rabbitmq\" config: host: \"localhost\" port: 5672 username: \"${RABBIT_USER}\" password: \"${RABBIT_PASSWORD}\"","title":"Use Environment Variables for Sensitive Information"},{"location":"configuration/#document-configuration-parameters","text":"Document all configuration parameters, including their purpose, allowed values, and default values.","title":"Document Configuration Parameters"},{"location":"configuration/#validate-configuration","text":"Validate the configuration file before using it: def validate_config(config): \"\"\"Validate the configuration file\"\"\" # Check required sections required_sections = [\"version\", \"plugins\", \"experiments\"] for section in required_sections: if section not in config: raise ValueError(f\"Missing required section: {section}\") # Check plugin configurations for plugin_type, plugin_config in config[\"plugins\"].items(): if \"enabled\" not in plugin_config: raise ValueError(f\"Missing 'enabled' field in {plugin_type} plugin configuration\") if plugin_config[\"enabled\"] and \"type\" not in plugin_config: raise ValueError(f\"Missing 'type' field in {plugin_type} plugin configuration\") if plugin_config[\"enabled\"] and \"config\" not in plugin_config: raise ValueError(f\"Missing 'config' field in {plugin_type} plugin configuration\") # Check experiment configurations for i, exp_config in enumerate(config[\"experiments\"]): if \"name\" not in exp_config: raise ValueError(f\"Missing 'name' field in experiment configuration at index {i}\") if \"module\" not in exp_config: raise ValueError(f\"Missing 'module' field in experiment configuration at index {i}\") if \"class\" not in exp_config: raise ValueError(f\"Missing 'class' field in experiment configuration at index {i}\")","title":"Validate Configuration"},{"location":"configuration/#use-default-values","text":"Provide default values for optional configuration parameters: def get_config_value(config, path, default=None): \"\"\"Get a value from the configuration, with a default value\"\"\" parts = path.split(\".\") current = config for part in parts: if part not in current: return default current = current[part] return current","title":"Use Default Values"},{"location":"configuration/#example-configurations","text":"","title":"Example Configurations"},{"location":"configuration/#development-configuration","text":"# hub_config.dev.yaml version: \"1.0\" global: log_level: \"debug\" environment: \"development\" plugins: mlops: enabled: true type: \"mlflow\" config: mlflow_uri: \"http://localhost:5000\" datastream: enabled: true type: \"rabbitmq\" config: host: \"localhost\" port: 5672 username: \"guest\" password: \"guest\" experiments: - name: \"time_series\" module: \"framework.TimeSeriesAnalysis\" class: \"TimeSeriesExperiment\" enabled: true - name: \"fault_detection\" module: \"framework.FaultDetection\" class: \"FaultDetectionExperiment\" enabled: true","title":"Development Configuration"},{"location":"configuration/#production-configuration","text":"# hub_config.prod.yaml version: \"1.0\" global: log_level: \"info\" environment: \"production\" plugins: mlops: enabled: true type: \"mlflow\" config: mlflow_uri: \"http://mlflow.example.com\" datastream: enabled: true type: \"rabbitmq\" config: host: \"rabbitmq.example.com\" port: 5672 username: \"${RABBIT_USER}\" password: \"${RABBIT_PASSWORD}\" experiments: - name: \"time_series\" module: \"framework.TimeSeriesAnalysis\" class: \"TimeSeriesExperiment\" enabled: true - name: \"fault_detection\" module: \"framework.FaultDetection\" class: \"FaultDetectionExperiment\" enabled: true - name: \"fault_isolation\" module: \"framework.FaultIsolation\" class: \"FaultIsolationExperiment\" enabled: true - name: \"process_mining\" module: \"framework.ProcessMining\" class: \"ProcessMiningExperiment\" enabled: true","title":"Production Configuration"},{"location":"configuration/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"configuration/#configuration-file-not-found","text":"If the configuration file is not found, the ExperimentHub will fall back to using environment variables. Make sure the configuration file exists and is accessible.","title":"Configuration File Not Found"},{"location":"configuration/#invalid-configuration","text":"If the configuration file is invalid, the ExperimentHub will raise an error. Check the error message for details on what's wrong with the configuration.","title":"Invalid Configuration"},{"location":"configuration/#plugin-initialization-errors","text":"If a plugin fails to initialize, check the plugin configuration and make sure all required parameters are provided.","title":"Plugin Initialization Errors"},{"location":"experiments/","text":"Experiments Experiments are the basic building blocks of the ExperimentHub. It is a Protocol class and can be used freely to derive an Experiment, such as the ones below. For specific goals and roles, we recommend buildign custom experiments, which require 2 important things: the configuration files, and the models Bases: Protocol A class representing an experiment. This class is a protocol that defines the methods that an experiment should implement. Some functions must be overloaded by an implementation class, while others can be left as is. Can only handle sklearn.BaseEstimator class, and so the used model must be overloaded Parameters: mlflow_uri ( str ) \u2013 the URI of the MLflow server model ( any ) \u2013 the model to be used in the experiment data ( DataFrame ) \u2013 the training data -- allocated during training new_data ( DataFrame ) \u2013 data that is not in the training set -- incoming data in prediction prediction ( DataFrame ) \u2013 the prediction of the new data metrics ( DataFrame ) \u2013 the metrics of the model cfg ( dict ) \u2013 the configuration of the experiment _model_registry ( dict [ str , type ] ) \u2013 a registry of model classes _report_registry ( dict [ str , Figure ] ) \u2013 a registry of reports run_id ( str ) \u2013 the ID of the run experiment_id ( str ) \u2013 the ID of the experiment name ( str ) \u2013 the name of the experiment eda_report ( str ) \u2013 the EDA report TODO: - proper logging - Add mmlw_estimator as a base class for the model - SPC chart - add support for data drift (eda) - add support for model drift (eda) - add support for model explainability (shap, lime, etc.) - add support for general data preprocessing (e.g. missing values, outliers, etc.) - add support for dim reduction (PCA, TSNE, etc.) - add support for feature selection (RFE, etc.) - dynamic model loading from specific folder/registry Source code in framework\\Experiment.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 class Experiment ( Protocol ): \"\"\"A class representing an experiment. This class is a protocol that defines the methods that an experiment should implement. Some functions must be overloaded by an implementation class, while others can be left as is. Can only handle sklearn.BaseEstimator class, and so the used model must be overloaded Parameters: mlflow_uri (str): the URI of the MLflow server model (any): the model to be used in the experiment data (pd.DataFrame): the training data -- allocated during training new_data (pd.DataFrame): data that is not in the training set -- incoming data in prediction prediction (pd.DataFrame): the prediction of the new data metrics (pd.DataFrame): the metrics of the model cfg (dict): the configuration of the experiment _model_registry (dict[str, type]): a registry of model classes _report_registry (dict[str, go.Figure]): a registry of reports run_id (str): the ID of the run experiment_id (str): the ID of the experiment name (str): the name of the experiment eda_report (str): the EDA report TODO: - proper logging - Add mmlw_estimator as a base class for the model - SPC chart - add support for data drift (eda) - add support for model drift (eda) - add support for model explainability (shap, lime, etc.) - add support for general data preprocessing (e.g. missing values, outliers, etc.) - add support for dim reduction (PCA, TSNE, etc.) - add support for feature selection (RFE, etc.) - dynamic model loading from specific folder/registry \"\"\" mlflow_uri : str = None model : any = None data : pd . DataFrame = None new_data : pd . DataFrame = pd . DataFrame ([]) metrics : pd . DataFrame = None cfg : dict = None _model_registry : dict [ str , type ] = dict () _report_registry : dict [ str , go . Figure ] = dict () run_id : str = None experiment_id : str = None name : str = \"\" _eda_registry : dict [ str , go . Figure ] = dict () scheme = None ###retrain_window : int, metric : str, metric_threshold : float, higher_better : bool = False retrain_window : int = 0 metric : str = None metric_threshold : float = 0.0 higher_better : bool = False data_format : dict = None def __init__ ( self , cfg : dict , experiment_id : str , run_id : str ) -> None : \"\"\"Initialize the Experiment class. This class is a protocol that defines the methods that an experiment should implement. Parameters: cfg (dict): the configuration of the experiment experiment_id (str): the ID of the experiment run_id (str): the ID of the run \"\"\" self . cfg = cfg self . experiment_id = experiment_id self . run_id = run_id def format_data ( self , data , format : dict = None ) -> pd . DataFrame : \"\"\"This function will provide a function to recover data from nonstandard json as pd.Dataframe. Parameters: data : data in json format or DataFrame format (dict) : settings for formatting. If None, then default pd.read_json is used.\"\"\" if format is None : if isinstance ( data , pd . DataFrame ): return data else : from io import StringIO data = pd . read_json ( StringIO ( data )) return data else : if format . get ( \"name\" , \"pivot\" ) == \"pivot\" : data = json . loads ( data ) _id = format . get ( \"id\" , \"tsdata\" ) mxlvl = format . get ( \"max_level\" , 1 ) data = pd . json_normalize ( data [ _id ], max_level = 1 ) column = format . get ( \"columns\" , \"target\" ) ind = format . get ( \"index\" , \"date\" ) vals = format . get ( \"values\" , \"value\" ) data = data . pivot ( columns = column , index = ind , values = vals ) data . columns . name = None data . reset_index ( drop = False , inplace = True ) return data else : raise Exception ( f \"Configuration contains faulty data formatting settings. Please make sure the setup keyword contaisn the necessary information. Settings are: { format } \" ) ###ADD Overloadable methods for Experiment class. def setup ( self , data : pd . DataFrame , * args , ** kwargs ): \"\"\"This function sets up the experiment. It is called before training the model.\"\"\" return NotImplementedError ( \"Implement this in the child class.\" ) def create_model ( self , * args , ** kwargs ): \"\"\"This function trains the model.\"\"\" return NotImplementedError ( \"Implement this in the child class.\" ) def predict ( self , data , * args , ** kwargs ): \"\"\"This function predicts the target variable.\"\"\" return NotImplementedError ( \"Implement this in the child class.\" ) def _score ( self , y , y_hat ) -> pd . DataFrame : \"\"\"This function calculates the metrics of the model.\"\"\" return NotImplementedError ( \"Implement this in the child class.\" ) def plot_model ( self , plot : str ) -> go . Figure : \"\"\"This function plots the model. Parameters: plot (str): the plot name that is to be displayed Returns: go.Figure: the plot \"\"\" if plot not in self . model . _figs : raise ValueError ( f \"Plot { plot } not found in model.\" ) return self . model . _figs [ plot ] def save ( self ): \"\"\"This function saves the experiment. It saves the model and the reports. Uses joblib to save the model and pickle to save the reports. \"\"\" with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : repository = get_artifact_repository ( run . info . artifact_uri ) try : repository . delete_artifacts ( mlflow . get_artifact_uri ( \"experiment.pkl\" )) repository . delete_artifacts ( mlflow . get_artifact_uri ( \"metadata.yaml\" )) repository . delete_artifacts ( mlflow . get_artifact_uri ( \"reports\" )) except : pass experiment_to_be_saved = deepcopy ( self ) if isinstance ( self . model , tf . keras . models . Model ): self . model . save ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"model.keras\" )) experiment_to_be_saved . model = None with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"model.keras\" ), \"model\" ) with open ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"experiment.pkl\" ), \"wb\" ) as f : joblib . dump ( experiment_to_be_saved , f ) for k , v in self . _report_registry . items (): with open ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } .json\" ), \"w\" ) as f : write_json ( v , f ) #os.path.join(os.getcwd(), \"runs\", self.run_id, \"reports\", f\"{k}.json\") with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } .json\" ), \"reports\" ) for k , v in self . _eda_registry . items (): with open ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \"eda_ { k } .json\" ), \"w\" ) as f : write_json ( v , f ) with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \"eda_ { k } .json\" ), \"reports\" ) yaml . dump ( self . cfg , open ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"metadata.yaml\" ), \"w\" )) with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"experiment.pkl\" ), \"\" ) mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"metadata.yaml\" ), \"\" ) def load ( self , run_id : str ) -> Experiment : \"\"\"This function loads the experiment. It loads the model and the reports. Uses joblib to load the model and pickle to load the reports. Parameters: run_id (str): the ID of the run Returns: Experiment: the experiment \"\"\" dst = os . path . join ( os . getcwd (), \"runs\" , run_id ) mlflow . artifacts . download_artifacts ( artifact_uri = f \"runs:/ { run_id } /experiment.pkl\" , dst_path = dst ) exp = joblib . load ( os . path . join ( dst , \"experiment.pkl\" )) if os . path . exists ( os . path . join ( dst , \"model.keras\" )): exp . model = tf . keras . models . load_model ( os . path . join ( dst , \"model.keras\" )) mlflow . artifacts . download_artifacts ( artifact_uri = f \"runs:/ { run_id } /reports\" , dst_path = dst ) report_dir = os . path . join ( dst , \"reports\" ) for file in os . listdir ( report_dir ): if file . endswith ( \".json\" ) and file . startswith ( \"eda_\" ): with open ( os . path . join ( report_dir , file ), \"rb\" ) as f : report = read_json ( f ) key = file [ 4 :] . split ( \".\" )[ 0 ] exp . _eda_registry [ key ] = report elif file . endswith ( \".json\" ): with open ( os . path . join ( report_dir , file ), \"rb\" ) as f : report = read_json ( f ) exp . _report_registry [ file . split ( \".\" )[ 0 ]] = report ##find .pkl files in report exp . model . _figs = exp . _report_registry exp . cfg = self . cfg exp . experiment_id = self . experiment_id exp . run_id = run_id ## load _report_registry ## assing it to model #exp.model = mlflow.sklearn.load_model(f\"runs:/{run_id}/model\") return exp def convert_datetime ( self , data : pd . DataFrame , format : str ): # TODO: get and return column only... if np . issubdtype ( data [ self . ds ] . dtype , np . integer ): data [ self . ds ] = pd . to_datetime ( data [ self . ds ], unit = format ) #, format = self.format .dt.strftime('%Y-%m-%d %H:%M:%S. %s') .astype(str), format = self.format elif data [ self . ds ] . dtype == str : data [ self . ds ] = pd . to_datetime ( data [ self . ds ], format = format ) #, format = self.format .dt.strftime('%Y-%m-%d %H:%M:%S. %s') .astype(str), format = self.format else : logging . warn ( f \"Unknown type { data [ self . ds ] . dtype } \" ) data [ self . ds ] = pd . to_datetime ( data [ self . ds ]) return data def eda ( self ): # interactions = None, \"\"\"This function performs simple exploratory data analysis. Returns: None \"\"\" print ( \"Performing EDA...\" ) ## add histograms self . _eda_registry = dict () for col in self . data . columns : fig = go . Figure () fig . add_trace ( go . Histogram ( x = self . data [ col ], name = col )) fig . update_layout ( title_text = f \" { col } Histogram\" ) fig . update_xaxes ( title_text = col ) fig . update_yaxes ( title_text = \"Count\" ) self . _eda_registry [ col ] = fig ## add correlation matrix labels = self . data . columns . to_list () labels . reverse () fig = go . Figure ( data = go . Heatmap ( z = self . data . corr (), x = labels , y = labels )) self . _eda_registry [ \"correlation_matrix\" ] = fig ## add missing values #fig = go.Figure(data=go.Heatmap(z=self.data.isnull().sum(), x=self.data.columns, y=self.data.columns)) #self._eda_registry[\"missing_values\"] = fig def retrain ( self ): if self . metric is not None : if self . higher_better : print ( self . metrics . iloc [ - 1 ] . loc [ self . metric ]) return self . metrics . iloc [ - 1 ] . loc [ self . metric ] < self . metric_threshold return self . metrics . iloc [ - 1 ] . loc [ self . metric ] > self . metric_threshold return False def spc ( self ): \"\"\"This function creates a statistical process control chart. WIP.\"\"\" pass def run ( self , data : pd . DataFrame ) -> None : \"\"\"This function runs the experiment. It trains the model and performs exploratory data analysis. Handles everythin internally... Parameters: data (pd.DataFrame): the training data \"\"\" if data is None : raise ValueError ( \"Data not found. Please provide data for training.\" ) if self . cfg is None : raise ValueError ( \"No training configuration found in metadata. The interface is not properly configured.\" ) ## for k in config run the function, with specified parameters... funcs = list ( self . cfg . keys ()) funcs . remove ( \"load_object\" ) self . data = data for k in funcs : if k == \"setup\" : self . setup ( data ) #self.eda( ) else : getattr ( self , k )() #self.setup(data, experiment_id, run_id) #self.model = self.create_model() #self.eda() # -> add eda to figs #self.spc() # -> add spc to figs #self.spc_chart() return None def join_data ( self ): \"\"\"Joins new data and previous train data.\"\"\" ndata = pd . concat (( self . data , self . new_data ), axis = 0 ) if self . retrain_window != 0 and self . retrain_window < ndata . shape [ 0 ]: ndata = ndata . iloc [ - self . retrain_window :, :] return ndata def export ( self ) -> None : \"\"\"This function exports the reports as HTML files and logs them to MLflow.\"\"\" with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : for k , v in self . _eda_registry . items (): v . write_html ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \"eda_ { k } _report.html\" )) mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \"eda_ { k } _report.html\" ), \"reports\" ) for k , v in self . model . _figs . items (): v . write_html ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } _report.html\" )) mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } _report.html\" ), \"reports\" ) def get_eda_reports ( self ) -> list [ str ]: \"\"\"This function returns the IDs of the EDA reports that are available in the model. Returns: list[str]: the IDs of the reports \"\"\" return list ( self . _eda_registry . keys ()) def get_fig_types ( self ) -> list [ str ]: \"\"\"This function returns the IDs of figures that are available in the model. Returns: list[str]: the IDs of figures \"\"\" return list ( self . model . _figs . keys ()) __init__ ( cfg , experiment_id , run_id ) Initialize the Experiment class. This class is a protocol that defines the methods that an experiment should implement. Parameters: cfg ( dict ) \u2013 the configuration of the experiment experiment_id ( str ) \u2013 the ID of the experiment run_id ( str ) \u2013 the ID of the run Source code in framework\\Experiment.py 75 76 77 78 79 80 81 82 83 84 85 86 def __init__ ( self , cfg : dict , experiment_id : str , run_id : str ) -> None : \"\"\"Initialize the Experiment class. This class is a protocol that defines the methods that an experiment should implement. Parameters: cfg (dict): the configuration of the experiment experiment_id (str): the ID of the experiment run_id (str): the ID of the run \"\"\" self . cfg = cfg self . experiment_id = experiment_id self . run_id = run_id create_model ( * args , ** kwargs ) This function trains the model. Source code in framework\\Experiment.py 122 123 124 def create_model ( self , * args , ** kwargs ): \"\"\"This function trains the model.\"\"\" return NotImplementedError ( \"Implement this in the child class.\" ) eda () This function performs simple exploratory data analysis. Returns: \u2013 None Source code in framework\\Experiment.py 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 def eda ( self ): # interactions = None, \"\"\"This function performs simple exploratory data analysis. Returns: None \"\"\" print ( \"Performing EDA...\" ) ## add histograms self . _eda_registry = dict () for col in self . data . columns : fig = go . Figure () fig . add_trace ( go . Histogram ( x = self . data [ col ], name = col )) fig . update_layout ( title_text = f \" { col } Histogram\" ) fig . update_xaxes ( title_text = col ) fig . update_yaxes ( title_text = \"Count\" ) self . _eda_registry [ col ] = fig ## add correlation matrix labels = self . data . columns . to_list () labels . reverse () fig = go . Figure ( data = go . Heatmap ( z = self . data . corr (), x = labels , y = labels )) self . _eda_registry [ \"correlation_matrix\" ] = fig export () This function exports the reports as HTML files and logs them to MLflow. Source code in framework\\Experiment.py 332 333 334 335 336 337 338 339 340 341 def export ( self ) -> None : \"\"\"This function exports the reports as HTML files and logs them to MLflow.\"\"\" with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : for k , v in self . _eda_registry . items (): v . write_html ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \"eda_ { k } _report.html\" )) mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \"eda_ { k } _report.html\" ), \"reports\" ) for k , v in self . model . _figs . items (): v . write_html ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } _report.html\" )) mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } _report.html\" ), \"reports\" ) format_data ( data , format = None ) This function will provide a function to recover data from nonstandard json as pd.Dataframe. Parameters: data : data in json format or DataFrame format (dict) : settings for formatting. If None, then default pd.read_json is used. Source code in framework\\Experiment.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def format_data ( self , data , format : dict = None ) -> pd . DataFrame : \"\"\"This function will provide a function to recover data from nonstandard json as pd.Dataframe. Parameters: data : data in json format or DataFrame format (dict) : settings for formatting. If None, then default pd.read_json is used.\"\"\" if format is None : if isinstance ( data , pd . DataFrame ): return data else : from io import StringIO data = pd . read_json ( StringIO ( data )) return data else : if format . get ( \"name\" , \"pivot\" ) == \"pivot\" : data = json . loads ( data ) _id = format . get ( \"id\" , \"tsdata\" ) mxlvl = format . get ( \"max_level\" , 1 ) data = pd . json_normalize ( data [ _id ], max_level = 1 ) column = format . get ( \"columns\" , \"target\" ) ind = format . get ( \"index\" , \"date\" ) vals = format . get ( \"values\" , \"value\" ) data = data . pivot ( columns = column , index = ind , values = vals ) data . columns . name = None data . reset_index ( drop = False , inplace = True ) return data else : raise Exception ( f \"Configuration contains faulty data formatting settings. Please make sure the setup keyword contaisn the necessary information. Settings are: { format } \" ) get_eda_reports () This function returns the IDs of the EDA reports that are available in the model. Returns: list [ str ] \u2013 list[str]: the IDs of the reports Source code in framework\\Experiment.py 343 344 345 346 347 348 349 def get_eda_reports ( self ) -> list [ str ]: \"\"\"This function returns the IDs of the EDA reports that are available in the model. Returns: list[str]: the IDs of the reports \"\"\" return list ( self . _eda_registry . keys ()) get_fig_types () This function returns the IDs of figures that are available in the model. Returns: list [ str ] \u2013 list[str]: the IDs of figures Source code in framework\\Experiment.py 351 352 353 354 355 356 357 def get_fig_types ( self ) -> list [ str ]: \"\"\"This function returns the IDs of figures that are available in the model. Returns: list[str]: the IDs of figures \"\"\" return list ( self . model . _figs . keys ()) join_data () Joins new data and previous train data. Source code in framework\\Experiment.py 323 324 325 326 327 328 329 330 def join_data ( self ): \"\"\"Joins new data and previous train data.\"\"\" ndata = pd . concat (( self . data , self . new_data ), axis = 0 ) if self . retrain_window != 0 and self . retrain_window < ndata . shape [ 0 ]: ndata = ndata . iloc [ - self . retrain_window :, :] return ndata load ( run_id ) This function loads the experiment. It loads the model and the reports. Uses joblib to load the model and pickle to load the reports. Parameters: run_id ( str ) \u2013 the ID of the run Returns: Experiment ( Experiment ) \u2013 the experiment Source code in framework\\Experiment.py 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 def load ( self , run_id : str ) -> Experiment : \"\"\"This function loads the experiment. It loads the model and the reports. Uses joblib to load the model and pickle to load the reports. Parameters: run_id (str): the ID of the run Returns: Experiment: the experiment \"\"\" dst = os . path . join ( os . getcwd (), \"runs\" , run_id ) mlflow . artifacts . download_artifacts ( artifact_uri = f \"runs:/ { run_id } /experiment.pkl\" , dst_path = dst ) exp = joblib . load ( os . path . join ( dst , \"experiment.pkl\" )) if os . path . exists ( os . path . join ( dst , \"model.keras\" )): exp . model = tf . keras . models . load_model ( os . path . join ( dst , \"model.keras\" )) mlflow . artifacts . download_artifacts ( artifact_uri = f \"runs:/ { run_id } /reports\" , dst_path = dst ) report_dir = os . path . join ( dst , \"reports\" ) for file in os . listdir ( report_dir ): if file . endswith ( \".json\" ) and file . startswith ( \"eda_\" ): with open ( os . path . join ( report_dir , file ), \"rb\" ) as f : report = read_json ( f ) key = file [ 4 :] . split ( \".\" )[ 0 ] exp . _eda_registry [ key ] = report elif file . endswith ( \".json\" ): with open ( os . path . join ( report_dir , file ), \"rb\" ) as f : report = read_json ( f ) exp . _report_registry [ file . split ( \".\" )[ 0 ]] = report ##find .pkl files in report exp . model . _figs = exp . _report_registry exp . cfg = self . cfg exp . experiment_id = self . experiment_id exp . run_id = run_id ## load _report_registry ## assing it to model #exp.model = mlflow.sklearn.load_model(f\"runs:/{run_id}/model\") return exp plot_model ( plot ) This function plots the model. Parameters: plot ( str ) \u2013 the plot name that is to be displayed Returns: Figure \u2013 go.Figure: the plot Source code in framework\\Experiment.py 134 135 136 137 138 139 140 141 142 143 144 145 def plot_model ( self , plot : str ) -> go . Figure : \"\"\"This function plots the model. Parameters: plot (str): the plot name that is to be displayed Returns: go.Figure: the plot \"\"\" if plot not in self . model . _figs : raise ValueError ( f \"Plot { plot } not found in model.\" ) return self . model . _figs [ plot ] predict ( data , * args , ** kwargs ) This function predicts the target variable. Source code in framework\\Experiment.py 126 127 128 def predict ( self , data , * args , ** kwargs ): \"\"\"This function predicts the target variable.\"\"\" return NotImplementedError ( \"Implement this in the child class.\" ) run ( data ) This function runs the experiment. It trains the model and performs exploratory data analysis. Handles everythin internally... Parameters: data ( DataFrame ) \u2013 the training data Source code in framework\\Experiment.py 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 def run ( self , data : pd . DataFrame ) -> None : \"\"\"This function runs the experiment. It trains the model and performs exploratory data analysis. Handles everythin internally... Parameters: data (pd.DataFrame): the training data \"\"\" if data is None : raise ValueError ( \"Data not found. Please provide data for training.\" ) if self . cfg is None : raise ValueError ( \"No training configuration found in metadata. The interface is not properly configured.\" ) ## for k in config run the function, with specified parameters... funcs = list ( self . cfg . keys ()) funcs . remove ( \"load_object\" ) self . data = data for k in funcs : if k == \"setup\" : self . setup ( data ) #self.eda( ) else : getattr ( self , k )() #self.setup(data, experiment_id, run_id) #self.model = self.create_model() #self.eda() # -> add eda to figs #self.spc() # -> add spc to figs #self.spc_chart() return None save () This function saves the experiment. It saves the model and the reports. Uses joblib to save the model and pickle to save the reports. Source code in framework\\Experiment.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 def save ( self ): \"\"\"This function saves the experiment. It saves the model and the reports. Uses joblib to save the model and pickle to save the reports. \"\"\" with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : repository = get_artifact_repository ( run . info . artifact_uri ) try : repository . delete_artifacts ( mlflow . get_artifact_uri ( \"experiment.pkl\" )) repository . delete_artifacts ( mlflow . get_artifact_uri ( \"metadata.yaml\" )) repository . delete_artifacts ( mlflow . get_artifact_uri ( \"reports\" )) except : pass experiment_to_be_saved = deepcopy ( self ) if isinstance ( self . model , tf . keras . models . Model ): self . model . save ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"model.keras\" )) experiment_to_be_saved . model = None with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"model.keras\" ), \"model\" ) with open ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"experiment.pkl\" ), \"wb\" ) as f : joblib . dump ( experiment_to_be_saved , f ) for k , v in self . _report_registry . items (): with open ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } .json\" ), \"w\" ) as f : write_json ( v , f ) #os.path.join(os.getcwd(), \"runs\", self.run_id, \"reports\", f\"{k}.json\") with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } .json\" ), \"reports\" ) for k , v in self . _eda_registry . items (): with open ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \"eda_ { k } .json\" ), \"w\" ) as f : write_json ( v , f ) with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \"eda_ { k } .json\" ), \"reports\" ) yaml . dump ( self . cfg , open ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"metadata.yaml\" ), \"w\" )) with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"experiment.pkl\" ), \"\" ) mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"metadata.yaml\" ), \"\" ) setup ( data , * args , ** kwargs ) This function sets up the experiment. It is called before training the model. Source code in framework\\Experiment.py 118 119 120 def setup ( self , data : pd . DataFrame , * args , ** kwargs ): \"\"\"This function sets up the experiment. It is called before training the model.\"\"\" return NotImplementedError ( \"Implement this in the child class.\" ) spc () This function creates a statistical process control chart. WIP. Source code in framework\\Experiment.py 285 286 287 def spc ( self ): \"\"\"This function creates a statistical process control chart. WIP.\"\"\" pass FaultIsolationExperiment Bases: Experiment This experiment file is used for fault isolation experiments. It is a subclass of the Experiment class in the framework.Experiment module. It is used to create, train, and predict using fault isolation models. Heavily relies on classification algorithms. Implemented models: Decision Tree : 'models.fault_isolation.DecisionTree' Random Forest : 'models.fault_isolation.RandomForest' Naive Bayes : 'models.fault_isolation.NaiveBayes' HMM : 'models.fault_isolation.HMM' Markov Chain : 'models.fault_isolation.MarkovChain' Inherits from Experiment Protocol class from framework.Experiment. If the experiment is overloaded, and new functions are added, one can call it in the system by adding the function name to the cfg file with relevant parameters. For example, if a new function 'new_function' is added to the system, the cfg file should have the following structure: cfg file should have the following structure: load_object: module: framework.FaultIsolation name: FaultIsolationExperiment setup: datetime_column: str The column that contains the datetime information. target: str The target column for the experiment. (output column) ... eda: create_model: model: str The model to be used for training. params: dict The parameters to be used for the model. new_function: param1: str The first parameter for the function. param2: str The second parameter for the function. ... Source code in framework\\FaultIsolation.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 class FaultIsolationExperiment ( Experiment ): \"\"\"This experiment file is used for fault isolation experiments. It is a subclass of the Experiment class in the framework.Experiment module. It is used to create, train, and predict using fault isolation models. Heavily relies on classification algorithms. Implemented models: - Decision Tree : 'models.fault_isolation.DecisionTree' - Random Forest : 'models.fault_isolation.RandomForest' - Naive Bayes : 'models.fault_isolation.NaiveBayes' - HMM : 'models.fault_isolation.HMM' - Markov Chain : 'models.fault_isolation.MarkovChain' Inherits from Experiment Protocol class from framework.Experiment. If the experiment is overloaded, and new functions are added, one can call it in the system by adding the function name to the cfg file with relevant parameters. For example, if a new function 'new_function' is added to the system, the cfg file should have the following structure: cfg file should have the following structure: ``` load_object: module: framework.FaultIsolation name: FaultIsolationExperiment setup: datetime_column: str The column that contains the datetime information. target: str The target column for the experiment. (output column) ... eda: create_model: model: str The model to be used for training. params: dict The parameters to be used for the model. new_function: param1: str The first parameter for the function. param2: str The second parameter for the function. ... ``` \"\"\" def __init__ ( self , cfg : dict , experiment_id : str , run_id : str , * args , ** kwargs ) -> None : \"\"\"Initialize the model registry. Currently cannot be changed after initialization. In future versions, we will allow for dynamic model loading through configuration files. Parameters: cfg (dict): The configuration file for the experiment. experiment_id (str): The experiment id for the experiment. run_id (str): The run id for the experiment. \"\"\" self . cfg = cfg self . experiment_id = experiment_id self . run_id = run_id self . _model_registry [ \"dt\" ] = DecisionTreeModel # decision tree self . _model_registry [ \"rf\" ] = RandomForestModel # random forest self . _model_registry [ \"nb\" ] = NaiveBayesModel # Naive Bayes self . _model_registry [ \"bn\" ] = BayesNet # Bayes Net! self . _model_registry [ \"hmm\" ] = HMM # hidden markov model self . _model_registry [ \"mc\" ] = MarkovChainModel # Markov Chain def setup ( self , data : str ) -> pd . DataFrame : \"\"\"Setup the data for training and prediction. This function is called before training the model. Parameters: data (str): The data to be used for training and prediction in json format. Returns: data (pd.DataFrame): The data set up for training and prediction. \"\"\" cfg = self . cfg [ \"setup\" ] self . data_format = cfg . get ( \"format\" , None ) data = self . format_data ( data , self . data_format ) self . data = data print ( self . data ) self . ds = cfg . get ( \"datetime_column\" , None ) self . format = cfg . get ( \"datetime_format\" , None ) self . predict_window = cfg . get ( \"predict_window\" , 0 ) if self . predict_window is None : self . predict_window = 0 if self . predict_window < 0 : self . predict_window *= - 1 retrain_cfg = cfg . get ( \"retrain\" , None ) if retrain_cfg is None or len ( retrain_cfg ) == 0 : self . retrain_window = 0 self . metric = None self . metric_threshold = 0.0 self . higher_better = True else : self . retrain_window = retrain_cfg . get ( \"retrain_window\" , 0 ) self . metric = retrain_cfg . get ( \"metric\" , None ) self . metric_threshold = retrain_cfg . get ( \"metric_threshold\" , 0.0 ) self . higher_better = retrain_cfg . get ( \"higher_better\" , True ) self . target = cfg [ \"target\" ] # self.target = self.target.replace(\" \", \"_\") # replace spaces with underscores # self.target = self.target.replace(\"//\", \"\") # self.target = self.target.replace(\"/\", \"\") # self.target = self.target.replace(\"(\", \"\") # self.target = self.target.replace(\")\", \"\") # self.target = self.target.replace(\"\\\\\", \"\") # self.target = self.target.replace(\".\", \"\") ## TODO: Abstrct data VC -- integrate dvc or other data versioning tools. if self . ds is not None : self . data = self . convert_datetime ( self . data , format = self . format ) self . data . set_index ( self . ds , inplace = True ) self . data . rename ( columns = { self . target : \"target\" }, inplace = True ) self . data [ \"target\" ] = self . data [ \"target\" ] . astype ( \"category\" ) self . input_scheme = self . data . columns . to_list () self . input_scheme . remove ( \"target\" ) return data def create_model ( self , * args , ** kwargs ) -> any : \"\"\"Create the model using the configuration file. The model is trained on the data set up in the 'setup' function. Returns: (any) The trained model.\"\"\" model = self . cfg [ \"create_model\" ][ \"model\" ] params = self . cfg [ \"create_model\" ] . get ( \"params\" , None ) if params is None : params = dict () if self . data is None : raise ValueError ( \"Data not found. Please use 'setup' to setup the data first.\" ) try : model_class = self . _model_registry [ model ] except KeyError : raise ValueError ( f \"Model { model } not found in model registry. Please check configuration file. Available models are { self . _model_registry . keys () } .\" ) self . model = model_class ( ** params ) . fit ( self . data ) self . _report_registry = self . model . _figs y = self . model . predict ( self . data ) try : self . metrics = self . _score ( self . data [ \"target\" ], y [ \"y_pred\" ]) self . _report_registry [ \"metrics\" ] = px . line ( self . metrics , x = self . metrics . index , y = self . metrics . columns , markers = True ) except Exception as e : self . metrics = pd . DataFrame ([ np . NaN , np . NaN , np . NaN , np . NaN ], index = [ \"Accuracy\" , \"F1\" , \"Precision\" , \"Recall\" ]) . transpose () logging . warn ( f \"Error calculating metrics: { e } . Model { self . model . __class__ . __name__ } may not have y_pred as output.\" ) with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : mlflow . set_tag ( \"model\" , model ) for k , v in self . model . _figs . items (): v . write_html ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } .html\" )) mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } .html\" ), \"reports\" ) for k , v in params . items (): mlflow . log_param ( k , v ) return self . model def predict ( self , data : str ): \"\"\"Predict using the trained model. The model should be trained before calling this function. Parameters: data (str): The data to be used for prediction in json format. Returns: (pd.DataFrame) The predictions made by the model.\"\"\" data = self . format_data ( data , self . data_format ) if self . model is None : raise ValueError ( \"Model not found. Please train the model first with 'create_model'.\" ) X = data . copy () if self . ds is not None : X = self . convert_datetime ( X , format = self . format ) X . set_index ( self . ds , inplace = True ) if self . target in X . columns : X . rename ( columns = { self . target : \"target\" }, inplace = True ) y = self . model . predict ( X ) metrics = self . _score ( X [ \"target\" ], y [ \"y_pred\" ]) self . metrics = pd . concat ([ self . metrics , metrics ], axis = 0 , ignore_index = True ) self . new_data = pd . concat ([ self . new_data , y ], axis = 0 ) if self . predict_window > self . new_data . shape [ 0 ] : self . model . update_predict ( self . new_data , reset_fig = True , update_fig = True ) else : self . model . update_predict ( self . new_data . iloc [ - self . predict_window :, :], reset_fig = True , update_fig = True ) self . _report_registry [ \"metrics\" ] = px . line ( self . metrics , x = self . metrics . index , y = self . metrics . columns , markers = True ) return y , metrics def _score ( self , y , y_hat ): \"\"\"Score the model using the predictions. This function is called after the predictions are made. Parameters: y (pd.Series): The actual values. y_hat (pd.Series): The predicted values. Returns: (pd.DataFrame) The metrics calculated for the model.\"\"\" if self . model is None : raise ValueError ( \"Model not found. Please train the model first with 'create_model'.\" ) if y is None : raise ValueError ( \"No data found for scoring. Please provide data for scoring.\" ) if y_hat is None : raise ValueError ( \"No predictions found for scoring. Please provide predictions for scoring.\" ) acc = accuracy_score ( y , y_hat ) f1 = f1_score ( y , y_hat ) prec = precision_score ( y , y_hat ) rec = recall_score ( y , y_hat ) return pd . DataFrame ([ acc , f1 , prec , rec ], index = [ \"Accuracy\" , \"F1\" , \"Precision\" , \"Recall\" ]) . transpose () __init__ ( cfg , experiment_id , run_id , * args , ** kwargs ) Initialize the model registry. Currently cannot be changed after initialization. In future versions, we will allow for dynamic model loading through configuration files. Parameters: cfg (dict): The configuration file for the experiment. experiment_id (str): The experiment id for the experiment. run_id (str): The run id for the experiment. Source code in framework\\FaultIsolation.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def __init__ ( self , cfg : dict , experiment_id : str , run_id : str , * args , ** kwargs ) -> None : \"\"\"Initialize the model registry. Currently cannot be changed after initialization. In future versions, we will allow for dynamic model loading through configuration files. Parameters: cfg (dict): The configuration file for the experiment. experiment_id (str): The experiment id for the experiment. run_id (str): The run id for the experiment. \"\"\" self . cfg = cfg self . experiment_id = experiment_id self . run_id = run_id self . _model_registry [ \"dt\" ] = DecisionTreeModel # decision tree self . _model_registry [ \"rf\" ] = RandomForestModel # random forest self . _model_registry [ \"nb\" ] = NaiveBayesModel # Naive Bayes self . _model_registry [ \"bn\" ] = BayesNet # Bayes Net! self . _model_registry [ \"hmm\" ] = HMM # hidden markov model self . _model_registry [ \"mc\" ] = MarkovChainModel # Markov Chain create_model ( * args , ** kwargs ) Create the model using the configuration file. The model is trained on the data set up in the 'setup' function. Returns: any \u2013 (any) The trained model. Source code in framework\\FaultIsolation.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def create_model ( self , * args , ** kwargs ) -> any : \"\"\"Create the model using the configuration file. The model is trained on the data set up in the 'setup' function. Returns: (any) The trained model.\"\"\" model = self . cfg [ \"create_model\" ][ \"model\" ] params = self . cfg [ \"create_model\" ] . get ( \"params\" , None ) if params is None : params = dict () if self . data is None : raise ValueError ( \"Data not found. Please use 'setup' to setup the data first.\" ) try : model_class = self . _model_registry [ model ] except KeyError : raise ValueError ( f \"Model { model } not found in model registry. Please check configuration file. Available models are { self . _model_registry . keys () } .\" ) self . model = model_class ( ** params ) . fit ( self . data ) self . _report_registry = self . model . _figs y = self . model . predict ( self . data ) try : self . metrics = self . _score ( self . data [ \"target\" ], y [ \"y_pred\" ]) self . _report_registry [ \"metrics\" ] = px . line ( self . metrics , x = self . metrics . index , y = self . metrics . columns , markers = True ) except Exception as e : self . metrics = pd . DataFrame ([ np . NaN , np . NaN , np . NaN , np . NaN ], index = [ \"Accuracy\" , \"F1\" , \"Precision\" , \"Recall\" ]) . transpose () logging . warn ( f \"Error calculating metrics: { e } . Model { self . model . __class__ . __name__ } may not have y_pred as output.\" ) with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : mlflow . set_tag ( \"model\" , model ) for k , v in self . model . _figs . items (): v . write_html ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } .html\" )) mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } .html\" ), \"reports\" ) for k , v in params . items (): mlflow . log_param ( k , v ) return self . model predict ( data ) Predict using the trained model. The model should be trained before calling this function. Parameters: data ( str ) \u2013 The data to be used for prediction in json format. Returns: \u2013 (pd.DataFrame) The predictions made by the model. Source code in framework\\FaultIsolation.py 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 def predict ( self , data : str ): \"\"\"Predict using the trained model. The model should be trained before calling this function. Parameters: data (str): The data to be used for prediction in json format. Returns: (pd.DataFrame) The predictions made by the model.\"\"\" data = self . format_data ( data , self . data_format ) if self . model is None : raise ValueError ( \"Model not found. Please train the model first with 'create_model'.\" ) X = data . copy () if self . ds is not None : X = self . convert_datetime ( X , format = self . format ) X . set_index ( self . ds , inplace = True ) if self . target in X . columns : X . rename ( columns = { self . target : \"target\" }, inplace = True ) y = self . model . predict ( X ) metrics = self . _score ( X [ \"target\" ], y [ \"y_pred\" ]) self . metrics = pd . concat ([ self . metrics , metrics ], axis = 0 , ignore_index = True ) self . new_data = pd . concat ([ self . new_data , y ], axis = 0 ) if self . predict_window > self . new_data . shape [ 0 ] : self . model . update_predict ( self . new_data , reset_fig = True , update_fig = True ) else : self . model . update_predict ( self . new_data . iloc [ - self . predict_window :, :], reset_fig = True , update_fig = True ) self . _report_registry [ \"metrics\" ] = px . line ( self . metrics , x = self . metrics . index , y = self . metrics . columns , markers = True ) return y , metrics setup ( data ) Setup the data for training and prediction. This function is called before training the model. Parameters: data (str): The data to be used for training and prediction in json format. Returns: data ( DataFrame ) \u2013 The data set up for training and prediction. Source code in framework\\FaultIsolation.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def setup ( self , data : str ) -> pd . DataFrame : \"\"\"Setup the data for training and prediction. This function is called before training the model. Parameters: data (str): The data to be used for training and prediction in json format. Returns: data (pd.DataFrame): The data set up for training and prediction. \"\"\" cfg = self . cfg [ \"setup\" ] self . data_format = cfg . get ( \"format\" , None ) data = self . format_data ( data , self . data_format ) self . data = data print ( self . data ) self . ds = cfg . get ( \"datetime_column\" , None ) self . format = cfg . get ( \"datetime_format\" , None ) self . predict_window = cfg . get ( \"predict_window\" , 0 ) if self . predict_window is None : self . predict_window = 0 if self . predict_window < 0 : self . predict_window *= - 1 retrain_cfg = cfg . get ( \"retrain\" , None ) if retrain_cfg is None or len ( retrain_cfg ) == 0 : self . retrain_window = 0 self . metric = None self . metric_threshold = 0.0 self . higher_better = True else : self . retrain_window = retrain_cfg . get ( \"retrain_window\" , 0 ) self . metric = retrain_cfg . get ( \"metric\" , None ) self . metric_threshold = retrain_cfg . get ( \"metric_threshold\" , 0.0 ) self . higher_better = retrain_cfg . get ( \"higher_better\" , True ) self . target = cfg [ \"target\" ] # self.target = self.target.replace(\" \", \"_\") # replace spaces with underscores # self.target = self.target.replace(\"//\", \"\") # self.target = self.target.replace(\"/\", \"\") # self.target = self.target.replace(\"(\", \"\") # self.target = self.target.replace(\")\", \"\") # self.target = self.target.replace(\"\\\\\", \"\") # self.target = self.target.replace(\".\", \"\") ## TODO: Abstrct data VC -- integrate dvc or other data versioning tools. if self . ds is not None : self . data = self . convert_datetime ( self . data , format = self . format ) self . data . set_index ( self . ds , inplace = True ) self . data . rename ( columns = { self . target : \"target\" }, inplace = True ) self . data [ \"target\" ] = self . data [ \"target\" ] . astype ( \"category\" ) self . input_scheme = self . data . columns . to_list () self . input_scheme . remove ( \"target\" ) return data FaultDetectionExperiment Bases: Experiment Source code in framework\\FaultDetection.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 class FaultDetectionExperiment ( Experiment ): def __init__ ( self , cfg : dict , experiment_id : str , run_id : str , * args , ** kwargs ) -> None : \"\"\"Initialize the model registry. Currently cannot be changed after initialization. In future versions, we will allow for dynamic model loading through configuration files. This experiment file is used for fault detection experiments. It is a subclass of the Experiment class in the framework.Experiment module. It is used to create, train, and predict using fault detection models. Heavily relies on outlier detection/clustering algorithms. Parameters: cfg: dict The configuration file for the experiment. experiment_id: str The experiment id for the experiment. run_id: str The run id for the experiment. Implemented models: - DBSCAN : 'models.fault_detection.DBSCANAnomalyDetection' - Elliptic Envelope 'models.fault_detection.EllipticEnvelopeAnomalyDetection' - Isolation Forest 'models.fault_detection.IsolationForestAnomaly' - PCA 'models.fault_detection.PCAAnomalyDetection' Inherits from Experiment Protocol class from framework.Experiment. If the experiment is overloaded, and new functions are added, one can call it in the system by adding the function name to the cfg file with relevant parameters. For example, if a new function 'new_function' is added to the system, the cfg file should have the following structure: cfg file should have the following structure: ``` load_object: module: framework.FaultIsolation name: FaultIsolationExperiment setup: datetime_column: str The column that contains the datetime information. target: str The target column for the experiment. (output column) ... eda: create_model: model: str The model to be used for training. params: dict The parameters to be used for the model. new_function: param1: str The first parameter for the function. param2: str The second parameter for the function. ``` The function will then be called in the system by calling the 'run' function. experiment.run(data, experiment_id, run_id) \"\"\" self . cfg = cfg self . experiment_id = experiment_id self . run_id = run_id self . _model_registry [ \"dbscan\" ] = DBSCANAnomalyDetection self . _model_registry [ \"ee\" ] = EllipticEnvelopeAnomalyDetection self . _model_registry [ \"iforest\" ] = IsolationForestAnomaly self . _model_registry [ \"pca\" ] = PCAAnomalyDetection def setup ( self , data : str ) -> pd . DataFrame : \"\"\"Setup the data for training and prediction. This function is called before any other function to set self.data that can be used in any other function. Also returns the data if need be. Parameters: data (str): The data to be used for training and prediction. Returns: pd.DataFrame The data that was set for the experiment. \"\"\" cfg = self . cfg [ \"setup\" ] self . ds = cfg . get ( \"datetime_column\" , None ) self . format = cfg . get ( \"datetime_format\" , None ) self . data_format = cfg . get ( \"format\" , None ) data = self . format_data ( data , self . data_format ) self . predict_window = cfg . get ( \"predict_window\" , 0 ) if self . predict_window is None : self . predict_window = 0 if self . predict_window < 0 : self . predict_window *= - 1 self . data = data if self . ds is not None : self . data = self . convert_datetime ( self . data , format = self . format ) self . data . set_index ( self . ds , inplace = True ) return data def create_model ( self , * args , ** kwargs ): \"\"\"Create the model using the configuration file. The model is trained on the data set up in the 'setup' function. Returns: any The trained model. \"\"\" model = self . cfg [ \"create_model\" ][ \"model\" ] params = self . cfg [ \"create_model\" ] . get ( \"params\" , None ) if params is None : params = dict () if self . data is None : raise ValueError ( \"Data not found. Please use 'setup' to setup the data first.\" ) try : model_class = self . _model_registry [ model ] except KeyError : raise ValueError ( f \"Model { model } not found in model registry. Please check configuration file. Available models are { self . _model_registry . keys () } .\" ) self . model = model_class ( ** params ) . fit ( self . data ) self . _report_registry = self . model . _figs with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : mlflow . set_tag ( \"model\" , model ) for k , v in self . model . _figs . items (): v . write_html ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } .html\" )) mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } .html\" ), \"reports\" ) #mlflow.sklearn.log_model(self.model, \"model\") for k , v in params . items (): mlflow . log_param ( k , v ) return self . model def predict ( self , data : str ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\"Predict using the trained model. The model should be trained before calling this function. Parameters: data (str): The data to be used for prediction. Returns: Tuple[pd.DataFrame, pd.DataFrame] The predictions made by the model and the metrics calculated for the model. \"\"\" data = self . format_data ( data , self . data_format ) if self . model is None : raise ValueError ( \"Model not found. Please train the model first with 'create_model'.\" ) X = data . copy () if self . ds is not None : X = self . convert_datetime ( X , format = self . format ) X . set_index ( self . ds , inplace = True ) y = self . model . predict ( X ) self . new_data = pd . concat ([ self . new_data , y ], axis = 0 ) if self . predict_window > self . new_data . shape [ 0 ] : self . model . update_predict ( self . new_data , reset_fig = True , update_fig = True ) else : self . model . update_predict ( self . new_data . iloc [ - self . predict_window :, :], reset_fig = True , update_fig = True ) return y , pd . DataFrame ([]) def _score ( self , y , y_hat ): \"\"\"Score the model using the predictions. This function is called after the predictions are made. Parameters: y: pd.Series The actual values. y_hat: pd.Series The predicted values. Returns: pd.DataFrame The metrics calculated for the model. \"\"\" logging . log ( \"Scoring function not implemented.\" ) return pd . DataFrame ([]) __init__ ( cfg , experiment_id , run_id , * args , ** kwargs ) Initialize the model registry. Currently cannot be changed after initialization. In future versions, we will allow for dynamic model loading through configuration files. This experiment file is used for fault detection experiments. It is a subclass of the Experiment class in the framework.Experiment module. It is used to create, train, and predict using fault detection models. Heavily relies on outlier detection/clustering algorithms. Parameters: cfg ( dict ) \u2013 dict The configuration file for the experiment. experiment_id ( str ) \u2013 str The experiment id for the experiment. run_id ( str ) \u2013 str The run id for the experiment. Implemented models: - DBSCAN : 'models.fault_detection.DBSCANAnomalyDetection' - Elliptic Envelope 'models.fault_detection.EllipticEnvelopeAnomalyDetection' - Isolation Forest 'models.fault_detection.IsolationForestAnomaly' - PCA 'models.fault_detection.PCAAnomalyDetection' Inherits from Experiment Protocol class from framework.Experiment. If the experiment is overloaded, and new functions are added, one can call it in the system by adding the function name to the cfg file with relevant parameters. For example, if a new function 'new_function' is added to the system, the cfg file should have the following structure: cfg file should have the following structure: load_object: module: framework.FaultIsolation name: FaultIsolationExperiment setup: datetime_column: str The column that contains the datetime information. target: str The target column for the experiment. (output column) ... eda: create_model: model: str The model to be used for training. params: dict The parameters to be used for the model. new_function: param1: str The first parameter for the function. param2: str The second parameter for the function. The function will then be called in the system by calling the 'run' function. experiment.run(data, experiment_id, run_id) Source code in framework\\FaultDetection.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def __init__ ( self , cfg : dict , experiment_id : str , run_id : str , * args , ** kwargs ) -> None : \"\"\"Initialize the model registry. Currently cannot be changed after initialization. In future versions, we will allow for dynamic model loading through configuration files. This experiment file is used for fault detection experiments. It is a subclass of the Experiment class in the framework.Experiment module. It is used to create, train, and predict using fault detection models. Heavily relies on outlier detection/clustering algorithms. Parameters: cfg: dict The configuration file for the experiment. experiment_id: str The experiment id for the experiment. run_id: str The run id for the experiment. Implemented models: - DBSCAN : 'models.fault_detection.DBSCANAnomalyDetection' - Elliptic Envelope 'models.fault_detection.EllipticEnvelopeAnomalyDetection' - Isolation Forest 'models.fault_detection.IsolationForestAnomaly' - PCA 'models.fault_detection.PCAAnomalyDetection' Inherits from Experiment Protocol class from framework.Experiment. If the experiment is overloaded, and new functions are added, one can call it in the system by adding the function name to the cfg file with relevant parameters. For example, if a new function 'new_function' is added to the system, the cfg file should have the following structure: cfg file should have the following structure: ``` load_object: module: framework.FaultIsolation name: FaultIsolationExperiment setup: datetime_column: str The column that contains the datetime information. target: str The target column for the experiment. (output column) ... eda: create_model: model: str The model to be used for training. params: dict The parameters to be used for the model. new_function: param1: str The first parameter for the function. param2: str The second parameter for the function. ``` The function will then be called in the system by calling the 'run' function. experiment.run(data, experiment_id, run_id) \"\"\" self . cfg = cfg self . experiment_id = experiment_id self . run_id = run_id self . _model_registry [ \"dbscan\" ] = DBSCANAnomalyDetection self . _model_registry [ \"ee\" ] = EllipticEnvelopeAnomalyDetection self . _model_registry [ \"iforest\" ] = IsolationForestAnomaly self . _model_registry [ \"pca\" ] = PCAAnomalyDetection create_model ( * args , ** kwargs ) Create the model using the configuration file. The model is trained on the data set up in the 'setup' function. Returns: any The trained model. Source code in framework\\FaultDetection.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def create_model ( self , * args , ** kwargs ): \"\"\"Create the model using the configuration file. The model is trained on the data set up in the 'setup' function. Returns: any The trained model. \"\"\" model = self . cfg [ \"create_model\" ][ \"model\" ] params = self . cfg [ \"create_model\" ] . get ( \"params\" , None ) if params is None : params = dict () if self . data is None : raise ValueError ( \"Data not found. Please use 'setup' to setup the data first.\" ) try : model_class = self . _model_registry [ model ] except KeyError : raise ValueError ( f \"Model { model } not found in model registry. Please check configuration file. Available models are { self . _model_registry . keys () } .\" ) self . model = model_class ( ** params ) . fit ( self . data ) self . _report_registry = self . model . _figs with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : mlflow . set_tag ( \"model\" , model ) for k , v in self . model . _figs . items (): v . write_html ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } .html\" )) mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } .html\" ), \"reports\" ) #mlflow.sklearn.log_model(self.model, \"model\") for k , v in params . items (): mlflow . log_param ( k , v ) return self . model predict ( data ) Predict using the trained model. The model should be trained before calling this function. Parameters: data (str): The data to be used for prediction. Returns: Tuple [ DataFrame , DataFrame ] \u2013 Tuple[pd.DataFrame, pd.DataFrame] The predictions made by the model and the metrics calculated for the model. Source code in framework\\FaultDetection.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 def predict ( self , data : str ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\"Predict using the trained model. The model should be trained before calling this function. Parameters: data (str): The data to be used for prediction. Returns: Tuple[pd.DataFrame, pd.DataFrame] The predictions made by the model and the metrics calculated for the model. \"\"\" data = self . format_data ( data , self . data_format ) if self . model is None : raise ValueError ( \"Model not found. Please train the model first with 'create_model'.\" ) X = data . copy () if self . ds is not None : X = self . convert_datetime ( X , format = self . format ) X . set_index ( self . ds , inplace = True ) y = self . model . predict ( X ) self . new_data = pd . concat ([ self . new_data , y ], axis = 0 ) if self . predict_window > self . new_data . shape [ 0 ] : self . model . update_predict ( self . new_data , reset_fig = True , update_fig = True ) else : self . model . update_predict ( self . new_data . iloc [ - self . predict_window :, :], reset_fig = True , update_fig = True ) return y , pd . DataFrame ([]) setup ( data ) Setup the data for training and prediction. This function is called before any other function to set self.data that can be used in any other function. Also returns the data if need be. Parameters: data ( str ) \u2013 The data to be used for training and prediction. Returns: DataFrame \u2013 pd.DataFrame The data that was set for the experiment. Source code in framework\\FaultDetection.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def setup ( self , data : str ) -> pd . DataFrame : \"\"\"Setup the data for training and prediction. This function is called before any other function to set self.data that can be used in any other function. Also returns the data if need be. Parameters: data (str): The data to be used for training and prediction. Returns: pd.DataFrame The data that was set for the experiment. \"\"\" cfg = self . cfg [ \"setup\" ] self . ds = cfg . get ( \"datetime_column\" , None ) self . format = cfg . get ( \"datetime_format\" , None ) self . data_format = cfg . get ( \"format\" , None ) data = self . format_data ( data , self . data_format ) self . predict_window = cfg . get ( \"predict_window\" , 0 ) if self . predict_window is None : self . predict_window = 0 if self . predict_window < 0 : self . predict_window *= - 1 self . data = data if self . ds is not None : self . data = self . convert_datetime ( self . data , format = self . format ) self . data . set_index ( self . ds , inplace = True ) return data TimeSeriesAnomalyExperiment Bases: Experiment Class for interacting with time series models. Plays a similar role to pycaret experiment. Deals with model training, predicting, metrics and serialization. Inherits from Experiment Protocol class from framework.Experiment. Implemented models: - Autoencoder : 'models.time_series_analysis.Autoencoder' - LSTM : 'models.time_series_analysis.LSTM' - Prophet : 'models.time_series_analysis.Prophet' - SSA : 'models.time_series_analysis.SSA' - ARIMA : 'models.time_series_analysis.ARIMA' - Exponential Smoothing : 'models.time_series_analysis.ExponentialSmoothing' If the experiment is overloaded, and new functions are added, one can call it in the system by adding the function name to the cfg file with relevant parameters. For example, if a new function 'new_function' is added to the system, the cfg file should have the following structure: cfg file should have the following structure: load_object: module: framework.TimeSeriesAnalysis name: TimeSeriesAnomalyExperiment setup: ds : str The column that contains the datetime information. y : str The target column for the experiment. (output column) ... create_model: model: str The model to be used for training. params: dict The parameters to be used for the model. ... new_function: # e.g. posterior reconciliation of results. param1: str The first parameter for the function. param2: str The second parameter for the function. Source code in framework\\TimeSeriesAnalysis.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 class TimeSeriesAnomalyExperiment ( Experiment ): \"\"\"Class for interacting with time series models. Plays a similar role to pycaret experiment. Deals with model training, predicting, metrics and serialization. Inherits from Experiment Protocol class from framework.Experiment. Implemented models: - Autoencoder : 'models.time_series_analysis.Autoencoder' - LSTM : 'models.time_series_analysis.LSTM' - Prophet : 'models.time_series_analysis.Prophet' - SSA : 'models.time_series_analysis.SSA' - ARIMA : 'models.time_series_analysis.ARIMA' - Exponential Smoothing : 'models.time_series_analysis.ExponentialSmoothing' If the experiment is overloaded, and new functions are added, one can call it in the system by adding the function name to the cfg file with relevant parameters. For example, if a new function 'new_function' is added to the system, the cfg file should have the following structure: cfg file should have the following structure: ``` load_object: module: framework.TimeSeriesAnalysis name: TimeSeriesAnomalyExperiment setup: ds : str The column that contains the datetime information. y : str The target column for the experiment. (output column) ... create_model: model: str The model to be used for training. params: dict The parameters to be used for the model. ... new_function: # e.g. posterior reconciliation of results. param1: str The first parameter for the function. param2: str The second parameter for the function. ``` \"\"\" def __init__ ( self , cfg : dict , experiment_id : str , run_id : str ) -> None : \"\"\"Initialize the model registry. Currently cannot be changed after initialization. In future versions, we will allow for dynamic model loading through configuration files. This experiment function is used for loading basic time series anomaly detection models. Parameters: cfg: dict The configuration file for the experiment. experiment_id: str The experiment id for the experiment. run_id: str The run id for the experiment. \"\"\" self . cfg = cfg self . experiment_id = experiment_id self . run_id = run_id self . _model_registry [ \"ae\" ] = Autoencoder self . _model_registry [ \"lstm\" ] = LSTM self . _model_registry [ \"prophet\" ] = ProphetAnomalyDetection self . _model_registry [ \"ssa\" ] = SSAAnomalyDetection self . _model_registry [ \"arima\" ] = ARIMAAnomalyDetector self . _model_registry [ \"es\" ] = ExponentialSmoothingAnomaly def setup ( self , data : str , * args , ** kwargs ): \"\"\"Setup the data for training and prediction. This function is called before training the model. In the future, it will also be used to preprocess the data and prepare it for training. Parameters: data: pd.DataFrame The data to be used for training and prediction. Returns: pd.DataFrame The data after processing. This data is used for training and prediction. Description: - The setup function is used to prepare the data for training and prediction. It is called before the model is trained. - The function renames the columns to 'ds' and 'y' for consistency. - The function logs the target and datestamp columns to mlflow. - The function returns the data after processing. \"\"\" cfg = self . cfg [ \"setup\" ] self . data_format = cfg . get ( \"format\" , None ) data = self . format_data ( data , self . data_format ) self . ds = cfg [ \"datetime_column\" ] self . target = cfg [ \"target\" ] self . format = cfg [ \"datetime_format\" ] self . predict_window = cfg . get ( \"predict_window\" , 0 ) if self . predict_window is None : self . predict_window = 0 if self . predict_window < 0 : self . predict_window *= - 1 retrain_cfg = cfg . get ( \"retrain\" , None ) if retrain_cfg is None or len ( retrain_cfg ) == 0 : self . retrain_window = 0 self . metric = None self . metric_threshold = 0.0 self . higher_better = True else : self . retrain_window = retrain_cfg . get ( \"retrain_window\" , 0 ) self . metric = retrain_cfg . get ( \"metric\" , None ) self . metric_threshold = retrain_cfg . get ( \"metric_threshold\" , 0.0 ) self . higher_better = retrain_cfg . get ( \"higher_better\" , True ) if self . ds not in data . columns : raise ValueError ( \"Datestamp column not found in data. Configuration file must specify datestamp column as ds.\" ) if self . target not in data . columns : raise ValueError ( \"Target column not found in data. Configuration file must specify target column as target.\" ) data = self . convert_datetime ( data , format = self . format ) data . rename ( columns = { self . ds : \"ds\" , self . target : \"y\" }, inplace = True ) self . data = data . loc [:, [ \"ds\" , \"y\" ]] self . input_scheme = [] with mlflow . start_run ( nested = True , experiment_id = self . experiment_id ) as run : mlflow . log_param ( \"target\" , self . target ) mlflow . log_param ( \"datetime_column\" , self . ds ) return data def create_model ( self , * args , ** kwargs ): \"\"\"Create the model using the configuration file. The model is trained on the data set up in the 'setup' function. Returns: any The trained model. Description: - The function creates the model using the configuration file. - The function logs the model and parameters to mlflow. - The function logs the metrics to mlflow. - The function saves the model to disk. - The function returns the trained model. \"\"\" model = self . cfg [ \"create_model\" ][ \"model\" ] params = self . cfg [ \"create_model\" ] . get ( \"params\" , None ) if params is None : params = dict () if self . data is None : raise ValueError ( \"Data not found. Please use 'setup' to setup the data first.\" ) try : model_class = self . _model_registry [ model ] except KeyError : raise ValueError ( f \"Model { model } not found in model registry. Please check configuration file. Available models are { self . _model_registry . keys () } .\" ) self . model = model_class ( ** params ) . fit ( self . data ) self . _report_registry = self . model . _figs y = self . model . predict ( self . data ) try : self . metrics = self . _score ( self . data [ self . target ], y [ \"y_pred\" ]) self . _report_registry [ \"metrics\" ] = px . line ( self . metrics , x = self . metrics . index , y = self . metrics . columns , markers = True ) except Exception as e : self . metrics = pd . DataFrame ([ np . NaN , np . NaN , np . NaN , np . NaN ], index = [ \"MSE\" , \"MAE\" , \"R2\" , \"MAPE\" ]) . transpose () logging . warn ( f \"Error calculating metrics: { e } . Model { self . model } may not have y_pred as output.\" ) with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : mlflow . set_tag ( \"model\" , model ) #mlflow.sklearn.save_model(self.model, f\"runs/{self.run_id}/model\") for k , v in params . items (): mlflow . log_param ( k , v ) for key , value in self . metrics . to_dict () . items (): mlflow . log_metric ( key , value [ 0 ]) for k , v in self . _report_registry . items (): v . write_html ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } _report.html\" )) mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } _report.html\" ), \"reports\" ) return self . model def predict ( self , data : str ): \"\"\"Predict using the trained model. The model should be trained before calling this function. Parameters: data str The data to be used for prediction. Returns: pd.DataFrame The predictions made by the model. Description: - The function predicts using the trained model. - The function updates the predict figures with the new data. - The function calculates the metrics for the model. - The function logs the metrics to mlflow. - The function saves the reports to disk. - The function logs the reports to mlflow. - The function returns the predictions and metrics. \"\"\" data = self . format_data ( data , self . data_format ) if self . model is None : raise ValueError ( \"Model not found. Please train the model first with 'create_model'.\" ) data . rename ( columns = { self . ds : \"ds\" , self . target : \"y\" }, inplace = True ) data = self . convert_datetime ( data , format = self . format ) data = data . loc [:, [ \"ds\" , \"y\" ]] y = self . model . predict ( data ) self . new_data = pd . concat ([ self . new_data , y ], axis = 0 ) if self . predict_window > self . new_data . shape [ 0 ] : self . model . update_predict ( self . new_data , reset_fig = True , update_fig = True ) else : self . model . update_predict ( self . new_data . iloc [ - self . predict_window :, :], reset_fig = True , update_fig = True ) if \"y_pred\" not in y . columns : metrics = pd . DataFrame ([ np . NaN , np . NaN , np . NaN , np . NaN ], index = [ \"MSE\" , \"MAE\" , \"R2\" , \"MAPE\" ]) . transpose () logging . warn ( \"y_pred not found in model output. Please make sure the model has a 'predict' method that returns a DataFrame with 'y_pred' column.\" ) else : metrics = self . _score ( data [ self . target ], y [ \"y_pred\" ]) self . metrics = pd . concat ([ self . metrics , metrics ], axis = 0 , ignore_index = True ) self . _report_registry [ \"metrics\" ] = px . line ( self . metrics , x = self . metrics . index , y = self . metrics . columns , markers = True ) #self.spc_chart(update_fig=True) self . _report_registry [ \"predict\" ] . write_html ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , \"predict_report.html\" )) self . _report_registry [ \"metrics\" ] . write_html ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , \"metrics_report.html\" )) with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , \"predict_report.html\" ), \"reports\" ) mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , \"metrics_report.html\" ), \"reports\" ) return y , metrics def _score ( self , y , y_hat ): \"\"\"Calculate the metrics for the model. Parameters: y: pd.Series The true values. y_hat: pd.Series The predicted values. Returns: pd.DataFrame The metrics for the model. Description: - The function calculates the metrics for the model. - The function returns: - Mean Squared Error - Mean Absolute Error - R2 Score - Mean Absolute Percentage Error \"\"\" assert isinstance ( y , pd . Series ) or isinstance ( y , pd . DataFrame ), \"y must be a pandas Series or DataFrame.\" assert isinstance ( y_hat , pd . Series ) or isinstance ( y_hat , pd . DataFrame ), \"y_hat must be a pandas Series or DataFrame.\" if isinstance ( y , pd . Series ): y = y . to_frame () if isinstance ( y_hat , pd . Series ): y_hat = y_hat . to_frame () assert y . shape == y_hat . shape , \"y and y_hat must have the same shape.\" mse = mean_squared_error ( y , y_hat ) mae = mean_absolute_error ( y , y_hat ) r2 = r2_score ( y , y_hat ) mape = mean_absolute_percentage_error ( y , y_hat ) return pd . DataFrame ([ mse , mae , r2 , mape ], index = [ \"MSE\" , \"MAE\" , \"R2\" , \"MAPE\" ]) . transpose () __init__ ( cfg , experiment_id , run_id ) Initialize the model registry. Currently cannot be changed after initialization. In future versions, we will allow for dynamic model loading through configuration files. This experiment function is used for loading basic time series anomaly detection models. Parameters: cfg: dict The configuration file for the experiment. experiment_id: str The experiment id for the experiment. run_id: str The run id for the experiment. Source code in framework\\TimeSeriesAnalysis.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def __init__ ( self , cfg : dict , experiment_id : str , run_id : str ) -> None : \"\"\"Initialize the model registry. Currently cannot be changed after initialization. In future versions, we will allow for dynamic model loading through configuration files. This experiment function is used for loading basic time series anomaly detection models. Parameters: cfg: dict The configuration file for the experiment. experiment_id: str The experiment id for the experiment. run_id: str The run id for the experiment. \"\"\" self . cfg = cfg self . experiment_id = experiment_id self . run_id = run_id self . _model_registry [ \"ae\" ] = Autoencoder self . _model_registry [ \"lstm\" ] = LSTM self . _model_registry [ \"prophet\" ] = ProphetAnomalyDetection self . _model_registry [ \"ssa\" ] = SSAAnomalyDetection self . _model_registry [ \"arima\" ] = ARIMAAnomalyDetector self . _model_registry [ \"es\" ] = ExponentialSmoothingAnomaly create_model ( * args , ** kwargs ) Create the model using the configuration file. The model is trained on the data set up in the 'setup' function. Returns: any The trained model. Description: - The function creates the model using the configuration file. - The function logs the model and parameters to mlflow. - The function logs the metrics to mlflow. - The function saves the model to disk. - The function returns the trained model. Source code in framework\\TimeSeriesAnalysis.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 def create_model ( self , * args , ** kwargs ): \"\"\"Create the model using the configuration file. The model is trained on the data set up in the 'setup' function. Returns: any The trained model. Description: - The function creates the model using the configuration file. - The function logs the model and parameters to mlflow. - The function logs the metrics to mlflow. - The function saves the model to disk. - The function returns the trained model. \"\"\" model = self . cfg [ \"create_model\" ][ \"model\" ] params = self . cfg [ \"create_model\" ] . get ( \"params\" , None ) if params is None : params = dict () if self . data is None : raise ValueError ( \"Data not found. Please use 'setup' to setup the data first.\" ) try : model_class = self . _model_registry [ model ] except KeyError : raise ValueError ( f \"Model { model } not found in model registry. Please check configuration file. Available models are { self . _model_registry . keys () } .\" ) self . model = model_class ( ** params ) . fit ( self . data ) self . _report_registry = self . model . _figs y = self . model . predict ( self . data ) try : self . metrics = self . _score ( self . data [ self . target ], y [ \"y_pred\" ]) self . _report_registry [ \"metrics\" ] = px . line ( self . metrics , x = self . metrics . index , y = self . metrics . columns , markers = True ) except Exception as e : self . metrics = pd . DataFrame ([ np . NaN , np . NaN , np . NaN , np . NaN ], index = [ \"MSE\" , \"MAE\" , \"R2\" , \"MAPE\" ]) . transpose () logging . warn ( f \"Error calculating metrics: { e } . Model { self . model } may not have y_pred as output.\" ) with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : mlflow . set_tag ( \"model\" , model ) #mlflow.sklearn.save_model(self.model, f\"runs/{self.run_id}/model\") for k , v in params . items (): mlflow . log_param ( k , v ) for key , value in self . metrics . to_dict () . items (): mlflow . log_metric ( key , value [ 0 ]) for k , v in self . _report_registry . items (): v . write_html ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } _report.html\" )) mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } _report.html\" ), \"reports\" ) return self . model predict ( data ) Predict using the trained model. The model should be trained before calling this function. Parameters: data str The data to be used for prediction. Returns: \u2013 pd.DataFrame The predictions made by the model. Description: - The function predicts using the trained model. - The function updates the predict figures with the new data. - The function calculates the metrics for the model. - The function logs the metrics to mlflow. - The function saves the reports to disk. - The function logs the reports to mlflow. - The function returns the predictions and metrics. Source code in framework\\TimeSeriesAnalysis.py 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 def predict ( self , data : str ): \"\"\"Predict using the trained model. The model should be trained before calling this function. Parameters: data str The data to be used for prediction. Returns: pd.DataFrame The predictions made by the model. Description: - The function predicts using the trained model. - The function updates the predict figures with the new data. - The function calculates the metrics for the model. - The function logs the metrics to mlflow. - The function saves the reports to disk. - The function logs the reports to mlflow. - The function returns the predictions and metrics. \"\"\" data = self . format_data ( data , self . data_format ) if self . model is None : raise ValueError ( \"Model not found. Please train the model first with 'create_model'.\" ) data . rename ( columns = { self . ds : \"ds\" , self . target : \"y\" }, inplace = True ) data = self . convert_datetime ( data , format = self . format ) data = data . loc [:, [ \"ds\" , \"y\" ]] y = self . model . predict ( data ) self . new_data = pd . concat ([ self . new_data , y ], axis = 0 ) if self . predict_window > self . new_data . shape [ 0 ] : self . model . update_predict ( self . new_data , reset_fig = True , update_fig = True ) else : self . model . update_predict ( self . new_data . iloc [ - self . predict_window :, :], reset_fig = True , update_fig = True ) if \"y_pred\" not in y . columns : metrics = pd . DataFrame ([ np . NaN , np . NaN , np . NaN , np . NaN ], index = [ \"MSE\" , \"MAE\" , \"R2\" , \"MAPE\" ]) . transpose () logging . warn ( \"y_pred not found in model output. Please make sure the model has a 'predict' method that returns a DataFrame with 'y_pred' column.\" ) else : metrics = self . _score ( data [ self . target ], y [ \"y_pred\" ]) self . metrics = pd . concat ([ self . metrics , metrics ], axis = 0 , ignore_index = True ) self . _report_registry [ \"metrics\" ] = px . line ( self . metrics , x = self . metrics . index , y = self . metrics . columns , markers = True ) #self.spc_chart(update_fig=True) self . _report_registry [ \"predict\" ] . write_html ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , \"predict_report.html\" )) self . _report_registry [ \"metrics\" ] . write_html ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , \"metrics_report.html\" )) with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , \"predict_report.html\" ), \"reports\" ) mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , \"metrics_report.html\" ), \"reports\" ) return y , metrics setup ( data , * args , ** kwargs ) Setup the data for training and prediction. This function is called before training the model. In the future, it will also be used to preprocess the data and prepare it for training. Parameters: data: pd.DataFrame The data to be used for training and prediction. Returns: \u2013 pd.DataFrame The data after processing. This data is used for training and prediction. Description: The setup function is used to prepare the data for training and prediction. It is called before the model is trained. The function renames the columns to 'ds' and 'y' for consistency. The function logs the target and datestamp columns to mlflow. The function returns the data after processing. Source code in framework\\TimeSeriesAnalysis.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def setup ( self , data : str , * args , ** kwargs ): \"\"\"Setup the data for training and prediction. This function is called before training the model. In the future, it will also be used to preprocess the data and prepare it for training. Parameters: data: pd.DataFrame The data to be used for training and prediction. Returns: pd.DataFrame The data after processing. This data is used for training and prediction. Description: - The setup function is used to prepare the data for training and prediction. It is called before the model is trained. - The function renames the columns to 'ds' and 'y' for consistency. - The function logs the target and datestamp columns to mlflow. - The function returns the data after processing. \"\"\" cfg = self . cfg [ \"setup\" ] self . data_format = cfg . get ( \"format\" , None ) data = self . format_data ( data , self . data_format ) self . ds = cfg [ \"datetime_column\" ] self . target = cfg [ \"target\" ] self . format = cfg [ \"datetime_format\" ] self . predict_window = cfg . get ( \"predict_window\" , 0 ) if self . predict_window is None : self . predict_window = 0 if self . predict_window < 0 : self . predict_window *= - 1 retrain_cfg = cfg . get ( \"retrain\" , None ) if retrain_cfg is None or len ( retrain_cfg ) == 0 : self . retrain_window = 0 self . metric = None self . metric_threshold = 0.0 self . higher_better = True else : self . retrain_window = retrain_cfg . get ( \"retrain_window\" , 0 ) self . metric = retrain_cfg . get ( \"metric\" , None ) self . metric_threshold = retrain_cfg . get ( \"metric_threshold\" , 0.0 ) self . higher_better = retrain_cfg . get ( \"higher_better\" , True ) if self . ds not in data . columns : raise ValueError ( \"Datestamp column not found in data. Configuration file must specify datestamp column as ds.\" ) if self . target not in data . columns : raise ValueError ( \"Target column not found in data. Configuration file must specify target column as target.\" ) data = self . convert_datetime ( data , format = self . format ) data . rename ( columns = { self . ds : \"ds\" , self . target : \"y\" }, inplace = True ) self . data = data . loc [:, [ \"ds\" , \"y\" ]] self . input_scheme = [] with mlflow . start_run ( nested = True , experiment_id = self . experiment_id ) as run : mlflow . log_param ( \"target\" , self . target ) mlflow . log_param ( \"datetime_column\" , self . ds ) return data ProcessMiningExperiment Bases: Experiment This experiment file is used for process mining experiments. It is a subclass of the Experiment class in the framework.Experiment module. It is used to create, train, and predict using process mining models. Heavily relies on sequence mining algorithms. Implemented models: - Apriori : 'models.spmf.Apriori' - CMSPAM : 'models.spmf.CM_SPAM' - TopKRules :'models.spmf.TopKRules' - Heuristics Miner : 'models.spmf.HeuristicsMiner' Inherits from Experiment Protocol class from framework.Experiment. If the experiment is overloaded, and new functions are added, one can call it in the system by adding the function name to the cfg file with relevant parameters. For example, if a new function 'new_function' is added to the system, the cfg file should have the following structure: cfg file should have the following structure: load_object: module: framework.ProcessMining name: ProcessMiningExperiment setup: ... create_model: model: str The model to be used for training. params: dict The parameters to be used for the model. new_function: param1: str The first parameter for the function. param2: str The second parameter for the function. ... Source code in framework\\ProcessMining.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 class ProcessMiningExperiment ( Experiment ): \"\"\" This experiment file is used for process mining experiments. It is a subclass of the Experiment class in the framework.Experiment module. It is used to create, train, and predict using process mining models. Heavily relies on sequence mining algorithms. Implemented models: - Apriori : 'models.spmf.Apriori' - CMSPAM : 'models.spmf.CM_SPAM' - TopKRules :'models.spmf.TopKRules' - Heuristics Miner : 'models.spmf.HeuristicsMiner' Inherits from Experiment Protocol class from framework.Experiment. If the experiment is overloaded, and new functions are added, one can call it in the system by adding the function name to the cfg file with relevant parameters. For example, if a new function 'new_function' is added to the system, the cfg file should have the following structure: cfg file should have the following structure: ``` load_object: module: framework.ProcessMining name: ProcessMiningExperiment setup: ... create_model: model: str The model to be used for training. params: dict The parameters to be used for the model. new_function: param1: str The first parameter for the function. param2: str The second parameter for the function. ... ``` \"\"\" def __init__ ( self , cfg : dict , experiment_id : str , run_id : str , * args , ** kwargs ) -> None : \"\"\"Initialize the model registry. Currently cannot be changed after initialization. Parameters: cfg: dict The configuration file for the experiment. experiment_id: str The experiment id for the experiment. run_id: str The run id for the experiment. \"\"\" self . cfg = cfg self . experiment_id = experiment_id self . run_id = run_id self . _model_registry [ \"apriori\" ] = Apriori self . _model_registry [ \"cmspam\" ] = CMSPAM self . _model_registry [ \"topk\" ] = TopKRules self . _model_registry [ \"heuristics\" ] = HeuristicsMiner def setup ( self , data : pd . DataFrame ): \"\"\"Setup the data for training and prediction. This function is called before training the model. Data must be in pandas DataFrame format - can be a columns, with several rows. Parameters: data: pd.DataFrame The data to be used for training and prediction. Returns: pd.DataFrame The data after processing. This data is used for training and prediction \"\"\" cfg = self . cfg [ \"setup\" ] self . data_format = cfg . get ( \"format\" , None ) data = self . format_data ( data , self . data_format ) # col_names = dict(\"Start_timestamp\" : \"start:timestamp\", # \"End_timestamp\" : \"time:timestamp\", # \"Event\" : \"concept:name\", # \"Case_id\" : \"case:concept:name\", # \"Resource\" : \"org:resource\", # \"Ordered\" : \"Ordered\", # \"Completed\" : \"Completed\", # \"Rejected\" : \"Rejected\", # \"MRB\" : \"MRB\", # \"Part\" : \"Part\") #data.rename(columns=col_names, inplace=True) #TODO : excavate a log ... self . data = data #.drop(columns=[self.ds]) return data def create_model ( self , * args , ** kwargs ) -> any : \"\"\"Create the model using the configuration file. The model is trained on the data set up in the 'setup' function. Returns: any The trained model. \"\"\" model = self . cfg [ \"create_model\" ][ \"model\" ] params = self . cfg [ \"create_model\" ] . get ( \"params\" , None ) if params is None : params = dict () if self . data is None : raise ValueError ( \"Data not found. Please use 'setup' to setup the data first.\" ) try : model_class = self . _model_registry [ model ] except KeyError : raise ValueError ( f \"Model { model } not found in model registry. Please check configuration file. Available models are { self . _model_registry . keys () } .\" ) ## data.to_csv() self . model = model_class ( ** params ) . fit ( self . data ) self . _report_registry = self . model . _figs with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : mlflow . set_tag ( \"model\" , model ) for k , v in self . model . _figs . items (): v . write_html ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } .html\" )) mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } .html\" ), \"reports\" ) for k , v in params . items (): mlflow . log_param ( k , v ) return self . model def predict ( self , data : pd . DataFrame ): \"\"\"Predict using the trained model. The model should be trained before calling this function. Parameters: data: pd.DataFrame The data to be used for prediction. Returns: pd.DataFrame The predictions made by the model. \"\"\" if self . model is None : raise ValueError ( \"Model not found. Please train the model first with 'create_model'.\" ) X = data . copy () y = self . model . predict ( X ) return y , pd . DataFrame ([]) def _score ( self , y , y_hat ): \"\"\"Score the model using the predictions. This function is called after the predictions are made. Parameters: y: pd.Series The actual values. y_hat: pd.Series The predicted values. Returns: pd.DataFrame The scores for the model.\"\"\" if self . model is None : raise ValueError ( \"Model not found. Please train the model first with 'create_model'.\" ) if y is None : raise ValueError ( \"No data found for scoring. Please provide data for scoring.\" ) if y_hat is None : raise ValueError ( \"No predictions found for scoring. Please provide predictions for scoring.\" ) return pd . DataFrame ([]) __init__ ( cfg , experiment_id , run_id , * args , ** kwargs ) Initialize the model registry. Currently cannot be changed after initialization. Parameters: cfg: dict The configuration file for the experiment. experiment_id: str The experiment id for the experiment. run_id: str The run id for the experiment. Source code in framework\\ProcessMining.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def __init__ ( self , cfg : dict , experiment_id : str , run_id : str , * args , ** kwargs ) -> None : \"\"\"Initialize the model registry. Currently cannot be changed after initialization. Parameters: cfg: dict The configuration file for the experiment. experiment_id: str The experiment id for the experiment. run_id: str The run id for the experiment. \"\"\" self . cfg = cfg self . experiment_id = experiment_id self . run_id = run_id self . _model_registry [ \"apriori\" ] = Apriori self . _model_registry [ \"cmspam\" ] = CMSPAM self . _model_registry [ \"topk\" ] = TopKRules self . _model_registry [ \"heuristics\" ] = HeuristicsMiner create_model ( * args , ** kwargs ) Create the model using the configuration file. The model is trained on the data set up in the 'setup' function. Returns: any The trained model. Source code in framework\\ProcessMining.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def create_model ( self , * args , ** kwargs ) -> any : \"\"\"Create the model using the configuration file. The model is trained on the data set up in the 'setup' function. Returns: any The trained model. \"\"\" model = self . cfg [ \"create_model\" ][ \"model\" ] params = self . cfg [ \"create_model\" ] . get ( \"params\" , None ) if params is None : params = dict () if self . data is None : raise ValueError ( \"Data not found. Please use 'setup' to setup the data first.\" ) try : model_class = self . _model_registry [ model ] except KeyError : raise ValueError ( f \"Model { model } not found in model registry. Please check configuration file. Available models are { self . _model_registry . keys () } .\" ) ## data.to_csv() self . model = model_class ( ** params ) . fit ( self . data ) self . _report_registry = self . model . _figs with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : mlflow . set_tag ( \"model\" , model ) for k , v in self . model . _figs . items (): v . write_html ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } .html\" )) mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } .html\" ), \"reports\" ) for k , v in params . items (): mlflow . log_param ( k , v ) return self . model predict ( data ) Predict using the trained model. The model should be trained before calling this function. Parameters: data: pd.DataFrame The data to be used for prediction. Returns: \u2013 pd.DataFrame The predictions made by the model. Source code in framework\\ProcessMining.py 134 135 136 137 138 139 140 141 142 143 144 145 146 def predict ( self , data : pd . DataFrame ): \"\"\"Predict using the trained model. The model should be trained before calling this function. Parameters: data: pd.DataFrame The data to be used for prediction. Returns: pd.DataFrame The predictions made by the model. \"\"\" if self . model is None : raise ValueError ( \"Model not found. Please train the model first with 'create_model'.\" ) X = data . copy () y = self . model . predict ( X ) return y , pd . DataFrame ([]) setup ( data ) Setup the data for training and prediction. This function is called before training the model. Data must be in pandas DataFrame format - can be a columns, with several rows. Parameters: data ( DataFrame ) \u2013 pd.DataFrame The data to be used for training and prediction. Returns: \u2013 pd.DataFrame The data after processing. This data is used for training and prediction Source code in framework\\ProcessMining.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def setup ( self , data : pd . DataFrame ): \"\"\"Setup the data for training and prediction. This function is called before training the model. Data must be in pandas DataFrame format - can be a columns, with several rows. Parameters: data: pd.DataFrame The data to be used for training and prediction. Returns: pd.DataFrame The data after processing. This data is used for training and prediction \"\"\" cfg = self . cfg [ \"setup\" ] self . data_format = cfg . get ( \"format\" , None ) data = self . format_data ( data , self . data_format ) # col_names = dict(\"Start_timestamp\" : \"start:timestamp\", # \"End_timestamp\" : \"time:timestamp\", # \"Event\" : \"concept:name\", # \"Case_id\" : \"case:concept:name\", # \"Resource\" : \"org:resource\", # \"Ordered\" : \"Ordered\", # \"Completed\" : \"Completed\", # \"Rejected\" : \"Rejected\", # \"MRB\" : \"MRB\", # \"Part\" : \"Part\") #data.rename(columns=col_names, inplace=True) #TODO : excavate a log ... self . data = data #.drop(columns=[self.ds]) return data","title":"Experiments"},{"location":"experiments/#experiments","text":"Experiments are the basic building blocks of the ExperimentHub. It is a Protocol class and can be used freely to derive an Experiment, such as the ones below. For specific goals and roles, we recommend buildign custom experiments, which require 2 important things: the configuration files, and the models Bases: Protocol A class representing an experiment. This class is a protocol that defines the methods that an experiment should implement. Some functions must be overloaded by an implementation class, while others can be left as is. Can only handle sklearn.BaseEstimator class, and so the used model must be overloaded Parameters: mlflow_uri ( str ) \u2013 the URI of the MLflow server model ( any ) \u2013 the model to be used in the experiment data ( DataFrame ) \u2013 the training data -- allocated during training new_data ( DataFrame ) \u2013 data that is not in the training set -- incoming data in prediction prediction ( DataFrame ) \u2013 the prediction of the new data metrics ( DataFrame ) \u2013 the metrics of the model cfg ( dict ) \u2013 the configuration of the experiment _model_registry ( dict [ str , type ] ) \u2013 a registry of model classes _report_registry ( dict [ str , Figure ] ) \u2013 a registry of reports run_id ( str ) \u2013 the ID of the run experiment_id ( str ) \u2013 the ID of the experiment name ( str ) \u2013 the name of the experiment eda_report ( str ) \u2013 the EDA report TODO: - proper logging - Add mmlw_estimator as a base class for the model - SPC chart - add support for data drift (eda) - add support for model drift (eda) - add support for model explainability (shap, lime, etc.) - add support for general data preprocessing (e.g. missing values, outliers, etc.) - add support for dim reduction (PCA, TSNE, etc.) - add support for feature selection (RFE, etc.) - dynamic model loading from specific folder/registry Source code in framework\\Experiment.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 class Experiment ( Protocol ): \"\"\"A class representing an experiment. This class is a protocol that defines the methods that an experiment should implement. Some functions must be overloaded by an implementation class, while others can be left as is. Can only handle sklearn.BaseEstimator class, and so the used model must be overloaded Parameters: mlflow_uri (str): the URI of the MLflow server model (any): the model to be used in the experiment data (pd.DataFrame): the training data -- allocated during training new_data (pd.DataFrame): data that is not in the training set -- incoming data in prediction prediction (pd.DataFrame): the prediction of the new data metrics (pd.DataFrame): the metrics of the model cfg (dict): the configuration of the experiment _model_registry (dict[str, type]): a registry of model classes _report_registry (dict[str, go.Figure]): a registry of reports run_id (str): the ID of the run experiment_id (str): the ID of the experiment name (str): the name of the experiment eda_report (str): the EDA report TODO: - proper logging - Add mmlw_estimator as a base class for the model - SPC chart - add support for data drift (eda) - add support for model drift (eda) - add support for model explainability (shap, lime, etc.) - add support for general data preprocessing (e.g. missing values, outliers, etc.) - add support for dim reduction (PCA, TSNE, etc.) - add support for feature selection (RFE, etc.) - dynamic model loading from specific folder/registry \"\"\" mlflow_uri : str = None model : any = None data : pd . DataFrame = None new_data : pd . DataFrame = pd . DataFrame ([]) metrics : pd . DataFrame = None cfg : dict = None _model_registry : dict [ str , type ] = dict () _report_registry : dict [ str , go . Figure ] = dict () run_id : str = None experiment_id : str = None name : str = \"\" _eda_registry : dict [ str , go . Figure ] = dict () scheme = None ###retrain_window : int, metric : str, metric_threshold : float, higher_better : bool = False retrain_window : int = 0 metric : str = None metric_threshold : float = 0.0 higher_better : bool = False data_format : dict = None def __init__ ( self , cfg : dict , experiment_id : str , run_id : str ) -> None : \"\"\"Initialize the Experiment class. This class is a protocol that defines the methods that an experiment should implement. Parameters: cfg (dict): the configuration of the experiment experiment_id (str): the ID of the experiment run_id (str): the ID of the run \"\"\" self . cfg = cfg self . experiment_id = experiment_id self . run_id = run_id def format_data ( self , data , format : dict = None ) -> pd . DataFrame : \"\"\"This function will provide a function to recover data from nonstandard json as pd.Dataframe. Parameters: data : data in json format or DataFrame format (dict) : settings for formatting. If None, then default pd.read_json is used.\"\"\" if format is None : if isinstance ( data , pd . DataFrame ): return data else : from io import StringIO data = pd . read_json ( StringIO ( data )) return data else : if format . get ( \"name\" , \"pivot\" ) == \"pivot\" : data = json . loads ( data ) _id = format . get ( \"id\" , \"tsdata\" ) mxlvl = format . get ( \"max_level\" , 1 ) data = pd . json_normalize ( data [ _id ], max_level = 1 ) column = format . get ( \"columns\" , \"target\" ) ind = format . get ( \"index\" , \"date\" ) vals = format . get ( \"values\" , \"value\" ) data = data . pivot ( columns = column , index = ind , values = vals ) data . columns . name = None data . reset_index ( drop = False , inplace = True ) return data else : raise Exception ( f \"Configuration contains faulty data formatting settings. Please make sure the setup keyword contaisn the necessary information. Settings are: { format } \" ) ###ADD Overloadable methods for Experiment class. def setup ( self , data : pd . DataFrame , * args , ** kwargs ): \"\"\"This function sets up the experiment. It is called before training the model.\"\"\" return NotImplementedError ( \"Implement this in the child class.\" ) def create_model ( self , * args , ** kwargs ): \"\"\"This function trains the model.\"\"\" return NotImplementedError ( \"Implement this in the child class.\" ) def predict ( self , data , * args , ** kwargs ): \"\"\"This function predicts the target variable.\"\"\" return NotImplementedError ( \"Implement this in the child class.\" ) def _score ( self , y , y_hat ) -> pd . DataFrame : \"\"\"This function calculates the metrics of the model.\"\"\" return NotImplementedError ( \"Implement this in the child class.\" ) def plot_model ( self , plot : str ) -> go . Figure : \"\"\"This function plots the model. Parameters: plot (str): the plot name that is to be displayed Returns: go.Figure: the plot \"\"\" if plot not in self . model . _figs : raise ValueError ( f \"Plot { plot } not found in model.\" ) return self . model . _figs [ plot ] def save ( self ): \"\"\"This function saves the experiment. It saves the model and the reports. Uses joblib to save the model and pickle to save the reports. \"\"\" with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : repository = get_artifact_repository ( run . info . artifact_uri ) try : repository . delete_artifacts ( mlflow . get_artifact_uri ( \"experiment.pkl\" )) repository . delete_artifacts ( mlflow . get_artifact_uri ( \"metadata.yaml\" )) repository . delete_artifacts ( mlflow . get_artifact_uri ( \"reports\" )) except : pass experiment_to_be_saved = deepcopy ( self ) if isinstance ( self . model , tf . keras . models . Model ): self . model . save ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"model.keras\" )) experiment_to_be_saved . model = None with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"model.keras\" ), \"model\" ) with open ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"experiment.pkl\" ), \"wb\" ) as f : joblib . dump ( experiment_to_be_saved , f ) for k , v in self . _report_registry . items (): with open ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } .json\" ), \"w\" ) as f : write_json ( v , f ) #os.path.join(os.getcwd(), \"runs\", self.run_id, \"reports\", f\"{k}.json\") with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } .json\" ), \"reports\" ) for k , v in self . _eda_registry . items (): with open ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \"eda_ { k } .json\" ), \"w\" ) as f : write_json ( v , f ) with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \"eda_ { k } .json\" ), \"reports\" ) yaml . dump ( self . cfg , open ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"metadata.yaml\" ), \"w\" )) with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"experiment.pkl\" ), \"\" ) mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"metadata.yaml\" ), \"\" ) def load ( self , run_id : str ) -> Experiment : \"\"\"This function loads the experiment. It loads the model and the reports. Uses joblib to load the model and pickle to load the reports. Parameters: run_id (str): the ID of the run Returns: Experiment: the experiment \"\"\" dst = os . path . join ( os . getcwd (), \"runs\" , run_id ) mlflow . artifacts . download_artifacts ( artifact_uri = f \"runs:/ { run_id } /experiment.pkl\" , dst_path = dst ) exp = joblib . load ( os . path . join ( dst , \"experiment.pkl\" )) if os . path . exists ( os . path . join ( dst , \"model.keras\" )): exp . model = tf . keras . models . load_model ( os . path . join ( dst , \"model.keras\" )) mlflow . artifacts . download_artifacts ( artifact_uri = f \"runs:/ { run_id } /reports\" , dst_path = dst ) report_dir = os . path . join ( dst , \"reports\" ) for file in os . listdir ( report_dir ): if file . endswith ( \".json\" ) and file . startswith ( \"eda_\" ): with open ( os . path . join ( report_dir , file ), \"rb\" ) as f : report = read_json ( f ) key = file [ 4 :] . split ( \".\" )[ 0 ] exp . _eda_registry [ key ] = report elif file . endswith ( \".json\" ): with open ( os . path . join ( report_dir , file ), \"rb\" ) as f : report = read_json ( f ) exp . _report_registry [ file . split ( \".\" )[ 0 ]] = report ##find .pkl files in report exp . model . _figs = exp . _report_registry exp . cfg = self . cfg exp . experiment_id = self . experiment_id exp . run_id = run_id ## load _report_registry ## assing it to model #exp.model = mlflow.sklearn.load_model(f\"runs:/{run_id}/model\") return exp def convert_datetime ( self , data : pd . DataFrame , format : str ): # TODO: get and return column only... if np . issubdtype ( data [ self . ds ] . dtype , np . integer ): data [ self . ds ] = pd . to_datetime ( data [ self . ds ], unit = format ) #, format = self.format .dt.strftime('%Y-%m-%d %H:%M:%S. %s') .astype(str), format = self.format elif data [ self . ds ] . dtype == str : data [ self . ds ] = pd . to_datetime ( data [ self . ds ], format = format ) #, format = self.format .dt.strftime('%Y-%m-%d %H:%M:%S. %s') .astype(str), format = self.format else : logging . warn ( f \"Unknown type { data [ self . ds ] . dtype } \" ) data [ self . ds ] = pd . to_datetime ( data [ self . ds ]) return data def eda ( self ): # interactions = None, \"\"\"This function performs simple exploratory data analysis. Returns: None \"\"\" print ( \"Performing EDA...\" ) ## add histograms self . _eda_registry = dict () for col in self . data . columns : fig = go . Figure () fig . add_trace ( go . Histogram ( x = self . data [ col ], name = col )) fig . update_layout ( title_text = f \" { col } Histogram\" ) fig . update_xaxes ( title_text = col ) fig . update_yaxes ( title_text = \"Count\" ) self . _eda_registry [ col ] = fig ## add correlation matrix labels = self . data . columns . to_list () labels . reverse () fig = go . Figure ( data = go . Heatmap ( z = self . data . corr (), x = labels , y = labels )) self . _eda_registry [ \"correlation_matrix\" ] = fig ## add missing values #fig = go.Figure(data=go.Heatmap(z=self.data.isnull().sum(), x=self.data.columns, y=self.data.columns)) #self._eda_registry[\"missing_values\"] = fig def retrain ( self ): if self . metric is not None : if self . higher_better : print ( self . metrics . iloc [ - 1 ] . loc [ self . metric ]) return self . metrics . iloc [ - 1 ] . loc [ self . metric ] < self . metric_threshold return self . metrics . iloc [ - 1 ] . loc [ self . metric ] > self . metric_threshold return False def spc ( self ): \"\"\"This function creates a statistical process control chart. WIP.\"\"\" pass def run ( self , data : pd . DataFrame ) -> None : \"\"\"This function runs the experiment. It trains the model and performs exploratory data analysis. Handles everythin internally... Parameters: data (pd.DataFrame): the training data \"\"\" if data is None : raise ValueError ( \"Data not found. Please provide data for training.\" ) if self . cfg is None : raise ValueError ( \"No training configuration found in metadata. The interface is not properly configured.\" ) ## for k in config run the function, with specified parameters... funcs = list ( self . cfg . keys ()) funcs . remove ( \"load_object\" ) self . data = data for k in funcs : if k == \"setup\" : self . setup ( data ) #self.eda( ) else : getattr ( self , k )() #self.setup(data, experiment_id, run_id) #self.model = self.create_model() #self.eda() # -> add eda to figs #self.spc() # -> add spc to figs #self.spc_chart() return None def join_data ( self ): \"\"\"Joins new data and previous train data.\"\"\" ndata = pd . concat (( self . data , self . new_data ), axis = 0 ) if self . retrain_window != 0 and self . retrain_window < ndata . shape [ 0 ]: ndata = ndata . iloc [ - self . retrain_window :, :] return ndata def export ( self ) -> None : \"\"\"This function exports the reports as HTML files and logs them to MLflow.\"\"\" with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : for k , v in self . _eda_registry . items (): v . write_html ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \"eda_ { k } _report.html\" )) mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \"eda_ { k } _report.html\" ), \"reports\" ) for k , v in self . model . _figs . items (): v . write_html ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } _report.html\" )) mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } _report.html\" ), \"reports\" ) def get_eda_reports ( self ) -> list [ str ]: \"\"\"This function returns the IDs of the EDA reports that are available in the model. Returns: list[str]: the IDs of the reports \"\"\" return list ( self . _eda_registry . keys ()) def get_fig_types ( self ) -> list [ str ]: \"\"\"This function returns the IDs of figures that are available in the model. Returns: list[str]: the IDs of figures \"\"\" return list ( self . model . _figs . keys ())","title":"Experiments"},{"location":"experiments/#framework.Experiment.Experiment.__init__","text":"Initialize the Experiment class. This class is a protocol that defines the methods that an experiment should implement. Parameters: cfg ( dict ) \u2013 the configuration of the experiment experiment_id ( str ) \u2013 the ID of the experiment run_id ( str ) \u2013 the ID of the run Source code in framework\\Experiment.py 75 76 77 78 79 80 81 82 83 84 85 86 def __init__ ( self , cfg : dict , experiment_id : str , run_id : str ) -> None : \"\"\"Initialize the Experiment class. This class is a protocol that defines the methods that an experiment should implement. Parameters: cfg (dict): the configuration of the experiment experiment_id (str): the ID of the experiment run_id (str): the ID of the run \"\"\" self . cfg = cfg self . experiment_id = experiment_id self . run_id = run_id","title":"__init__"},{"location":"experiments/#framework.Experiment.Experiment.create_model","text":"This function trains the model. Source code in framework\\Experiment.py 122 123 124 def create_model ( self , * args , ** kwargs ): \"\"\"This function trains the model.\"\"\" return NotImplementedError ( \"Implement this in the child class.\" )","title":"create_model"},{"location":"experiments/#framework.Experiment.Experiment.eda","text":"This function performs simple exploratory data analysis. Returns: \u2013 None Source code in framework\\Experiment.py 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 def eda ( self ): # interactions = None, \"\"\"This function performs simple exploratory data analysis. Returns: None \"\"\" print ( \"Performing EDA...\" ) ## add histograms self . _eda_registry = dict () for col in self . data . columns : fig = go . Figure () fig . add_trace ( go . Histogram ( x = self . data [ col ], name = col )) fig . update_layout ( title_text = f \" { col } Histogram\" ) fig . update_xaxes ( title_text = col ) fig . update_yaxes ( title_text = \"Count\" ) self . _eda_registry [ col ] = fig ## add correlation matrix labels = self . data . columns . to_list () labels . reverse () fig = go . Figure ( data = go . Heatmap ( z = self . data . corr (), x = labels , y = labels )) self . _eda_registry [ \"correlation_matrix\" ] = fig","title":"eda"},{"location":"experiments/#framework.Experiment.Experiment.export","text":"This function exports the reports as HTML files and logs them to MLflow. Source code in framework\\Experiment.py 332 333 334 335 336 337 338 339 340 341 def export ( self ) -> None : \"\"\"This function exports the reports as HTML files and logs them to MLflow.\"\"\" with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : for k , v in self . _eda_registry . items (): v . write_html ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \"eda_ { k } _report.html\" )) mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \"eda_ { k } _report.html\" ), \"reports\" ) for k , v in self . model . _figs . items (): v . write_html ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } _report.html\" )) mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } _report.html\" ), \"reports\" )","title":"export"},{"location":"experiments/#framework.Experiment.Experiment.format_data","text":"This function will provide a function to recover data from nonstandard json as pd.Dataframe. Parameters: data : data in json format or DataFrame format (dict) : settings for formatting. If None, then default pd.read_json is used. Source code in framework\\Experiment.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def format_data ( self , data , format : dict = None ) -> pd . DataFrame : \"\"\"This function will provide a function to recover data from nonstandard json as pd.Dataframe. Parameters: data : data in json format or DataFrame format (dict) : settings for formatting. If None, then default pd.read_json is used.\"\"\" if format is None : if isinstance ( data , pd . DataFrame ): return data else : from io import StringIO data = pd . read_json ( StringIO ( data )) return data else : if format . get ( \"name\" , \"pivot\" ) == \"pivot\" : data = json . loads ( data ) _id = format . get ( \"id\" , \"tsdata\" ) mxlvl = format . get ( \"max_level\" , 1 ) data = pd . json_normalize ( data [ _id ], max_level = 1 ) column = format . get ( \"columns\" , \"target\" ) ind = format . get ( \"index\" , \"date\" ) vals = format . get ( \"values\" , \"value\" ) data = data . pivot ( columns = column , index = ind , values = vals ) data . columns . name = None data . reset_index ( drop = False , inplace = True ) return data else : raise Exception ( f \"Configuration contains faulty data formatting settings. Please make sure the setup keyword contaisn the necessary information. Settings are: { format } \" )","title":"format_data"},{"location":"experiments/#framework.Experiment.Experiment.get_eda_reports","text":"This function returns the IDs of the EDA reports that are available in the model. Returns: list [ str ] \u2013 list[str]: the IDs of the reports Source code in framework\\Experiment.py 343 344 345 346 347 348 349 def get_eda_reports ( self ) -> list [ str ]: \"\"\"This function returns the IDs of the EDA reports that are available in the model. Returns: list[str]: the IDs of the reports \"\"\" return list ( self . _eda_registry . keys ())","title":"get_eda_reports"},{"location":"experiments/#framework.Experiment.Experiment.get_fig_types","text":"This function returns the IDs of figures that are available in the model. Returns: list [ str ] \u2013 list[str]: the IDs of figures Source code in framework\\Experiment.py 351 352 353 354 355 356 357 def get_fig_types ( self ) -> list [ str ]: \"\"\"This function returns the IDs of figures that are available in the model. Returns: list[str]: the IDs of figures \"\"\" return list ( self . model . _figs . keys ())","title":"get_fig_types"},{"location":"experiments/#framework.Experiment.Experiment.join_data","text":"Joins new data and previous train data. Source code in framework\\Experiment.py 323 324 325 326 327 328 329 330 def join_data ( self ): \"\"\"Joins new data and previous train data.\"\"\" ndata = pd . concat (( self . data , self . new_data ), axis = 0 ) if self . retrain_window != 0 and self . retrain_window < ndata . shape [ 0 ]: ndata = ndata . iloc [ - self . retrain_window :, :] return ndata","title":"join_data"},{"location":"experiments/#framework.Experiment.Experiment.load","text":"This function loads the experiment. It loads the model and the reports. Uses joblib to load the model and pickle to load the reports. Parameters: run_id ( str ) \u2013 the ID of the run Returns: Experiment ( Experiment ) \u2013 the experiment Source code in framework\\Experiment.py 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 def load ( self , run_id : str ) -> Experiment : \"\"\"This function loads the experiment. It loads the model and the reports. Uses joblib to load the model and pickle to load the reports. Parameters: run_id (str): the ID of the run Returns: Experiment: the experiment \"\"\" dst = os . path . join ( os . getcwd (), \"runs\" , run_id ) mlflow . artifacts . download_artifacts ( artifact_uri = f \"runs:/ { run_id } /experiment.pkl\" , dst_path = dst ) exp = joblib . load ( os . path . join ( dst , \"experiment.pkl\" )) if os . path . exists ( os . path . join ( dst , \"model.keras\" )): exp . model = tf . keras . models . load_model ( os . path . join ( dst , \"model.keras\" )) mlflow . artifacts . download_artifacts ( artifact_uri = f \"runs:/ { run_id } /reports\" , dst_path = dst ) report_dir = os . path . join ( dst , \"reports\" ) for file in os . listdir ( report_dir ): if file . endswith ( \".json\" ) and file . startswith ( \"eda_\" ): with open ( os . path . join ( report_dir , file ), \"rb\" ) as f : report = read_json ( f ) key = file [ 4 :] . split ( \".\" )[ 0 ] exp . _eda_registry [ key ] = report elif file . endswith ( \".json\" ): with open ( os . path . join ( report_dir , file ), \"rb\" ) as f : report = read_json ( f ) exp . _report_registry [ file . split ( \".\" )[ 0 ]] = report ##find .pkl files in report exp . model . _figs = exp . _report_registry exp . cfg = self . cfg exp . experiment_id = self . experiment_id exp . run_id = run_id ## load _report_registry ## assing it to model #exp.model = mlflow.sklearn.load_model(f\"runs:/{run_id}/model\") return exp","title":"load"},{"location":"experiments/#framework.Experiment.Experiment.plot_model","text":"This function plots the model. Parameters: plot ( str ) \u2013 the plot name that is to be displayed Returns: Figure \u2013 go.Figure: the plot Source code in framework\\Experiment.py 134 135 136 137 138 139 140 141 142 143 144 145 def plot_model ( self , plot : str ) -> go . Figure : \"\"\"This function plots the model. Parameters: plot (str): the plot name that is to be displayed Returns: go.Figure: the plot \"\"\" if plot not in self . model . _figs : raise ValueError ( f \"Plot { plot } not found in model.\" ) return self . model . _figs [ plot ]","title":"plot_model"},{"location":"experiments/#framework.Experiment.Experiment.predict","text":"This function predicts the target variable. Source code in framework\\Experiment.py 126 127 128 def predict ( self , data , * args , ** kwargs ): \"\"\"This function predicts the target variable.\"\"\" return NotImplementedError ( \"Implement this in the child class.\" )","title":"predict"},{"location":"experiments/#framework.Experiment.Experiment.run","text":"This function runs the experiment. It trains the model and performs exploratory data analysis. Handles everythin internally... Parameters: data ( DataFrame ) \u2013 the training data Source code in framework\\Experiment.py 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 def run ( self , data : pd . DataFrame ) -> None : \"\"\"This function runs the experiment. It trains the model and performs exploratory data analysis. Handles everythin internally... Parameters: data (pd.DataFrame): the training data \"\"\" if data is None : raise ValueError ( \"Data not found. Please provide data for training.\" ) if self . cfg is None : raise ValueError ( \"No training configuration found in metadata. The interface is not properly configured.\" ) ## for k in config run the function, with specified parameters... funcs = list ( self . cfg . keys ()) funcs . remove ( \"load_object\" ) self . data = data for k in funcs : if k == \"setup\" : self . setup ( data ) #self.eda( ) else : getattr ( self , k )() #self.setup(data, experiment_id, run_id) #self.model = self.create_model() #self.eda() # -> add eda to figs #self.spc() # -> add spc to figs #self.spc_chart() return None","title":"run"},{"location":"experiments/#framework.Experiment.Experiment.save","text":"This function saves the experiment. It saves the model and the reports. Uses joblib to save the model and pickle to save the reports. Source code in framework\\Experiment.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 def save ( self ): \"\"\"This function saves the experiment. It saves the model and the reports. Uses joblib to save the model and pickle to save the reports. \"\"\" with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : repository = get_artifact_repository ( run . info . artifact_uri ) try : repository . delete_artifacts ( mlflow . get_artifact_uri ( \"experiment.pkl\" )) repository . delete_artifacts ( mlflow . get_artifact_uri ( \"metadata.yaml\" )) repository . delete_artifacts ( mlflow . get_artifact_uri ( \"reports\" )) except : pass experiment_to_be_saved = deepcopy ( self ) if isinstance ( self . model , tf . keras . models . Model ): self . model . save ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"model.keras\" )) experiment_to_be_saved . model = None with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"model.keras\" ), \"model\" ) with open ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"experiment.pkl\" ), \"wb\" ) as f : joblib . dump ( experiment_to_be_saved , f ) for k , v in self . _report_registry . items (): with open ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } .json\" ), \"w\" ) as f : write_json ( v , f ) #os.path.join(os.getcwd(), \"runs\", self.run_id, \"reports\", f\"{k}.json\") with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } .json\" ), \"reports\" ) for k , v in self . _eda_registry . items (): with open ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \"eda_ { k } .json\" ), \"w\" ) as f : write_json ( v , f ) with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \"eda_ { k } .json\" ), \"reports\" ) yaml . dump ( self . cfg , open ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"metadata.yaml\" ), \"w\" )) with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"experiment.pkl\" ), \"\" ) mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"metadata.yaml\" ), \"\" )","title":"save"},{"location":"experiments/#framework.Experiment.Experiment.setup","text":"This function sets up the experiment. It is called before training the model. Source code in framework\\Experiment.py 118 119 120 def setup ( self , data : pd . DataFrame , * args , ** kwargs ): \"\"\"This function sets up the experiment. It is called before training the model.\"\"\" return NotImplementedError ( \"Implement this in the child class.\" )","title":"setup"},{"location":"experiments/#framework.Experiment.Experiment.spc","text":"This function creates a statistical process control chart. WIP. Source code in framework\\Experiment.py 285 286 287 def spc ( self ): \"\"\"This function creates a statistical process control chart. WIP.\"\"\" pass","title":"spc"},{"location":"experiments/#faultisolationexperiment","text":"Bases: Experiment This experiment file is used for fault isolation experiments. It is a subclass of the Experiment class in the framework.Experiment module. It is used to create, train, and predict using fault isolation models. Heavily relies on classification algorithms. Implemented models: Decision Tree : 'models.fault_isolation.DecisionTree' Random Forest : 'models.fault_isolation.RandomForest' Naive Bayes : 'models.fault_isolation.NaiveBayes' HMM : 'models.fault_isolation.HMM' Markov Chain : 'models.fault_isolation.MarkovChain' Inherits from Experiment Protocol class from framework.Experiment. If the experiment is overloaded, and new functions are added, one can call it in the system by adding the function name to the cfg file with relevant parameters. For example, if a new function 'new_function' is added to the system, the cfg file should have the following structure: cfg file should have the following structure: load_object: module: framework.FaultIsolation name: FaultIsolationExperiment setup: datetime_column: str The column that contains the datetime information. target: str The target column for the experiment. (output column) ... eda: create_model: model: str The model to be used for training. params: dict The parameters to be used for the model. new_function: param1: str The first parameter for the function. param2: str The second parameter for the function. ... Source code in framework\\FaultIsolation.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 class FaultIsolationExperiment ( Experiment ): \"\"\"This experiment file is used for fault isolation experiments. It is a subclass of the Experiment class in the framework.Experiment module. It is used to create, train, and predict using fault isolation models. Heavily relies on classification algorithms. Implemented models: - Decision Tree : 'models.fault_isolation.DecisionTree' - Random Forest : 'models.fault_isolation.RandomForest' - Naive Bayes : 'models.fault_isolation.NaiveBayes' - HMM : 'models.fault_isolation.HMM' - Markov Chain : 'models.fault_isolation.MarkovChain' Inherits from Experiment Protocol class from framework.Experiment. If the experiment is overloaded, and new functions are added, one can call it in the system by adding the function name to the cfg file with relevant parameters. For example, if a new function 'new_function' is added to the system, the cfg file should have the following structure: cfg file should have the following structure: ``` load_object: module: framework.FaultIsolation name: FaultIsolationExperiment setup: datetime_column: str The column that contains the datetime information. target: str The target column for the experiment. (output column) ... eda: create_model: model: str The model to be used for training. params: dict The parameters to be used for the model. new_function: param1: str The first parameter for the function. param2: str The second parameter for the function. ... ``` \"\"\" def __init__ ( self , cfg : dict , experiment_id : str , run_id : str , * args , ** kwargs ) -> None : \"\"\"Initialize the model registry. Currently cannot be changed after initialization. In future versions, we will allow for dynamic model loading through configuration files. Parameters: cfg (dict): The configuration file for the experiment. experiment_id (str): The experiment id for the experiment. run_id (str): The run id for the experiment. \"\"\" self . cfg = cfg self . experiment_id = experiment_id self . run_id = run_id self . _model_registry [ \"dt\" ] = DecisionTreeModel # decision tree self . _model_registry [ \"rf\" ] = RandomForestModel # random forest self . _model_registry [ \"nb\" ] = NaiveBayesModel # Naive Bayes self . _model_registry [ \"bn\" ] = BayesNet # Bayes Net! self . _model_registry [ \"hmm\" ] = HMM # hidden markov model self . _model_registry [ \"mc\" ] = MarkovChainModel # Markov Chain def setup ( self , data : str ) -> pd . DataFrame : \"\"\"Setup the data for training and prediction. This function is called before training the model. Parameters: data (str): The data to be used for training and prediction in json format. Returns: data (pd.DataFrame): The data set up for training and prediction. \"\"\" cfg = self . cfg [ \"setup\" ] self . data_format = cfg . get ( \"format\" , None ) data = self . format_data ( data , self . data_format ) self . data = data print ( self . data ) self . ds = cfg . get ( \"datetime_column\" , None ) self . format = cfg . get ( \"datetime_format\" , None ) self . predict_window = cfg . get ( \"predict_window\" , 0 ) if self . predict_window is None : self . predict_window = 0 if self . predict_window < 0 : self . predict_window *= - 1 retrain_cfg = cfg . get ( \"retrain\" , None ) if retrain_cfg is None or len ( retrain_cfg ) == 0 : self . retrain_window = 0 self . metric = None self . metric_threshold = 0.0 self . higher_better = True else : self . retrain_window = retrain_cfg . get ( \"retrain_window\" , 0 ) self . metric = retrain_cfg . get ( \"metric\" , None ) self . metric_threshold = retrain_cfg . get ( \"metric_threshold\" , 0.0 ) self . higher_better = retrain_cfg . get ( \"higher_better\" , True ) self . target = cfg [ \"target\" ] # self.target = self.target.replace(\" \", \"_\") # replace spaces with underscores # self.target = self.target.replace(\"//\", \"\") # self.target = self.target.replace(\"/\", \"\") # self.target = self.target.replace(\"(\", \"\") # self.target = self.target.replace(\")\", \"\") # self.target = self.target.replace(\"\\\\\", \"\") # self.target = self.target.replace(\".\", \"\") ## TODO: Abstrct data VC -- integrate dvc or other data versioning tools. if self . ds is not None : self . data = self . convert_datetime ( self . data , format = self . format ) self . data . set_index ( self . ds , inplace = True ) self . data . rename ( columns = { self . target : \"target\" }, inplace = True ) self . data [ \"target\" ] = self . data [ \"target\" ] . astype ( \"category\" ) self . input_scheme = self . data . columns . to_list () self . input_scheme . remove ( \"target\" ) return data def create_model ( self , * args , ** kwargs ) -> any : \"\"\"Create the model using the configuration file. The model is trained on the data set up in the 'setup' function. Returns: (any) The trained model.\"\"\" model = self . cfg [ \"create_model\" ][ \"model\" ] params = self . cfg [ \"create_model\" ] . get ( \"params\" , None ) if params is None : params = dict () if self . data is None : raise ValueError ( \"Data not found. Please use 'setup' to setup the data first.\" ) try : model_class = self . _model_registry [ model ] except KeyError : raise ValueError ( f \"Model { model } not found in model registry. Please check configuration file. Available models are { self . _model_registry . keys () } .\" ) self . model = model_class ( ** params ) . fit ( self . data ) self . _report_registry = self . model . _figs y = self . model . predict ( self . data ) try : self . metrics = self . _score ( self . data [ \"target\" ], y [ \"y_pred\" ]) self . _report_registry [ \"metrics\" ] = px . line ( self . metrics , x = self . metrics . index , y = self . metrics . columns , markers = True ) except Exception as e : self . metrics = pd . DataFrame ([ np . NaN , np . NaN , np . NaN , np . NaN ], index = [ \"Accuracy\" , \"F1\" , \"Precision\" , \"Recall\" ]) . transpose () logging . warn ( f \"Error calculating metrics: { e } . Model { self . model . __class__ . __name__ } may not have y_pred as output.\" ) with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : mlflow . set_tag ( \"model\" , model ) for k , v in self . model . _figs . items (): v . write_html ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } .html\" )) mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } .html\" ), \"reports\" ) for k , v in params . items (): mlflow . log_param ( k , v ) return self . model def predict ( self , data : str ): \"\"\"Predict using the trained model. The model should be trained before calling this function. Parameters: data (str): The data to be used for prediction in json format. Returns: (pd.DataFrame) The predictions made by the model.\"\"\" data = self . format_data ( data , self . data_format ) if self . model is None : raise ValueError ( \"Model not found. Please train the model first with 'create_model'.\" ) X = data . copy () if self . ds is not None : X = self . convert_datetime ( X , format = self . format ) X . set_index ( self . ds , inplace = True ) if self . target in X . columns : X . rename ( columns = { self . target : \"target\" }, inplace = True ) y = self . model . predict ( X ) metrics = self . _score ( X [ \"target\" ], y [ \"y_pred\" ]) self . metrics = pd . concat ([ self . metrics , metrics ], axis = 0 , ignore_index = True ) self . new_data = pd . concat ([ self . new_data , y ], axis = 0 ) if self . predict_window > self . new_data . shape [ 0 ] : self . model . update_predict ( self . new_data , reset_fig = True , update_fig = True ) else : self . model . update_predict ( self . new_data . iloc [ - self . predict_window :, :], reset_fig = True , update_fig = True ) self . _report_registry [ \"metrics\" ] = px . line ( self . metrics , x = self . metrics . index , y = self . metrics . columns , markers = True ) return y , metrics def _score ( self , y , y_hat ): \"\"\"Score the model using the predictions. This function is called after the predictions are made. Parameters: y (pd.Series): The actual values. y_hat (pd.Series): The predicted values. Returns: (pd.DataFrame) The metrics calculated for the model.\"\"\" if self . model is None : raise ValueError ( \"Model not found. Please train the model first with 'create_model'.\" ) if y is None : raise ValueError ( \"No data found for scoring. Please provide data for scoring.\" ) if y_hat is None : raise ValueError ( \"No predictions found for scoring. Please provide predictions for scoring.\" ) acc = accuracy_score ( y , y_hat ) f1 = f1_score ( y , y_hat ) prec = precision_score ( y , y_hat ) rec = recall_score ( y , y_hat ) return pd . DataFrame ([ acc , f1 , prec , rec ], index = [ \"Accuracy\" , \"F1\" , \"Precision\" , \"Recall\" ]) . transpose ()","title":"FaultIsolationExperiment"},{"location":"experiments/#framework.FaultIsolation.FaultIsolationExperiment.__init__","text":"Initialize the model registry. Currently cannot be changed after initialization. In future versions, we will allow for dynamic model loading through configuration files. Parameters: cfg (dict): The configuration file for the experiment. experiment_id (str): The experiment id for the experiment. run_id (str): The run id for the experiment. Source code in framework\\FaultIsolation.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def __init__ ( self , cfg : dict , experiment_id : str , run_id : str , * args , ** kwargs ) -> None : \"\"\"Initialize the model registry. Currently cannot be changed after initialization. In future versions, we will allow for dynamic model loading through configuration files. Parameters: cfg (dict): The configuration file for the experiment. experiment_id (str): The experiment id for the experiment. run_id (str): The run id for the experiment. \"\"\" self . cfg = cfg self . experiment_id = experiment_id self . run_id = run_id self . _model_registry [ \"dt\" ] = DecisionTreeModel # decision tree self . _model_registry [ \"rf\" ] = RandomForestModel # random forest self . _model_registry [ \"nb\" ] = NaiveBayesModel # Naive Bayes self . _model_registry [ \"bn\" ] = BayesNet # Bayes Net! self . _model_registry [ \"hmm\" ] = HMM # hidden markov model self . _model_registry [ \"mc\" ] = MarkovChainModel # Markov Chain","title":"__init__"},{"location":"experiments/#framework.FaultIsolation.FaultIsolationExperiment.create_model","text":"Create the model using the configuration file. The model is trained on the data set up in the 'setup' function. Returns: any \u2013 (any) The trained model. Source code in framework\\FaultIsolation.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def create_model ( self , * args , ** kwargs ) -> any : \"\"\"Create the model using the configuration file. The model is trained on the data set up in the 'setup' function. Returns: (any) The trained model.\"\"\" model = self . cfg [ \"create_model\" ][ \"model\" ] params = self . cfg [ \"create_model\" ] . get ( \"params\" , None ) if params is None : params = dict () if self . data is None : raise ValueError ( \"Data not found. Please use 'setup' to setup the data first.\" ) try : model_class = self . _model_registry [ model ] except KeyError : raise ValueError ( f \"Model { model } not found in model registry. Please check configuration file. Available models are { self . _model_registry . keys () } .\" ) self . model = model_class ( ** params ) . fit ( self . data ) self . _report_registry = self . model . _figs y = self . model . predict ( self . data ) try : self . metrics = self . _score ( self . data [ \"target\" ], y [ \"y_pred\" ]) self . _report_registry [ \"metrics\" ] = px . line ( self . metrics , x = self . metrics . index , y = self . metrics . columns , markers = True ) except Exception as e : self . metrics = pd . DataFrame ([ np . NaN , np . NaN , np . NaN , np . NaN ], index = [ \"Accuracy\" , \"F1\" , \"Precision\" , \"Recall\" ]) . transpose () logging . warn ( f \"Error calculating metrics: { e } . Model { self . model . __class__ . __name__ } may not have y_pred as output.\" ) with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : mlflow . set_tag ( \"model\" , model ) for k , v in self . model . _figs . items (): v . write_html ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } .html\" )) mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } .html\" ), \"reports\" ) for k , v in params . items (): mlflow . log_param ( k , v ) return self . model","title":"create_model"},{"location":"experiments/#framework.FaultIsolation.FaultIsolationExperiment.predict","text":"Predict using the trained model. The model should be trained before calling this function. Parameters: data ( str ) \u2013 The data to be used for prediction in json format. Returns: \u2013 (pd.DataFrame) The predictions made by the model. Source code in framework\\FaultIsolation.py 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 def predict ( self , data : str ): \"\"\"Predict using the trained model. The model should be trained before calling this function. Parameters: data (str): The data to be used for prediction in json format. Returns: (pd.DataFrame) The predictions made by the model.\"\"\" data = self . format_data ( data , self . data_format ) if self . model is None : raise ValueError ( \"Model not found. Please train the model first with 'create_model'.\" ) X = data . copy () if self . ds is not None : X = self . convert_datetime ( X , format = self . format ) X . set_index ( self . ds , inplace = True ) if self . target in X . columns : X . rename ( columns = { self . target : \"target\" }, inplace = True ) y = self . model . predict ( X ) metrics = self . _score ( X [ \"target\" ], y [ \"y_pred\" ]) self . metrics = pd . concat ([ self . metrics , metrics ], axis = 0 , ignore_index = True ) self . new_data = pd . concat ([ self . new_data , y ], axis = 0 ) if self . predict_window > self . new_data . shape [ 0 ] : self . model . update_predict ( self . new_data , reset_fig = True , update_fig = True ) else : self . model . update_predict ( self . new_data . iloc [ - self . predict_window :, :], reset_fig = True , update_fig = True ) self . _report_registry [ \"metrics\" ] = px . line ( self . metrics , x = self . metrics . index , y = self . metrics . columns , markers = True ) return y , metrics","title":"predict"},{"location":"experiments/#framework.FaultIsolation.FaultIsolationExperiment.setup","text":"Setup the data for training and prediction. This function is called before training the model. Parameters: data (str): The data to be used for training and prediction in json format. Returns: data ( DataFrame ) \u2013 The data set up for training and prediction. Source code in framework\\FaultIsolation.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def setup ( self , data : str ) -> pd . DataFrame : \"\"\"Setup the data for training and prediction. This function is called before training the model. Parameters: data (str): The data to be used for training and prediction in json format. Returns: data (pd.DataFrame): The data set up for training and prediction. \"\"\" cfg = self . cfg [ \"setup\" ] self . data_format = cfg . get ( \"format\" , None ) data = self . format_data ( data , self . data_format ) self . data = data print ( self . data ) self . ds = cfg . get ( \"datetime_column\" , None ) self . format = cfg . get ( \"datetime_format\" , None ) self . predict_window = cfg . get ( \"predict_window\" , 0 ) if self . predict_window is None : self . predict_window = 0 if self . predict_window < 0 : self . predict_window *= - 1 retrain_cfg = cfg . get ( \"retrain\" , None ) if retrain_cfg is None or len ( retrain_cfg ) == 0 : self . retrain_window = 0 self . metric = None self . metric_threshold = 0.0 self . higher_better = True else : self . retrain_window = retrain_cfg . get ( \"retrain_window\" , 0 ) self . metric = retrain_cfg . get ( \"metric\" , None ) self . metric_threshold = retrain_cfg . get ( \"metric_threshold\" , 0.0 ) self . higher_better = retrain_cfg . get ( \"higher_better\" , True ) self . target = cfg [ \"target\" ] # self.target = self.target.replace(\" \", \"_\") # replace spaces with underscores # self.target = self.target.replace(\"//\", \"\") # self.target = self.target.replace(\"/\", \"\") # self.target = self.target.replace(\"(\", \"\") # self.target = self.target.replace(\")\", \"\") # self.target = self.target.replace(\"\\\\\", \"\") # self.target = self.target.replace(\".\", \"\") ## TODO: Abstrct data VC -- integrate dvc or other data versioning tools. if self . ds is not None : self . data = self . convert_datetime ( self . data , format = self . format ) self . data . set_index ( self . ds , inplace = True ) self . data . rename ( columns = { self . target : \"target\" }, inplace = True ) self . data [ \"target\" ] = self . data [ \"target\" ] . astype ( \"category\" ) self . input_scheme = self . data . columns . to_list () self . input_scheme . remove ( \"target\" ) return data","title":"setup"},{"location":"experiments/#faultdetectionexperiment","text":"Bases: Experiment Source code in framework\\FaultDetection.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 class FaultDetectionExperiment ( Experiment ): def __init__ ( self , cfg : dict , experiment_id : str , run_id : str , * args , ** kwargs ) -> None : \"\"\"Initialize the model registry. Currently cannot be changed after initialization. In future versions, we will allow for dynamic model loading through configuration files. This experiment file is used for fault detection experiments. It is a subclass of the Experiment class in the framework.Experiment module. It is used to create, train, and predict using fault detection models. Heavily relies on outlier detection/clustering algorithms. Parameters: cfg: dict The configuration file for the experiment. experiment_id: str The experiment id for the experiment. run_id: str The run id for the experiment. Implemented models: - DBSCAN : 'models.fault_detection.DBSCANAnomalyDetection' - Elliptic Envelope 'models.fault_detection.EllipticEnvelopeAnomalyDetection' - Isolation Forest 'models.fault_detection.IsolationForestAnomaly' - PCA 'models.fault_detection.PCAAnomalyDetection' Inherits from Experiment Protocol class from framework.Experiment. If the experiment is overloaded, and new functions are added, one can call it in the system by adding the function name to the cfg file with relevant parameters. For example, if a new function 'new_function' is added to the system, the cfg file should have the following structure: cfg file should have the following structure: ``` load_object: module: framework.FaultIsolation name: FaultIsolationExperiment setup: datetime_column: str The column that contains the datetime information. target: str The target column for the experiment. (output column) ... eda: create_model: model: str The model to be used for training. params: dict The parameters to be used for the model. new_function: param1: str The first parameter for the function. param2: str The second parameter for the function. ``` The function will then be called in the system by calling the 'run' function. experiment.run(data, experiment_id, run_id) \"\"\" self . cfg = cfg self . experiment_id = experiment_id self . run_id = run_id self . _model_registry [ \"dbscan\" ] = DBSCANAnomalyDetection self . _model_registry [ \"ee\" ] = EllipticEnvelopeAnomalyDetection self . _model_registry [ \"iforest\" ] = IsolationForestAnomaly self . _model_registry [ \"pca\" ] = PCAAnomalyDetection def setup ( self , data : str ) -> pd . DataFrame : \"\"\"Setup the data for training and prediction. This function is called before any other function to set self.data that can be used in any other function. Also returns the data if need be. Parameters: data (str): The data to be used for training and prediction. Returns: pd.DataFrame The data that was set for the experiment. \"\"\" cfg = self . cfg [ \"setup\" ] self . ds = cfg . get ( \"datetime_column\" , None ) self . format = cfg . get ( \"datetime_format\" , None ) self . data_format = cfg . get ( \"format\" , None ) data = self . format_data ( data , self . data_format ) self . predict_window = cfg . get ( \"predict_window\" , 0 ) if self . predict_window is None : self . predict_window = 0 if self . predict_window < 0 : self . predict_window *= - 1 self . data = data if self . ds is not None : self . data = self . convert_datetime ( self . data , format = self . format ) self . data . set_index ( self . ds , inplace = True ) return data def create_model ( self , * args , ** kwargs ): \"\"\"Create the model using the configuration file. The model is trained on the data set up in the 'setup' function. Returns: any The trained model. \"\"\" model = self . cfg [ \"create_model\" ][ \"model\" ] params = self . cfg [ \"create_model\" ] . get ( \"params\" , None ) if params is None : params = dict () if self . data is None : raise ValueError ( \"Data not found. Please use 'setup' to setup the data first.\" ) try : model_class = self . _model_registry [ model ] except KeyError : raise ValueError ( f \"Model { model } not found in model registry. Please check configuration file. Available models are { self . _model_registry . keys () } .\" ) self . model = model_class ( ** params ) . fit ( self . data ) self . _report_registry = self . model . _figs with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : mlflow . set_tag ( \"model\" , model ) for k , v in self . model . _figs . items (): v . write_html ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } .html\" )) mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } .html\" ), \"reports\" ) #mlflow.sklearn.log_model(self.model, \"model\") for k , v in params . items (): mlflow . log_param ( k , v ) return self . model def predict ( self , data : str ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\"Predict using the trained model. The model should be trained before calling this function. Parameters: data (str): The data to be used for prediction. Returns: Tuple[pd.DataFrame, pd.DataFrame] The predictions made by the model and the metrics calculated for the model. \"\"\" data = self . format_data ( data , self . data_format ) if self . model is None : raise ValueError ( \"Model not found. Please train the model first with 'create_model'.\" ) X = data . copy () if self . ds is not None : X = self . convert_datetime ( X , format = self . format ) X . set_index ( self . ds , inplace = True ) y = self . model . predict ( X ) self . new_data = pd . concat ([ self . new_data , y ], axis = 0 ) if self . predict_window > self . new_data . shape [ 0 ] : self . model . update_predict ( self . new_data , reset_fig = True , update_fig = True ) else : self . model . update_predict ( self . new_data . iloc [ - self . predict_window :, :], reset_fig = True , update_fig = True ) return y , pd . DataFrame ([]) def _score ( self , y , y_hat ): \"\"\"Score the model using the predictions. This function is called after the predictions are made. Parameters: y: pd.Series The actual values. y_hat: pd.Series The predicted values. Returns: pd.DataFrame The metrics calculated for the model. \"\"\" logging . log ( \"Scoring function not implemented.\" ) return pd . DataFrame ([])","title":"FaultDetectionExperiment"},{"location":"experiments/#framework.FaultDetection.FaultDetectionExperiment.__init__","text":"Initialize the model registry. Currently cannot be changed after initialization. In future versions, we will allow for dynamic model loading through configuration files. This experiment file is used for fault detection experiments. It is a subclass of the Experiment class in the framework.Experiment module. It is used to create, train, and predict using fault detection models. Heavily relies on outlier detection/clustering algorithms. Parameters: cfg ( dict ) \u2013 dict The configuration file for the experiment. experiment_id ( str ) \u2013 str The experiment id for the experiment. run_id ( str ) \u2013 str The run id for the experiment. Implemented models: - DBSCAN : 'models.fault_detection.DBSCANAnomalyDetection' - Elliptic Envelope 'models.fault_detection.EllipticEnvelopeAnomalyDetection' - Isolation Forest 'models.fault_detection.IsolationForestAnomaly' - PCA 'models.fault_detection.PCAAnomalyDetection' Inherits from Experiment Protocol class from framework.Experiment. If the experiment is overloaded, and new functions are added, one can call it in the system by adding the function name to the cfg file with relevant parameters. For example, if a new function 'new_function' is added to the system, the cfg file should have the following structure: cfg file should have the following structure: load_object: module: framework.FaultIsolation name: FaultIsolationExperiment setup: datetime_column: str The column that contains the datetime information. target: str The target column for the experiment. (output column) ... eda: create_model: model: str The model to be used for training. params: dict The parameters to be used for the model. new_function: param1: str The first parameter for the function. param2: str The second parameter for the function. The function will then be called in the system by calling the 'run' function. experiment.run(data, experiment_id, run_id) Source code in framework\\FaultDetection.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def __init__ ( self , cfg : dict , experiment_id : str , run_id : str , * args , ** kwargs ) -> None : \"\"\"Initialize the model registry. Currently cannot be changed after initialization. In future versions, we will allow for dynamic model loading through configuration files. This experiment file is used for fault detection experiments. It is a subclass of the Experiment class in the framework.Experiment module. It is used to create, train, and predict using fault detection models. Heavily relies on outlier detection/clustering algorithms. Parameters: cfg: dict The configuration file for the experiment. experiment_id: str The experiment id for the experiment. run_id: str The run id for the experiment. Implemented models: - DBSCAN : 'models.fault_detection.DBSCANAnomalyDetection' - Elliptic Envelope 'models.fault_detection.EllipticEnvelopeAnomalyDetection' - Isolation Forest 'models.fault_detection.IsolationForestAnomaly' - PCA 'models.fault_detection.PCAAnomalyDetection' Inherits from Experiment Protocol class from framework.Experiment. If the experiment is overloaded, and new functions are added, one can call it in the system by adding the function name to the cfg file with relevant parameters. For example, if a new function 'new_function' is added to the system, the cfg file should have the following structure: cfg file should have the following structure: ``` load_object: module: framework.FaultIsolation name: FaultIsolationExperiment setup: datetime_column: str The column that contains the datetime information. target: str The target column for the experiment. (output column) ... eda: create_model: model: str The model to be used for training. params: dict The parameters to be used for the model. new_function: param1: str The first parameter for the function. param2: str The second parameter for the function. ``` The function will then be called in the system by calling the 'run' function. experiment.run(data, experiment_id, run_id) \"\"\" self . cfg = cfg self . experiment_id = experiment_id self . run_id = run_id self . _model_registry [ \"dbscan\" ] = DBSCANAnomalyDetection self . _model_registry [ \"ee\" ] = EllipticEnvelopeAnomalyDetection self . _model_registry [ \"iforest\" ] = IsolationForestAnomaly self . _model_registry [ \"pca\" ] = PCAAnomalyDetection","title":"__init__"},{"location":"experiments/#framework.FaultDetection.FaultDetectionExperiment.create_model","text":"Create the model using the configuration file. The model is trained on the data set up in the 'setup' function. Returns: any The trained model. Source code in framework\\FaultDetection.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def create_model ( self , * args , ** kwargs ): \"\"\"Create the model using the configuration file. The model is trained on the data set up in the 'setup' function. Returns: any The trained model. \"\"\" model = self . cfg [ \"create_model\" ][ \"model\" ] params = self . cfg [ \"create_model\" ] . get ( \"params\" , None ) if params is None : params = dict () if self . data is None : raise ValueError ( \"Data not found. Please use 'setup' to setup the data first.\" ) try : model_class = self . _model_registry [ model ] except KeyError : raise ValueError ( f \"Model { model } not found in model registry. Please check configuration file. Available models are { self . _model_registry . keys () } .\" ) self . model = model_class ( ** params ) . fit ( self . data ) self . _report_registry = self . model . _figs with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : mlflow . set_tag ( \"model\" , model ) for k , v in self . model . _figs . items (): v . write_html ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } .html\" )) mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } .html\" ), \"reports\" ) #mlflow.sklearn.log_model(self.model, \"model\") for k , v in params . items (): mlflow . log_param ( k , v ) return self . model","title":"create_model"},{"location":"experiments/#framework.FaultDetection.FaultDetectionExperiment.predict","text":"Predict using the trained model. The model should be trained before calling this function. Parameters: data (str): The data to be used for prediction. Returns: Tuple [ DataFrame , DataFrame ] \u2013 Tuple[pd.DataFrame, pd.DataFrame] The predictions made by the model and the metrics calculated for the model. Source code in framework\\FaultDetection.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 def predict ( self , data : str ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\"Predict using the trained model. The model should be trained before calling this function. Parameters: data (str): The data to be used for prediction. Returns: Tuple[pd.DataFrame, pd.DataFrame] The predictions made by the model and the metrics calculated for the model. \"\"\" data = self . format_data ( data , self . data_format ) if self . model is None : raise ValueError ( \"Model not found. Please train the model first with 'create_model'.\" ) X = data . copy () if self . ds is not None : X = self . convert_datetime ( X , format = self . format ) X . set_index ( self . ds , inplace = True ) y = self . model . predict ( X ) self . new_data = pd . concat ([ self . new_data , y ], axis = 0 ) if self . predict_window > self . new_data . shape [ 0 ] : self . model . update_predict ( self . new_data , reset_fig = True , update_fig = True ) else : self . model . update_predict ( self . new_data . iloc [ - self . predict_window :, :], reset_fig = True , update_fig = True ) return y , pd . DataFrame ([])","title":"predict"},{"location":"experiments/#framework.FaultDetection.FaultDetectionExperiment.setup","text":"Setup the data for training and prediction. This function is called before any other function to set self.data that can be used in any other function. Also returns the data if need be. Parameters: data ( str ) \u2013 The data to be used for training and prediction. Returns: DataFrame \u2013 pd.DataFrame The data that was set for the experiment. Source code in framework\\FaultDetection.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def setup ( self , data : str ) -> pd . DataFrame : \"\"\"Setup the data for training and prediction. This function is called before any other function to set self.data that can be used in any other function. Also returns the data if need be. Parameters: data (str): The data to be used for training and prediction. Returns: pd.DataFrame The data that was set for the experiment. \"\"\" cfg = self . cfg [ \"setup\" ] self . ds = cfg . get ( \"datetime_column\" , None ) self . format = cfg . get ( \"datetime_format\" , None ) self . data_format = cfg . get ( \"format\" , None ) data = self . format_data ( data , self . data_format ) self . predict_window = cfg . get ( \"predict_window\" , 0 ) if self . predict_window is None : self . predict_window = 0 if self . predict_window < 0 : self . predict_window *= - 1 self . data = data if self . ds is not None : self . data = self . convert_datetime ( self . data , format = self . format ) self . data . set_index ( self . ds , inplace = True ) return data","title":"setup"},{"location":"experiments/#timeseriesanomalyexperiment","text":"Bases: Experiment Class for interacting with time series models. Plays a similar role to pycaret experiment. Deals with model training, predicting, metrics and serialization. Inherits from Experiment Protocol class from framework.Experiment. Implemented models: - Autoencoder : 'models.time_series_analysis.Autoencoder' - LSTM : 'models.time_series_analysis.LSTM' - Prophet : 'models.time_series_analysis.Prophet' - SSA : 'models.time_series_analysis.SSA' - ARIMA : 'models.time_series_analysis.ARIMA' - Exponential Smoothing : 'models.time_series_analysis.ExponentialSmoothing' If the experiment is overloaded, and new functions are added, one can call it in the system by adding the function name to the cfg file with relevant parameters. For example, if a new function 'new_function' is added to the system, the cfg file should have the following structure: cfg file should have the following structure: load_object: module: framework.TimeSeriesAnalysis name: TimeSeriesAnomalyExperiment setup: ds : str The column that contains the datetime information. y : str The target column for the experiment. (output column) ... create_model: model: str The model to be used for training. params: dict The parameters to be used for the model. ... new_function: # e.g. posterior reconciliation of results. param1: str The first parameter for the function. param2: str The second parameter for the function. Source code in framework\\TimeSeriesAnalysis.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 class TimeSeriesAnomalyExperiment ( Experiment ): \"\"\"Class for interacting with time series models. Plays a similar role to pycaret experiment. Deals with model training, predicting, metrics and serialization. Inherits from Experiment Protocol class from framework.Experiment. Implemented models: - Autoencoder : 'models.time_series_analysis.Autoencoder' - LSTM : 'models.time_series_analysis.LSTM' - Prophet : 'models.time_series_analysis.Prophet' - SSA : 'models.time_series_analysis.SSA' - ARIMA : 'models.time_series_analysis.ARIMA' - Exponential Smoothing : 'models.time_series_analysis.ExponentialSmoothing' If the experiment is overloaded, and new functions are added, one can call it in the system by adding the function name to the cfg file with relevant parameters. For example, if a new function 'new_function' is added to the system, the cfg file should have the following structure: cfg file should have the following structure: ``` load_object: module: framework.TimeSeriesAnalysis name: TimeSeriesAnomalyExperiment setup: ds : str The column that contains the datetime information. y : str The target column for the experiment. (output column) ... create_model: model: str The model to be used for training. params: dict The parameters to be used for the model. ... new_function: # e.g. posterior reconciliation of results. param1: str The first parameter for the function. param2: str The second parameter for the function. ``` \"\"\" def __init__ ( self , cfg : dict , experiment_id : str , run_id : str ) -> None : \"\"\"Initialize the model registry. Currently cannot be changed after initialization. In future versions, we will allow for dynamic model loading through configuration files. This experiment function is used for loading basic time series anomaly detection models. Parameters: cfg: dict The configuration file for the experiment. experiment_id: str The experiment id for the experiment. run_id: str The run id for the experiment. \"\"\" self . cfg = cfg self . experiment_id = experiment_id self . run_id = run_id self . _model_registry [ \"ae\" ] = Autoencoder self . _model_registry [ \"lstm\" ] = LSTM self . _model_registry [ \"prophet\" ] = ProphetAnomalyDetection self . _model_registry [ \"ssa\" ] = SSAAnomalyDetection self . _model_registry [ \"arima\" ] = ARIMAAnomalyDetector self . _model_registry [ \"es\" ] = ExponentialSmoothingAnomaly def setup ( self , data : str , * args , ** kwargs ): \"\"\"Setup the data for training and prediction. This function is called before training the model. In the future, it will also be used to preprocess the data and prepare it for training. Parameters: data: pd.DataFrame The data to be used for training and prediction. Returns: pd.DataFrame The data after processing. This data is used for training and prediction. Description: - The setup function is used to prepare the data for training and prediction. It is called before the model is trained. - The function renames the columns to 'ds' and 'y' for consistency. - The function logs the target and datestamp columns to mlflow. - The function returns the data after processing. \"\"\" cfg = self . cfg [ \"setup\" ] self . data_format = cfg . get ( \"format\" , None ) data = self . format_data ( data , self . data_format ) self . ds = cfg [ \"datetime_column\" ] self . target = cfg [ \"target\" ] self . format = cfg [ \"datetime_format\" ] self . predict_window = cfg . get ( \"predict_window\" , 0 ) if self . predict_window is None : self . predict_window = 0 if self . predict_window < 0 : self . predict_window *= - 1 retrain_cfg = cfg . get ( \"retrain\" , None ) if retrain_cfg is None or len ( retrain_cfg ) == 0 : self . retrain_window = 0 self . metric = None self . metric_threshold = 0.0 self . higher_better = True else : self . retrain_window = retrain_cfg . get ( \"retrain_window\" , 0 ) self . metric = retrain_cfg . get ( \"metric\" , None ) self . metric_threshold = retrain_cfg . get ( \"metric_threshold\" , 0.0 ) self . higher_better = retrain_cfg . get ( \"higher_better\" , True ) if self . ds not in data . columns : raise ValueError ( \"Datestamp column not found in data. Configuration file must specify datestamp column as ds.\" ) if self . target not in data . columns : raise ValueError ( \"Target column not found in data. Configuration file must specify target column as target.\" ) data = self . convert_datetime ( data , format = self . format ) data . rename ( columns = { self . ds : \"ds\" , self . target : \"y\" }, inplace = True ) self . data = data . loc [:, [ \"ds\" , \"y\" ]] self . input_scheme = [] with mlflow . start_run ( nested = True , experiment_id = self . experiment_id ) as run : mlflow . log_param ( \"target\" , self . target ) mlflow . log_param ( \"datetime_column\" , self . ds ) return data def create_model ( self , * args , ** kwargs ): \"\"\"Create the model using the configuration file. The model is trained on the data set up in the 'setup' function. Returns: any The trained model. Description: - The function creates the model using the configuration file. - The function logs the model and parameters to mlflow. - The function logs the metrics to mlflow. - The function saves the model to disk. - The function returns the trained model. \"\"\" model = self . cfg [ \"create_model\" ][ \"model\" ] params = self . cfg [ \"create_model\" ] . get ( \"params\" , None ) if params is None : params = dict () if self . data is None : raise ValueError ( \"Data not found. Please use 'setup' to setup the data first.\" ) try : model_class = self . _model_registry [ model ] except KeyError : raise ValueError ( f \"Model { model } not found in model registry. Please check configuration file. Available models are { self . _model_registry . keys () } .\" ) self . model = model_class ( ** params ) . fit ( self . data ) self . _report_registry = self . model . _figs y = self . model . predict ( self . data ) try : self . metrics = self . _score ( self . data [ self . target ], y [ \"y_pred\" ]) self . _report_registry [ \"metrics\" ] = px . line ( self . metrics , x = self . metrics . index , y = self . metrics . columns , markers = True ) except Exception as e : self . metrics = pd . DataFrame ([ np . NaN , np . NaN , np . NaN , np . NaN ], index = [ \"MSE\" , \"MAE\" , \"R2\" , \"MAPE\" ]) . transpose () logging . warn ( f \"Error calculating metrics: { e } . Model { self . model } may not have y_pred as output.\" ) with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : mlflow . set_tag ( \"model\" , model ) #mlflow.sklearn.save_model(self.model, f\"runs/{self.run_id}/model\") for k , v in params . items (): mlflow . log_param ( k , v ) for key , value in self . metrics . to_dict () . items (): mlflow . log_metric ( key , value [ 0 ]) for k , v in self . _report_registry . items (): v . write_html ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } _report.html\" )) mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } _report.html\" ), \"reports\" ) return self . model def predict ( self , data : str ): \"\"\"Predict using the trained model. The model should be trained before calling this function. Parameters: data str The data to be used for prediction. Returns: pd.DataFrame The predictions made by the model. Description: - The function predicts using the trained model. - The function updates the predict figures with the new data. - The function calculates the metrics for the model. - The function logs the metrics to mlflow. - The function saves the reports to disk. - The function logs the reports to mlflow. - The function returns the predictions and metrics. \"\"\" data = self . format_data ( data , self . data_format ) if self . model is None : raise ValueError ( \"Model not found. Please train the model first with 'create_model'.\" ) data . rename ( columns = { self . ds : \"ds\" , self . target : \"y\" }, inplace = True ) data = self . convert_datetime ( data , format = self . format ) data = data . loc [:, [ \"ds\" , \"y\" ]] y = self . model . predict ( data ) self . new_data = pd . concat ([ self . new_data , y ], axis = 0 ) if self . predict_window > self . new_data . shape [ 0 ] : self . model . update_predict ( self . new_data , reset_fig = True , update_fig = True ) else : self . model . update_predict ( self . new_data . iloc [ - self . predict_window :, :], reset_fig = True , update_fig = True ) if \"y_pred\" not in y . columns : metrics = pd . DataFrame ([ np . NaN , np . NaN , np . NaN , np . NaN ], index = [ \"MSE\" , \"MAE\" , \"R2\" , \"MAPE\" ]) . transpose () logging . warn ( \"y_pred not found in model output. Please make sure the model has a 'predict' method that returns a DataFrame with 'y_pred' column.\" ) else : metrics = self . _score ( data [ self . target ], y [ \"y_pred\" ]) self . metrics = pd . concat ([ self . metrics , metrics ], axis = 0 , ignore_index = True ) self . _report_registry [ \"metrics\" ] = px . line ( self . metrics , x = self . metrics . index , y = self . metrics . columns , markers = True ) #self.spc_chart(update_fig=True) self . _report_registry [ \"predict\" ] . write_html ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , \"predict_report.html\" )) self . _report_registry [ \"metrics\" ] . write_html ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , \"metrics_report.html\" )) with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , \"predict_report.html\" ), \"reports\" ) mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , \"metrics_report.html\" ), \"reports\" ) return y , metrics def _score ( self , y , y_hat ): \"\"\"Calculate the metrics for the model. Parameters: y: pd.Series The true values. y_hat: pd.Series The predicted values. Returns: pd.DataFrame The metrics for the model. Description: - The function calculates the metrics for the model. - The function returns: - Mean Squared Error - Mean Absolute Error - R2 Score - Mean Absolute Percentage Error \"\"\" assert isinstance ( y , pd . Series ) or isinstance ( y , pd . DataFrame ), \"y must be a pandas Series or DataFrame.\" assert isinstance ( y_hat , pd . Series ) or isinstance ( y_hat , pd . DataFrame ), \"y_hat must be a pandas Series or DataFrame.\" if isinstance ( y , pd . Series ): y = y . to_frame () if isinstance ( y_hat , pd . Series ): y_hat = y_hat . to_frame () assert y . shape == y_hat . shape , \"y and y_hat must have the same shape.\" mse = mean_squared_error ( y , y_hat ) mae = mean_absolute_error ( y , y_hat ) r2 = r2_score ( y , y_hat ) mape = mean_absolute_percentage_error ( y , y_hat ) return pd . DataFrame ([ mse , mae , r2 , mape ], index = [ \"MSE\" , \"MAE\" , \"R2\" , \"MAPE\" ]) . transpose ()","title":"TimeSeriesAnomalyExperiment"},{"location":"experiments/#framework.TimeSeriesAnalysis.TimeSeriesAnomalyExperiment.__init__","text":"Initialize the model registry. Currently cannot be changed after initialization. In future versions, we will allow for dynamic model loading through configuration files. This experiment function is used for loading basic time series anomaly detection models. Parameters: cfg: dict The configuration file for the experiment. experiment_id: str The experiment id for the experiment. run_id: str The run id for the experiment. Source code in framework\\TimeSeriesAnalysis.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def __init__ ( self , cfg : dict , experiment_id : str , run_id : str ) -> None : \"\"\"Initialize the model registry. Currently cannot be changed after initialization. In future versions, we will allow for dynamic model loading through configuration files. This experiment function is used for loading basic time series anomaly detection models. Parameters: cfg: dict The configuration file for the experiment. experiment_id: str The experiment id for the experiment. run_id: str The run id for the experiment. \"\"\" self . cfg = cfg self . experiment_id = experiment_id self . run_id = run_id self . _model_registry [ \"ae\" ] = Autoencoder self . _model_registry [ \"lstm\" ] = LSTM self . _model_registry [ \"prophet\" ] = ProphetAnomalyDetection self . _model_registry [ \"ssa\" ] = SSAAnomalyDetection self . _model_registry [ \"arima\" ] = ARIMAAnomalyDetector self . _model_registry [ \"es\" ] = ExponentialSmoothingAnomaly","title":"__init__"},{"location":"experiments/#framework.TimeSeriesAnalysis.TimeSeriesAnomalyExperiment.create_model","text":"Create the model using the configuration file. The model is trained on the data set up in the 'setup' function. Returns: any The trained model. Description: - The function creates the model using the configuration file. - The function logs the model and parameters to mlflow. - The function logs the metrics to mlflow. - The function saves the model to disk. - The function returns the trained model. Source code in framework\\TimeSeriesAnalysis.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 def create_model ( self , * args , ** kwargs ): \"\"\"Create the model using the configuration file. The model is trained on the data set up in the 'setup' function. Returns: any The trained model. Description: - The function creates the model using the configuration file. - The function logs the model and parameters to mlflow. - The function logs the metrics to mlflow. - The function saves the model to disk. - The function returns the trained model. \"\"\" model = self . cfg [ \"create_model\" ][ \"model\" ] params = self . cfg [ \"create_model\" ] . get ( \"params\" , None ) if params is None : params = dict () if self . data is None : raise ValueError ( \"Data not found. Please use 'setup' to setup the data first.\" ) try : model_class = self . _model_registry [ model ] except KeyError : raise ValueError ( f \"Model { model } not found in model registry. Please check configuration file. Available models are { self . _model_registry . keys () } .\" ) self . model = model_class ( ** params ) . fit ( self . data ) self . _report_registry = self . model . _figs y = self . model . predict ( self . data ) try : self . metrics = self . _score ( self . data [ self . target ], y [ \"y_pred\" ]) self . _report_registry [ \"metrics\" ] = px . line ( self . metrics , x = self . metrics . index , y = self . metrics . columns , markers = True ) except Exception as e : self . metrics = pd . DataFrame ([ np . NaN , np . NaN , np . NaN , np . NaN ], index = [ \"MSE\" , \"MAE\" , \"R2\" , \"MAPE\" ]) . transpose () logging . warn ( f \"Error calculating metrics: { e } . Model { self . model } may not have y_pred as output.\" ) with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : mlflow . set_tag ( \"model\" , model ) #mlflow.sklearn.save_model(self.model, f\"runs/{self.run_id}/model\") for k , v in params . items (): mlflow . log_param ( k , v ) for key , value in self . metrics . to_dict () . items (): mlflow . log_metric ( key , value [ 0 ]) for k , v in self . _report_registry . items (): v . write_html ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } _report.html\" )) mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } _report.html\" ), \"reports\" ) return self . model","title":"create_model"},{"location":"experiments/#framework.TimeSeriesAnalysis.TimeSeriesAnomalyExperiment.predict","text":"Predict using the trained model. The model should be trained before calling this function. Parameters: data str The data to be used for prediction. Returns: \u2013 pd.DataFrame The predictions made by the model. Description: - The function predicts using the trained model. - The function updates the predict figures with the new data. - The function calculates the metrics for the model. - The function logs the metrics to mlflow. - The function saves the reports to disk. - The function logs the reports to mlflow. - The function returns the predictions and metrics. Source code in framework\\TimeSeriesAnalysis.py 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 def predict ( self , data : str ): \"\"\"Predict using the trained model. The model should be trained before calling this function. Parameters: data str The data to be used for prediction. Returns: pd.DataFrame The predictions made by the model. Description: - The function predicts using the trained model. - The function updates the predict figures with the new data. - The function calculates the metrics for the model. - The function logs the metrics to mlflow. - The function saves the reports to disk. - The function logs the reports to mlflow. - The function returns the predictions and metrics. \"\"\" data = self . format_data ( data , self . data_format ) if self . model is None : raise ValueError ( \"Model not found. Please train the model first with 'create_model'.\" ) data . rename ( columns = { self . ds : \"ds\" , self . target : \"y\" }, inplace = True ) data = self . convert_datetime ( data , format = self . format ) data = data . loc [:, [ \"ds\" , \"y\" ]] y = self . model . predict ( data ) self . new_data = pd . concat ([ self . new_data , y ], axis = 0 ) if self . predict_window > self . new_data . shape [ 0 ] : self . model . update_predict ( self . new_data , reset_fig = True , update_fig = True ) else : self . model . update_predict ( self . new_data . iloc [ - self . predict_window :, :], reset_fig = True , update_fig = True ) if \"y_pred\" not in y . columns : metrics = pd . DataFrame ([ np . NaN , np . NaN , np . NaN , np . NaN ], index = [ \"MSE\" , \"MAE\" , \"R2\" , \"MAPE\" ]) . transpose () logging . warn ( \"y_pred not found in model output. Please make sure the model has a 'predict' method that returns a DataFrame with 'y_pred' column.\" ) else : metrics = self . _score ( data [ self . target ], y [ \"y_pred\" ]) self . metrics = pd . concat ([ self . metrics , metrics ], axis = 0 , ignore_index = True ) self . _report_registry [ \"metrics\" ] = px . line ( self . metrics , x = self . metrics . index , y = self . metrics . columns , markers = True ) #self.spc_chart(update_fig=True) self . _report_registry [ \"predict\" ] . write_html ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , \"predict_report.html\" )) self . _report_registry [ \"metrics\" ] . write_html ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , \"metrics_report.html\" )) with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , \"predict_report.html\" ), \"reports\" ) mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , \"metrics_report.html\" ), \"reports\" ) return y , metrics","title":"predict"},{"location":"experiments/#framework.TimeSeriesAnalysis.TimeSeriesAnomalyExperiment.setup","text":"Setup the data for training and prediction. This function is called before training the model. In the future, it will also be used to preprocess the data and prepare it for training. Parameters: data: pd.DataFrame The data to be used for training and prediction. Returns: \u2013 pd.DataFrame The data after processing. This data is used for training and prediction. Description: The setup function is used to prepare the data for training and prediction. It is called before the model is trained. The function renames the columns to 'ds' and 'y' for consistency. The function logs the target and datestamp columns to mlflow. The function returns the data after processing. Source code in framework\\TimeSeriesAnalysis.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def setup ( self , data : str , * args , ** kwargs ): \"\"\"Setup the data for training and prediction. This function is called before training the model. In the future, it will also be used to preprocess the data and prepare it for training. Parameters: data: pd.DataFrame The data to be used for training and prediction. Returns: pd.DataFrame The data after processing. This data is used for training and prediction. Description: - The setup function is used to prepare the data for training and prediction. It is called before the model is trained. - The function renames the columns to 'ds' and 'y' for consistency. - The function logs the target and datestamp columns to mlflow. - The function returns the data after processing. \"\"\" cfg = self . cfg [ \"setup\" ] self . data_format = cfg . get ( \"format\" , None ) data = self . format_data ( data , self . data_format ) self . ds = cfg [ \"datetime_column\" ] self . target = cfg [ \"target\" ] self . format = cfg [ \"datetime_format\" ] self . predict_window = cfg . get ( \"predict_window\" , 0 ) if self . predict_window is None : self . predict_window = 0 if self . predict_window < 0 : self . predict_window *= - 1 retrain_cfg = cfg . get ( \"retrain\" , None ) if retrain_cfg is None or len ( retrain_cfg ) == 0 : self . retrain_window = 0 self . metric = None self . metric_threshold = 0.0 self . higher_better = True else : self . retrain_window = retrain_cfg . get ( \"retrain_window\" , 0 ) self . metric = retrain_cfg . get ( \"metric\" , None ) self . metric_threshold = retrain_cfg . get ( \"metric_threshold\" , 0.0 ) self . higher_better = retrain_cfg . get ( \"higher_better\" , True ) if self . ds not in data . columns : raise ValueError ( \"Datestamp column not found in data. Configuration file must specify datestamp column as ds.\" ) if self . target not in data . columns : raise ValueError ( \"Target column not found in data. Configuration file must specify target column as target.\" ) data = self . convert_datetime ( data , format = self . format ) data . rename ( columns = { self . ds : \"ds\" , self . target : \"y\" }, inplace = True ) self . data = data . loc [:, [ \"ds\" , \"y\" ]] self . input_scheme = [] with mlflow . start_run ( nested = True , experiment_id = self . experiment_id ) as run : mlflow . log_param ( \"target\" , self . target ) mlflow . log_param ( \"datetime_column\" , self . ds ) return data","title":"setup"},{"location":"experiments/#processminingexperiment","text":"Bases: Experiment This experiment file is used for process mining experiments. It is a subclass of the Experiment class in the framework.Experiment module. It is used to create, train, and predict using process mining models. Heavily relies on sequence mining algorithms. Implemented models: - Apriori : 'models.spmf.Apriori' - CMSPAM : 'models.spmf.CM_SPAM' - TopKRules :'models.spmf.TopKRules' - Heuristics Miner : 'models.spmf.HeuristicsMiner' Inherits from Experiment Protocol class from framework.Experiment. If the experiment is overloaded, and new functions are added, one can call it in the system by adding the function name to the cfg file with relevant parameters. For example, if a new function 'new_function' is added to the system, the cfg file should have the following structure: cfg file should have the following structure: load_object: module: framework.ProcessMining name: ProcessMiningExperiment setup: ... create_model: model: str The model to be used for training. params: dict The parameters to be used for the model. new_function: param1: str The first parameter for the function. param2: str The second parameter for the function. ... Source code in framework\\ProcessMining.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 class ProcessMiningExperiment ( Experiment ): \"\"\" This experiment file is used for process mining experiments. It is a subclass of the Experiment class in the framework.Experiment module. It is used to create, train, and predict using process mining models. Heavily relies on sequence mining algorithms. Implemented models: - Apriori : 'models.spmf.Apriori' - CMSPAM : 'models.spmf.CM_SPAM' - TopKRules :'models.spmf.TopKRules' - Heuristics Miner : 'models.spmf.HeuristicsMiner' Inherits from Experiment Protocol class from framework.Experiment. If the experiment is overloaded, and new functions are added, one can call it in the system by adding the function name to the cfg file with relevant parameters. For example, if a new function 'new_function' is added to the system, the cfg file should have the following structure: cfg file should have the following structure: ``` load_object: module: framework.ProcessMining name: ProcessMiningExperiment setup: ... create_model: model: str The model to be used for training. params: dict The parameters to be used for the model. new_function: param1: str The first parameter for the function. param2: str The second parameter for the function. ... ``` \"\"\" def __init__ ( self , cfg : dict , experiment_id : str , run_id : str , * args , ** kwargs ) -> None : \"\"\"Initialize the model registry. Currently cannot be changed after initialization. Parameters: cfg: dict The configuration file for the experiment. experiment_id: str The experiment id for the experiment. run_id: str The run id for the experiment. \"\"\" self . cfg = cfg self . experiment_id = experiment_id self . run_id = run_id self . _model_registry [ \"apriori\" ] = Apriori self . _model_registry [ \"cmspam\" ] = CMSPAM self . _model_registry [ \"topk\" ] = TopKRules self . _model_registry [ \"heuristics\" ] = HeuristicsMiner def setup ( self , data : pd . DataFrame ): \"\"\"Setup the data for training and prediction. This function is called before training the model. Data must be in pandas DataFrame format - can be a columns, with several rows. Parameters: data: pd.DataFrame The data to be used for training and prediction. Returns: pd.DataFrame The data after processing. This data is used for training and prediction \"\"\" cfg = self . cfg [ \"setup\" ] self . data_format = cfg . get ( \"format\" , None ) data = self . format_data ( data , self . data_format ) # col_names = dict(\"Start_timestamp\" : \"start:timestamp\", # \"End_timestamp\" : \"time:timestamp\", # \"Event\" : \"concept:name\", # \"Case_id\" : \"case:concept:name\", # \"Resource\" : \"org:resource\", # \"Ordered\" : \"Ordered\", # \"Completed\" : \"Completed\", # \"Rejected\" : \"Rejected\", # \"MRB\" : \"MRB\", # \"Part\" : \"Part\") #data.rename(columns=col_names, inplace=True) #TODO : excavate a log ... self . data = data #.drop(columns=[self.ds]) return data def create_model ( self , * args , ** kwargs ) -> any : \"\"\"Create the model using the configuration file. The model is trained on the data set up in the 'setup' function. Returns: any The trained model. \"\"\" model = self . cfg [ \"create_model\" ][ \"model\" ] params = self . cfg [ \"create_model\" ] . get ( \"params\" , None ) if params is None : params = dict () if self . data is None : raise ValueError ( \"Data not found. Please use 'setup' to setup the data first.\" ) try : model_class = self . _model_registry [ model ] except KeyError : raise ValueError ( f \"Model { model } not found in model registry. Please check configuration file. Available models are { self . _model_registry . keys () } .\" ) ## data.to_csv() self . model = model_class ( ** params ) . fit ( self . data ) self . _report_registry = self . model . _figs with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : mlflow . set_tag ( \"model\" , model ) for k , v in self . model . _figs . items (): v . write_html ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } .html\" )) mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } .html\" ), \"reports\" ) for k , v in params . items (): mlflow . log_param ( k , v ) return self . model def predict ( self , data : pd . DataFrame ): \"\"\"Predict using the trained model. The model should be trained before calling this function. Parameters: data: pd.DataFrame The data to be used for prediction. Returns: pd.DataFrame The predictions made by the model. \"\"\" if self . model is None : raise ValueError ( \"Model not found. Please train the model first with 'create_model'.\" ) X = data . copy () y = self . model . predict ( X ) return y , pd . DataFrame ([]) def _score ( self , y , y_hat ): \"\"\"Score the model using the predictions. This function is called after the predictions are made. Parameters: y: pd.Series The actual values. y_hat: pd.Series The predicted values. Returns: pd.DataFrame The scores for the model.\"\"\" if self . model is None : raise ValueError ( \"Model not found. Please train the model first with 'create_model'.\" ) if y is None : raise ValueError ( \"No data found for scoring. Please provide data for scoring.\" ) if y_hat is None : raise ValueError ( \"No predictions found for scoring. Please provide predictions for scoring.\" ) return pd . DataFrame ([])","title":"ProcessMiningExperiment"},{"location":"experiments/#framework.ProcessMining.ProcessMiningExperiment.__init__","text":"Initialize the model registry. Currently cannot be changed after initialization. Parameters: cfg: dict The configuration file for the experiment. experiment_id: str The experiment id for the experiment. run_id: str The run id for the experiment. Source code in framework\\ProcessMining.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def __init__ ( self , cfg : dict , experiment_id : str , run_id : str , * args , ** kwargs ) -> None : \"\"\"Initialize the model registry. Currently cannot be changed after initialization. Parameters: cfg: dict The configuration file for the experiment. experiment_id: str The experiment id for the experiment. run_id: str The run id for the experiment. \"\"\" self . cfg = cfg self . experiment_id = experiment_id self . run_id = run_id self . _model_registry [ \"apriori\" ] = Apriori self . _model_registry [ \"cmspam\" ] = CMSPAM self . _model_registry [ \"topk\" ] = TopKRules self . _model_registry [ \"heuristics\" ] = HeuristicsMiner","title":"__init__"},{"location":"experiments/#framework.ProcessMining.ProcessMiningExperiment.create_model","text":"Create the model using the configuration file. The model is trained on the data set up in the 'setup' function. Returns: any The trained model. Source code in framework\\ProcessMining.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def create_model ( self , * args , ** kwargs ) -> any : \"\"\"Create the model using the configuration file. The model is trained on the data set up in the 'setup' function. Returns: any The trained model. \"\"\" model = self . cfg [ \"create_model\" ][ \"model\" ] params = self . cfg [ \"create_model\" ] . get ( \"params\" , None ) if params is None : params = dict () if self . data is None : raise ValueError ( \"Data not found. Please use 'setup' to setup the data first.\" ) try : model_class = self . _model_registry [ model ] except KeyError : raise ValueError ( f \"Model { model } not found in model registry. Please check configuration file. Available models are { self . _model_registry . keys () } .\" ) ## data.to_csv() self . model = model_class ( ** params ) . fit ( self . data ) self . _report_registry = self . model . _figs with mlflow . start_run ( nested = True , experiment_id = self . experiment_id , run_id = self . run_id ) as run : mlflow . set_tag ( \"model\" , model ) for k , v in self . model . _figs . items (): v . write_html ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } .html\" )) mlflow . log_artifact ( os . path . join ( os . getcwd (), \"runs\" , self . run_id , \"reports\" , f \" { k } .html\" ), \"reports\" ) for k , v in params . items (): mlflow . log_param ( k , v ) return self . model","title":"create_model"},{"location":"experiments/#framework.ProcessMining.ProcessMiningExperiment.predict","text":"Predict using the trained model. The model should be trained before calling this function. Parameters: data: pd.DataFrame The data to be used for prediction. Returns: \u2013 pd.DataFrame The predictions made by the model. Source code in framework\\ProcessMining.py 134 135 136 137 138 139 140 141 142 143 144 145 146 def predict ( self , data : pd . DataFrame ): \"\"\"Predict using the trained model. The model should be trained before calling this function. Parameters: data: pd.DataFrame The data to be used for prediction. Returns: pd.DataFrame The predictions made by the model. \"\"\" if self . model is None : raise ValueError ( \"Model not found. Please train the model first with 'create_model'.\" ) X = data . copy () y = self . model . predict ( X ) return y , pd . DataFrame ([])","title":"predict"},{"location":"experiments/#framework.ProcessMining.ProcessMiningExperiment.setup","text":"Setup the data for training and prediction. This function is called before training the model. Data must be in pandas DataFrame format - can be a columns, with several rows. Parameters: data ( DataFrame ) \u2013 pd.DataFrame The data to be used for training and prediction. Returns: \u2013 pd.DataFrame The data after processing. This data is used for training and prediction Source code in framework\\ProcessMining.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def setup ( self , data : pd . DataFrame ): \"\"\"Setup the data for training and prediction. This function is called before training the model. Data must be in pandas DataFrame format - can be a columns, with several rows. Parameters: data: pd.DataFrame The data to be used for training and prediction. Returns: pd.DataFrame The data after processing. This data is used for training and prediction \"\"\" cfg = self . cfg [ \"setup\" ] self . data_format = cfg . get ( \"format\" , None ) data = self . format_data ( data , self . data_format ) # col_names = dict(\"Start_timestamp\" : \"start:timestamp\", # \"End_timestamp\" : \"time:timestamp\", # \"Event\" : \"concept:name\", # \"Case_id\" : \"case:concept:name\", # \"Resource\" : \"org:resource\", # \"Ordered\" : \"Ordered\", # \"Completed\" : \"Completed\", # \"Rejected\" : \"Rejected\", # \"MRB\" : \"MRB\", # \"Part\" : \"Part\") #data.rename(columns=col_names, inplace=True) #TODO : excavate a log ... self . data = data #.drop(columns=[self.ds]) return data","title":"setup"},{"location":"localdev/","text":"Running the ObServML Application locally Setting up the development/local environment. The \"main\" branch contains production ready codes, which can be used to build Docker images. Images play a definitive role in running the application on several instances, and provide an easy way to set up the system. Locally running the system is preferable during development phase, as you do not have to wait for the image build time. Note, this tutorial is only required if you DO NOT USE DOCKER . Follow the steps to set up the environment for the \"main\" branch of the repository: install Git download and install graphviz download and install python 3.11.9 Create a virtual environment and project. git clone git@github.com:adamipkovich/observml.git -- The repository is publicly available. git pull git checkout main optional -- navigate into the folder (cd or through IDE) - or open it as a separate project. create a python virtual environment, preferably through IDE pip install -r requirements.txt --> these are required by the backend. pip install streamlit hydra-core mkdocs uvicorn gunicorn tensorflow or you can pip install -r full_deps.txt which will recreate my virtual environment, and will install various unused packages. Running the app locally. To start mlflow, open up a terminal and enter: mlflow server Rabbit docker image is a must have. docker run --publish 5100:5672 rabbitmq:3.12.14-management-alpine To start backend, enter this into the command line: python ExperimentHubAPI.py Frontend: streamlit run streamlit_frontend.py Why use the app locally? If you are trying to fix the code or simply experiment with it, it might be beneficial not to start a Linux container just to test a feature. The template project in the \"main\" repository contains test cases, therefore, one can also test without switching to other branches. For example, run: python train_script_local.py after starting all services, provided that at least one of the configs.yaml's test cases are NOT commented out. Interfaces If the compose file has not been changed, then the mlflow client can be opened thought a webbrowser: http://localhost:5000 The backend and frontend is also viewable: Backend: http://localhost:8010/docs Frontend: http://localhost:8105 The frontend will show error if no models are found. DO NOT PANIC! It will work after the first training/loading session. If everything opened, then congratulations, you have set up the application!","title":"Local Development"},{"location":"localdev/#running-the-observml-application-locally","text":"","title":"Running the ObServML Application locally"},{"location":"localdev/#setting-up-the-developmentlocal-environment","text":"The \"main\" branch contains production ready codes, which can be used to build Docker images. Images play a definitive role in running the application on several instances, and provide an easy way to set up the system. Locally running the system is preferable during development phase, as you do not have to wait for the image build time. Note, this tutorial is only required if you DO NOT USE DOCKER . Follow the steps to set up the environment for the \"main\" branch of the repository: install Git download and install graphviz download and install python 3.11.9 Create a virtual environment and project. git clone git@github.com:adamipkovich/observml.git -- The repository is publicly available. git pull git checkout main optional -- navigate into the folder (cd or through IDE) - or open it as a separate project. create a python virtual environment, preferably through IDE pip install -r requirements.txt --> these are required by the backend. pip install streamlit hydra-core mkdocs uvicorn gunicorn tensorflow or you can pip install -r full_deps.txt which will recreate my virtual environment, and will install various unused packages.","title":"Setting up the development/local environment."},{"location":"localdev/#running-the-app-locally","text":"To start mlflow, open up a terminal and enter: mlflow server Rabbit docker image is a must have. docker run --publish 5100:5672 rabbitmq:3.12.14-management-alpine To start backend, enter this into the command line: python ExperimentHubAPI.py Frontend: streamlit run streamlit_frontend.py Why use the app locally? If you are trying to fix the code or simply experiment with it, it might be beneficial not to start a Linux container just to test a feature. The template project in the \"main\" repository contains test cases, therefore, one can also test without switching to other branches. For example, run: python train_script_local.py after starting all services, provided that at least one of the configs.yaml's test cases are NOT commented out.","title":"Running the app locally."},{"location":"localdev/#interfaces","text":"If the compose file has not been changed, then the mlflow client can be opened thought a webbrowser: http://localhost:5000 The backend and frontend is also viewable: Backend: http://localhost:8010/docs Frontend: http://localhost:8105 The frontend will show error if no models are found. DO NOT PANIC! It will work after the first training/loading session. If everything opened, then congratulations, you have set up the application!","title":"Interfaces"},{"location":"models/","text":"Models and Configuration Guide This section provides comprehensive examples of machine learning models available in ObServML, organized by experiment type. Each model includes practical configuration examples from real use cases, REST API usage patterns, and parameter explanations. ObServML supports four main experiment types: - Time Series Analysis : Forecasting and anomaly detection in temporal data - Fault Detection : Unsupervised anomaly detection in sensor data - Fault Isolation : Supervised classification for root cause analysis - Process Mining : Analysis of sequential processes and workflows Configuration Structure All models follow a consistent YAML configuration structure: load_object: module: framework.{ExperimentType} name: {ExperimentType}Experiment setup: datetime_column: \"timestamp\" # Column containing timestamps target: \"target_variable\" # Target variable (for supervised learning) # Additional setup parameters... eda: # Optional: Exploratory Data Analysis create_model: model: \"model_name\" params: # Model-specific parameters Time Series Analysis Time Series Analysis experiments are designed for monitoring sensor data over time, offering insights into individual sensor behavior and enabling predictive maintenance through forecasting and anomaly detection. Prophet Prophet is excellent for time series forecasting with strong seasonal patterns and holiday effects. Configuration Example: load_object: module: framework.TimeSeriesAnalysis name: TimeSeriesAnomalyExperiment setup: datetime_column: \"ds\" datetime_format: \"ms\" target: \"y\" predict_window: 1000 retrain: retrain_window: 5000 metric: \"MAPE\" metric_threshold: 0.3 higher_better: false eda: create_model: model: \"prophet\" params: periods: 0 factor: 1.0 forecast_window: 100 REST API Usage: # Train Prophet model curl -X POST \"http://localhost:8010/timeseries_prophet/train\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"load_object\": { \"module\": \"framework.TimeSeriesAnalysis\", \"name\": \"TimeSeriesAnomalyExperiment\" }, \"setup\": { \"datetime_column\": \"ds\", \"target\": \"y\", \"predict_window\": 1000 }, \"create_model\": { \"model\": \"prophet\", \"params\": { \"periods\": 0, \"factor\": 1.0, \"forecast_window\": 100 } } }' # Make predictions curl -X POST \"http://localhost:8010/timeseries_prophet/predict\" # Get forecast plot curl \"http://localhost:8010/timeseries_prophet/plot/forecast\" Parameters: - periods : Number of periods to forecast forward - factor : Anomaly detection factor - forecast_window : Number of future points to predict ARIMA/SARIMA ARIMA models are ideal for stationary time series with clear autoregressive patterns. Configuration Example: load_object: module: framework.TimeSeriesAnalysis name: TimeSeriesAnomalyExperiment setup: datetime_column: \"ds\" target: \"y\" eda: create_model: model: \"arima\" params: start_p: 10 d: 1 start_q: 10 max_p: 100 max_q: 100 seasonal: false threshold_for_anomaly: 3 Parameters: - start_p : Starting value for autoregressive order - d : Differencing order - start_q : Starting value for moving average order - max_p/max_q : Maximum values for parameter search - seasonal : Enable seasonal ARIMA (SARIMA) - threshold_for_anomaly : Standard deviations for anomaly detection LSTM Long Short-Term Memory networks for complex temporal patterns and non-linear relationships. Configuration Example: load_object: module: framework.TimeSeriesAnalysis name: TimeSeriesAnomalyExperiment setup: datetime_column: \"ds\" target: \"y\" eda: create_model: model: \"lstm\" params: seq_length: 50 layer_no: 2 cell_no: 64 epoch_no: 100 batch_size: 32 shuffle: true patience: 10 threshold_for_anomaly: 3 Parameters: - seq_length : Length of input sequences - layer_no : Number of LSTM layers - cell_no : Number of cells per LSTM layer - epoch_no : Training epochs - batch_size : Training batch size - patience : Early stopping patience - threshold_for_anomaly : Anomaly detection threshold Autoencoder Neural network autoencoders for anomaly detection in time series through reconstruction error. Configuration Example: load_object: module: framework.TimeSeriesAnalysis name: TimeSeriesAnomalyExperiment setup: datetime_column: \"ds\" target: \"y\" eda: create_model: model: \"ae\" params: layer_no: 6 window: 250 epoch_no: 50 batch_size: 64 shuffle: false threshold_for_anomaly: 3 neuron_no_enc: [30, 25, 20, 15, 10, 5] neuron_no_dec: [5, 10, 15, 20, 25, 30] act_enc: 'relu' act_dec: 'relu' Parameters: - layer_no : Number of layers in encoder/decoder - window : Moving window size for sequences - neuron_no_enc/dec : Neurons per layer in encoder/decoder - act_enc/dec : Activation functions SSA (Singular Spectrum Analysis) SSA for time series decomposition and anomaly detection. Configuration Example: load_object: module: framework.TimeSeriesAnalysis name: TimeSeriesAnomalyExperiment setup: datetime_column: \"ds\" target: \"y\" eda: create_model: model: \"ssa\" params: window_size: 10 lower_frequency_bound: 0.05 lower_frequency_contribution: 0.975 threshold: 3 Fault Detection Fault Detection experiments focus on unsupervised anomaly detection in multivariate sensor data, identifying deviations from normal operating conditions without requiring labeled data. Isolation Forest Isolation Forest is highly effective for anomaly detection in high-dimensional data by isolating anomalies through random partitioning. Configuration Example: load_object: module: framework.FaultDetection name: FaultDetectionExperiment setup: datetime_column: \"ds\" datetime_format: \"ms\" eda: create_model: model: \"iforest\" params: n_estimators: 100 contamination: \"auto\" random_state: 0 REST API Usage: # Train Isolation Forest model curl -X POST \"http://localhost:8010/pump_anomaly/train\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"load_object\": { \"module\": \"framework.FaultDetection\", \"name\": \"FaultDetectionExperiment\" }, \"setup\": { \"datetime_column\": \"ds\" }, \"create_model\": { \"model\": \"iforest\", \"params\": { \"n_estimators\": 100, \"contamination\": \"auto\" } } }' # Detect anomalies in new data curl -X POST \"http://localhost:8010/pump_anomaly/predict\" # Get anomaly visualization curl \"http://localhost:8010/pump_anomaly/plot/outliers\" Parameters: - n_estimators : Number of isolation trees - contamination : Expected proportion of anomalies (\"auto\" for automatic detection) - random_state : Random seed for reproducibility PCA (Principal Component Analysis) PCA with Hotelling's T\u00b2 and SPE tests for multivariate anomaly detection, as demonstrated in the research paper with pump sensor data. Configuration Example: load_object: module: framework.FaultDetection name: FaultDetectionExperiment setup: datetime_column: \"ds\" datetime_format: \"ms\" eda: create_model: model: \"pca\" params: alpha: 0.05 detect_outliers: ['ht2', 'spe'] n_components: 0.95 normalize: true Use Case Example (from research paper): The pump dataset contains 50 sensors without labeled data, making it ideal for fault detection. PCA reduces dimensionality while preserving 95% of variance, then applies statistical tests to identify anomalies. Parameters: - alpha : Significance level for Hotelling's T\u00b2 test - detect_outliers : Types of outlier detection ['ht2', 'spe'] - n_components : Variance explained by principal components - normalize : Apply data normalization DBSCAN Density-based clustering for anomaly detection, effective for identifying clusters of normal behavior. Configuration Example: load_object: module: framework.FaultDetection name: FaultDetectionExperiment setup: datetime_column: \"ds\" datetime_format: \"ms\" eda: create_model: model: \"dbscan\" params: eps: 2 min_samples: 5 Parameters: - eps : Maximum distance between points in a neighborhood - min_samples : Minimum points required to form a cluster Elliptic Envelope Robust covariance estimation for outlier detection in multivariate data. Configuration Example: load_object: module: framework.FaultDetection name: FaultDetectionExperiment setup: datetime_column: \"ds\" datetime_format: \"ms\" eda: create_model: model: \"ee\" params: contamination: 0.1 Parameters: - contamination : Proportion of outliers in the dataset (0.0 to 0.5) Fault Isolation Fault Isolation experiments perform supervised classification when labeled data is available, enabling root cause analysis and identification of specific fault types or machine states. Decision Tree Decision trees provide interpretable classification with feature importance analysis, as demonstrated in the research paper with electrical circuit data. Configuration Example: load_object: module: framework.FaultIsolation name: FaultIsolationExperiment setup: datetime_column: \"ds\" datetime_format: \"ms\" target: \"Output (S)\" predict_window: 1000 retrain: retrain_window: 5000 metric: \"Accuracy\" metric_threshold: 0.9 higher_better: true eda: create_model: model: \"dt\" params: REST API Usage: # Train Decision Tree for fault isolation curl -X POST \"http://localhost:8010/electrical_faults/train\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"load_object\": { \"module\": \"framework.FaultIsolation\", \"name\": \"FaultIsolationExperiment\" }, \"setup\": { \"datetime_column\": \"ds\", \"target\": \"Output (S)\", \"predict_window\": 1000 }, \"create_model\": { \"model\": \"dt\", \"params\": {} } }' # Classify new faults curl -X POST \"http://localhost:8010/electrical_faults/predict\" # Get feature importance plot curl \"http://localhost:8010/electrical_faults/plot/feature_importance\" # Get confusion matrix curl \"http://localhost:8010/electrical_faults/plot/confusion_matrix\" Use Case Example (from research paper): Electrical circuit monitoring with 3-phase electricity data (Va, Vb, Vc voltages and Ia, Ib, Ic currents). The decision tree identifies which sensors contribute most to fault detection, with current Ib showing highest feature importance. Random Forest Ensemble method combining multiple decision trees for robust classification. Configuration Example: load_object: module: framework.FaultIsolation name: FaultIsolationExperiment setup: datetime_column: \"ds\" target: \"fault_type\" eda: create_model: model: \"rf\" params: n_estimators: 100 max_depth: 10 random_state: 42 Naive Bayes Probabilistic classifier based on Bayes' theorem, effective for categorical fault classification. Configuration Example: load_object: module: framework.FaultIsolation name: FaultIsolationExperiment setup: datetime_column: \"ds\" target: \"fault_category\" eda: create_model: model: \"nb\" params: Hidden Markov Models (HMM) HMM for sequential fault pattern recognition and state-based fault isolation. Configuration Example: load_object: module: framework.FaultIsolation name: FaultIsolationExperiment setup: datetime_column: \"ds\" target: \"machine_state\" eda: create_model: model: \"hmm\" params: n_iter: 1000 covariance_type: \"diag\" n_mix: 10 Parameters: - n_iter : Maximum number of EM iterations - covariance_type : Type of covariance matrix - n_mix : Number of mixture components Bayesian Network Probabilistic graphical model for understanding causal relationships between variables. Configuration Example: load_object: module: framework.FaultIsolation name: FaultIsolationExperiment setup: datetime_column: \"ds\" target: \"fault_root_cause\" eda: create_model: model: \"bn\" params: learningMethod: 'MIIC' prior: 'Smoothing' priorWeight: 1 discretizationNbBins: 30 discretizationStrategy: \"quantile\" discretizationThreshold: 0.01 usePR: false Note: Bayesian Network prediction requires target variable and may have Docker compatibility issues with pyAgrum. Process Mining Process Mining experiments analyze sequential data to understand workflows, operator behavior, and process optimization opportunities. Heuristics Miner Discovers process models from event logs using heuristic rules. Configuration Example: load_object: module: framework.ProcessMining name: ProcessMiningExperiment setup: eda: create_model: model: \"heuristics\" params: Apriori Association Rules Finds frequent patterns and association rules in sequential data. Configuration Example: load_object: module: framework.ProcessMining name: ProcessMiningExperiment setup: eda: create_model: model: \"apriori\" params: min_support: 0.1 min_confidence: 0.8 TopK Rules Discovers the top-K most interesting association rules. Configuration Example: load_object: module: framework.ProcessMining name: ProcessMiningExperiment setup: eda: create_model: model: \"topk\" params: k: 100 min_confidence: 0.5 CMSPAM Closed sequential pattern mining for discovering frequent subsequences. Configuration Example: load_object: module: framework.ProcessMining name: ProcessMiningExperiment setup: eda: create_model: model: \"cmspam\" params: min_support: 0.1 Advanced Configuration Options Automatic Retraining Configure automatic model retraining based on performance metrics: setup: retrain: retrain_window: 5000 # Use last 5000 samples for retraining metric: \"Accuracy\" # Metric to monitor (Accuracy, F1, Precision, Recall, MAPE, MSE) metric_threshold: 0.9 # Threshold that triggers retraining higher_better: true # Whether higher metric values are better Data Formatting Handle different data formats with custom formatting options: setup: format: name: \"pivot\" # Formatting mode id: \"tsdata\" # Data variable name max_level: 1 columns: \"target\" index: \"date\" values: \"value\" Prediction Windows Control visualization and prediction scope: setup: predict_window: 1000 # Number of samples to show in prediction plots forecast_window: 100 # Number of future points to predict REST API Patterns Common Endpoints All experiments support these standard endpoints: POST /{experiment_name}/train - Train a new model POST /{experiment_name}/predict - Make predictions POST /{experiment_name}/save - Save model to MLflow POST /{experiment_name}/load - Load model from MLflow GET /{experiment_name}/plot/{plot_name} - Get visualization GET /{experiment_name}/cfg - Get configuration GET /{experiment_name}/run_id - Get MLflow run ID Batch Processing Example # Train multiple models in sequence for model in \"iforest\" \"pca\" \"dbscan\"; do curl -X POST \"http://localhost:8010/sensor_${model}/train\" \\ -H \"Content-Type: application/json\" \\ -d @configs/pump/${model}.yaml done # Monitor all models for model in \"iforest\" \"pca\" \"dbscan\"; do curl -X POST \"http://localhost:8010/sensor_${model}/predict\" done Model Selection Guidelines Time Series Analysis Prophet : Strong seasonality, holiday effects, missing data tolerance ARIMA : Stationary data, clear autoregressive patterns LSTM : Complex non-linear patterns, large datasets Autoencoder : Anomaly detection, reconstruction-based analysis SSA : Trend and seasonal decomposition Fault Detection Isolation Forest : High-dimensional data, unknown anomaly types PCA : Multivariate data, statistical anomaly detection DBSCAN : Density-based clusters, irregular cluster shapes Elliptic Envelope : Gaussian-distributed data, robust outlier detection Fault Isolation Decision Tree : Interpretability required, feature importance analysis Random Forest : Robust classification, ensemble benefits Naive Bayes : Categorical features, probabilistic classification HMM : Sequential patterns, state-based analysis Bayesian Network : Causal relationships, probabilistic inference Process Mining Heuristics Miner : Process discovery from event logs Apriori : Frequent pattern mining, association rules TopK Rules : Most interesting patterns, rule ranking CMSPAM : Sequential pattern mining, closed patterns Troubleshooting Common Issues BayesNet Docker Issues : Works locally but may fail in Docker due to pyAgrum compilation requirements PCA Indexing : Avoid duplicate indices in input data Process Mining : Some models repeat training during prediction phase Memory Requirements : Deep learning models (LSTM, Autoencoder) require sufficient RAM Performance Optimization Use appropriate predict_window sizes to balance visualization and performance Configure retrain_window based on data velocity and model stability Monitor MLflow for model performance metrics and storage usage Use batch processing for multiple model training This comprehensive guide provides the foundation for implementing robust monitoring solutions with ObServML across various industrial applications and use cases.","title":"Models"},{"location":"models/#models-and-configuration-guide","text":"This section provides comprehensive examples of machine learning models available in ObServML, organized by experiment type. Each model includes practical configuration examples from real use cases, REST API usage patterns, and parameter explanations. ObServML supports four main experiment types: - Time Series Analysis : Forecasting and anomaly detection in temporal data - Fault Detection : Unsupervised anomaly detection in sensor data - Fault Isolation : Supervised classification for root cause analysis - Process Mining : Analysis of sequential processes and workflows","title":"Models and Configuration Guide"},{"location":"models/#configuration-structure","text":"All models follow a consistent YAML configuration structure: load_object: module: framework.{ExperimentType} name: {ExperimentType}Experiment setup: datetime_column: \"timestamp\" # Column containing timestamps target: \"target_variable\" # Target variable (for supervised learning) # Additional setup parameters... eda: # Optional: Exploratory Data Analysis create_model: model: \"model_name\" params: # Model-specific parameters","title":"Configuration Structure"},{"location":"models/#time-series-analysis","text":"Time Series Analysis experiments are designed for monitoring sensor data over time, offering insights into individual sensor behavior and enabling predictive maintenance through forecasting and anomaly detection.","title":"Time Series Analysis"},{"location":"models/#prophet","text":"Prophet is excellent for time series forecasting with strong seasonal patterns and holiday effects. Configuration Example: load_object: module: framework.TimeSeriesAnalysis name: TimeSeriesAnomalyExperiment setup: datetime_column: \"ds\" datetime_format: \"ms\" target: \"y\" predict_window: 1000 retrain: retrain_window: 5000 metric: \"MAPE\" metric_threshold: 0.3 higher_better: false eda: create_model: model: \"prophet\" params: periods: 0 factor: 1.0 forecast_window: 100 REST API Usage: # Train Prophet model curl -X POST \"http://localhost:8010/timeseries_prophet/train\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"load_object\": { \"module\": \"framework.TimeSeriesAnalysis\", \"name\": \"TimeSeriesAnomalyExperiment\" }, \"setup\": { \"datetime_column\": \"ds\", \"target\": \"y\", \"predict_window\": 1000 }, \"create_model\": { \"model\": \"prophet\", \"params\": { \"periods\": 0, \"factor\": 1.0, \"forecast_window\": 100 } } }' # Make predictions curl -X POST \"http://localhost:8010/timeseries_prophet/predict\" # Get forecast plot curl \"http://localhost:8010/timeseries_prophet/plot/forecast\" Parameters: - periods : Number of periods to forecast forward - factor : Anomaly detection factor - forecast_window : Number of future points to predict","title":"Prophet"},{"location":"models/#arimasarima","text":"ARIMA models are ideal for stationary time series with clear autoregressive patterns. Configuration Example: load_object: module: framework.TimeSeriesAnalysis name: TimeSeriesAnomalyExperiment setup: datetime_column: \"ds\" target: \"y\" eda: create_model: model: \"arima\" params: start_p: 10 d: 1 start_q: 10 max_p: 100 max_q: 100 seasonal: false threshold_for_anomaly: 3 Parameters: - start_p : Starting value for autoregressive order - d : Differencing order - start_q : Starting value for moving average order - max_p/max_q : Maximum values for parameter search - seasonal : Enable seasonal ARIMA (SARIMA) - threshold_for_anomaly : Standard deviations for anomaly detection","title":"ARIMA/SARIMA"},{"location":"models/#lstm","text":"Long Short-Term Memory networks for complex temporal patterns and non-linear relationships. Configuration Example: load_object: module: framework.TimeSeriesAnalysis name: TimeSeriesAnomalyExperiment setup: datetime_column: \"ds\" target: \"y\" eda: create_model: model: \"lstm\" params: seq_length: 50 layer_no: 2 cell_no: 64 epoch_no: 100 batch_size: 32 shuffle: true patience: 10 threshold_for_anomaly: 3 Parameters: - seq_length : Length of input sequences - layer_no : Number of LSTM layers - cell_no : Number of cells per LSTM layer - epoch_no : Training epochs - batch_size : Training batch size - patience : Early stopping patience - threshold_for_anomaly : Anomaly detection threshold","title":"LSTM"},{"location":"models/#autoencoder","text":"Neural network autoencoders for anomaly detection in time series through reconstruction error. Configuration Example: load_object: module: framework.TimeSeriesAnalysis name: TimeSeriesAnomalyExperiment setup: datetime_column: \"ds\" target: \"y\" eda: create_model: model: \"ae\" params: layer_no: 6 window: 250 epoch_no: 50 batch_size: 64 shuffle: false threshold_for_anomaly: 3 neuron_no_enc: [30, 25, 20, 15, 10, 5] neuron_no_dec: [5, 10, 15, 20, 25, 30] act_enc: 'relu' act_dec: 'relu' Parameters: - layer_no : Number of layers in encoder/decoder - window : Moving window size for sequences - neuron_no_enc/dec : Neurons per layer in encoder/decoder - act_enc/dec : Activation functions","title":"Autoencoder"},{"location":"models/#ssa-singular-spectrum-analysis","text":"SSA for time series decomposition and anomaly detection. Configuration Example: load_object: module: framework.TimeSeriesAnalysis name: TimeSeriesAnomalyExperiment setup: datetime_column: \"ds\" target: \"y\" eda: create_model: model: \"ssa\" params: window_size: 10 lower_frequency_bound: 0.05 lower_frequency_contribution: 0.975 threshold: 3","title":"SSA (Singular Spectrum Analysis)"},{"location":"models/#fault-detection","text":"Fault Detection experiments focus on unsupervised anomaly detection in multivariate sensor data, identifying deviations from normal operating conditions without requiring labeled data.","title":"Fault Detection"},{"location":"models/#isolation-forest","text":"Isolation Forest is highly effective for anomaly detection in high-dimensional data by isolating anomalies through random partitioning. Configuration Example: load_object: module: framework.FaultDetection name: FaultDetectionExperiment setup: datetime_column: \"ds\" datetime_format: \"ms\" eda: create_model: model: \"iforest\" params: n_estimators: 100 contamination: \"auto\" random_state: 0 REST API Usage: # Train Isolation Forest model curl -X POST \"http://localhost:8010/pump_anomaly/train\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"load_object\": { \"module\": \"framework.FaultDetection\", \"name\": \"FaultDetectionExperiment\" }, \"setup\": { \"datetime_column\": \"ds\" }, \"create_model\": { \"model\": \"iforest\", \"params\": { \"n_estimators\": 100, \"contamination\": \"auto\" } } }' # Detect anomalies in new data curl -X POST \"http://localhost:8010/pump_anomaly/predict\" # Get anomaly visualization curl \"http://localhost:8010/pump_anomaly/plot/outliers\" Parameters: - n_estimators : Number of isolation trees - contamination : Expected proportion of anomalies (\"auto\" for automatic detection) - random_state : Random seed for reproducibility","title":"Isolation Forest"},{"location":"models/#pca-principal-component-analysis","text":"PCA with Hotelling's T\u00b2 and SPE tests for multivariate anomaly detection, as demonstrated in the research paper with pump sensor data. Configuration Example: load_object: module: framework.FaultDetection name: FaultDetectionExperiment setup: datetime_column: \"ds\" datetime_format: \"ms\" eda: create_model: model: \"pca\" params: alpha: 0.05 detect_outliers: ['ht2', 'spe'] n_components: 0.95 normalize: true Use Case Example (from research paper): The pump dataset contains 50 sensors without labeled data, making it ideal for fault detection. PCA reduces dimensionality while preserving 95% of variance, then applies statistical tests to identify anomalies. Parameters: - alpha : Significance level for Hotelling's T\u00b2 test - detect_outliers : Types of outlier detection ['ht2', 'spe'] - n_components : Variance explained by principal components - normalize : Apply data normalization","title":"PCA (Principal Component Analysis)"},{"location":"models/#dbscan","text":"Density-based clustering for anomaly detection, effective for identifying clusters of normal behavior. Configuration Example: load_object: module: framework.FaultDetection name: FaultDetectionExperiment setup: datetime_column: \"ds\" datetime_format: \"ms\" eda: create_model: model: \"dbscan\" params: eps: 2 min_samples: 5 Parameters: - eps : Maximum distance between points in a neighborhood - min_samples : Minimum points required to form a cluster","title":"DBSCAN"},{"location":"models/#elliptic-envelope","text":"Robust covariance estimation for outlier detection in multivariate data. Configuration Example: load_object: module: framework.FaultDetection name: FaultDetectionExperiment setup: datetime_column: \"ds\" datetime_format: \"ms\" eda: create_model: model: \"ee\" params: contamination: 0.1 Parameters: - contamination : Proportion of outliers in the dataset (0.0 to 0.5)","title":"Elliptic Envelope"},{"location":"models/#fault-isolation","text":"Fault Isolation experiments perform supervised classification when labeled data is available, enabling root cause analysis and identification of specific fault types or machine states.","title":"Fault Isolation"},{"location":"models/#decision-tree","text":"Decision trees provide interpretable classification with feature importance analysis, as demonstrated in the research paper with electrical circuit data. Configuration Example: load_object: module: framework.FaultIsolation name: FaultIsolationExperiment setup: datetime_column: \"ds\" datetime_format: \"ms\" target: \"Output (S)\" predict_window: 1000 retrain: retrain_window: 5000 metric: \"Accuracy\" metric_threshold: 0.9 higher_better: true eda: create_model: model: \"dt\" params: REST API Usage: # Train Decision Tree for fault isolation curl -X POST \"http://localhost:8010/electrical_faults/train\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"load_object\": { \"module\": \"framework.FaultIsolation\", \"name\": \"FaultIsolationExperiment\" }, \"setup\": { \"datetime_column\": \"ds\", \"target\": \"Output (S)\", \"predict_window\": 1000 }, \"create_model\": { \"model\": \"dt\", \"params\": {} } }' # Classify new faults curl -X POST \"http://localhost:8010/electrical_faults/predict\" # Get feature importance plot curl \"http://localhost:8010/electrical_faults/plot/feature_importance\" # Get confusion matrix curl \"http://localhost:8010/electrical_faults/plot/confusion_matrix\" Use Case Example (from research paper): Electrical circuit monitoring with 3-phase electricity data (Va, Vb, Vc voltages and Ia, Ib, Ic currents). The decision tree identifies which sensors contribute most to fault detection, with current Ib showing highest feature importance.","title":"Decision Tree"},{"location":"models/#random-forest","text":"Ensemble method combining multiple decision trees for robust classification. Configuration Example: load_object: module: framework.FaultIsolation name: FaultIsolationExperiment setup: datetime_column: \"ds\" target: \"fault_type\" eda: create_model: model: \"rf\" params: n_estimators: 100 max_depth: 10 random_state: 42","title":"Random Forest"},{"location":"models/#naive-bayes","text":"Probabilistic classifier based on Bayes' theorem, effective for categorical fault classification. Configuration Example: load_object: module: framework.FaultIsolation name: FaultIsolationExperiment setup: datetime_column: \"ds\" target: \"fault_category\" eda: create_model: model: \"nb\" params:","title":"Naive Bayes"},{"location":"models/#hidden-markov-models-hmm","text":"HMM for sequential fault pattern recognition and state-based fault isolation. Configuration Example: load_object: module: framework.FaultIsolation name: FaultIsolationExperiment setup: datetime_column: \"ds\" target: \"machine_state\" eda: create_model: model: \"hmm\" params: n_iter: 1000 covariance_type: \"diag\" n_mix: 10 Parameters: - n_iter : Maximum number of EM iterations - covariance_type : Type of covariance matrix - n_mix : Number of mixture components","title":"Hidden Markov Models (HMM)"},{"location":"models/#bayesian-network","text":"Probabilistic graphical model for understanding causal relationships between variables. Configuration Example: load_object: module: framework.FaultIsolation name: FaultIsolationExperiment setup: datetime_column: \"ds\" target: \"fault_root_cause\" eda: create_model: model: \"bn\" params: learningMethod: 'MIIC' prior: 'Smoothing' priorWeight: 1 discretizationNbBins: 30 discretizationStrategy: \"quantile\" discretizationThreshold: 0.01 usePR: false Note: Bayesian Network prediction requires target variable and may have Docker compatibility issues with pyAgrum.","title":"Bayesian Network"},{"location":"models/#process-mining","text":"Process Mining experiments analyze sequential data to understand workflows, operator behavior, and process optimization opportunities.","title":"Process Mining"},{"location":"models/#heuristics-miner","text":"Discovers process models from event logs using heuristic rules. Configuration Example: load_object: module: framework.ProcessMining name: ProcessMiningExperiment setup: eda: create_model: model: \"heuristics\" params:","title":"Heuristics Miner"},{"location":"models/#apriori-association-rules","text":"Finds frequent patterns and association rules in sequential data. Configuration Example: load_object: module: framework.ProcessMining name: ProcessMiningExperiment setup: eda: create_model: model: \"apriori\" params: min_support: 0.1 min_confidence: 0.8","title":"Apriori Association Rules"},{"location":"models/#topk-rules","text":"Discovers the top-K most interesting association rules. Configuration Example: load_object: module: framework.ProcessMining name: ProcessMiningExperiment setup: eda: create_model: model: \"topk\" params: k: 100 min_confidence: 0.5","title":"TopK Rules"},{"location":"models/#cmspam","text":"Closed sequential pattern mining for discovering frequent subsequences. Configuration Example: load_object: module: framework.ProcessMining name: ProcessMiningExperiment setup: eda: create_model: model: \"cmspam\" params: min_support: 0.1","title":"CMSPAM"},{"location":"models/#advanced-configuration-options","text":"","title":"Advanced Configuration Options"},{"location":"models/#automatic-retraining","text":"Configure automatic model retraining based on performance metrics: setup: retrain: retrain_window: 5000 # Use last 5000 samples for retraining metric: \"Accuracy\" # Metric to monitor (Accuracy, F1, Precision, Recall, MAPE, MSE) metric_threshold: 0.9 # Threshold that triggers retraining higher_better: true # Whether higher metric values are better","title":"Automatic Retraining"},{"location":"models/#data-formatting","text":"Handle different data formats with custom formatting options: setup: format: name: \"pivot\" # Formatting mode id: \"tsdata\" # Data variable name max_level: 1 columns: \"target\" index: \"date\" values: \"value\"","title":"Data Formatting"},{"location":"models/#prediction-windows","text":"Control visualization and prediction scope: setup: predict_window: 1000 # Number of samples to show in prediction plots forecast_window: 100 # Number of future points to predict","title":"Prediction Windows"},{"location":"models/#rest-api-patterns","text":"","title":"REST API Patterns"},{"location":"models/#common-endpoints","text":"All experiments support these standard endpoints: POST /{experiment_name}/train - Train a new model POST /{experiment_name}/predict - Make predictions POST /{experiment_name}/save - Save model to MLflow POST /{experiment_name}/load - Load model from MLflow GET /{experiment_name}/plot/{plot_name} - Get visualization GET /{experiment_name}/cfg - Get configuration GET /{experiment_name}/run_id - Get MLflow run ID","title":"Common Endpoints"},{"location":"models/#batch-processing-example","text":"# Train multiple models in sequence for model in \"iforest\" \"pca\" \"dbscan\"; do curl -X POST \"http://localhost:8010/sensor_${model}/train\" \\ -H \"Content-Type: application/json\" \\ -d @configs/pump/${model}.yaml done # Monitor all models for model in \"iforest\" \"pca\" \"dbscan\"; do curl -X POST \"http://localhost:8010/sensor_${model}/predict\" done","title":"Batch Processing Example"},{"location":"models/#model-selection-guidelines","text":"","title":"Model Selection Guidelines"},{"location":"models/#time-series-analysis_1","text":"Prophet : Strong seasonality, holiday effects, missing data tolerance ARIMA : Stationary data, clear autoregressive patterns LSTM : Complex non-linear patterns, large datasets Autoencoder : Anomaly detection, reconstruction-based analysis SSA : Trend and seasonal decomposition","title":"Time Series Analysis"},{"location":"models/#fault-detection_1","text":"Isolation Forest : High-dimensional data, unknown anomaly types PCA : Multivariate data, statistical anomaly detection DBSCAN : Density-based clusters, irregular cluster shapes Elliptic Envelope : Gaussian-distributed data, robust outlier detection","title":"Fault Detection"},{"location":"models/#fault-isolation_1","text":"Decision Tree : Interpretability required, feature importance analysis Random Forest : Robust classification, ensemble benefits Naive Bayes : Categorical features, probabilistic classification HMM : Sequential patterns, state-based analysis Bayesian Network : Causal relationships, probabilistic inference","title":"Fault Isolation"},{"location":"models/#process-mining_1","text":"Heuristics Miner : Process discovery from event logs Apriori : Frequent pattern mining, association rules TopK Rules : Most interesting patterns, rule ranking CMSPAM : Sequential pattern mining, closed patterns","title":"Process Mining"},{"location":"models/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"models/#common-issues","text":"BayesNet Docker Issues : Works locally but may fail in Docker due to pyAgrum compilation requirements PCA Indexing : Avoid duplicate indices in input data Process Mining : Some models repeat training during prediction phase Memory Requirements : Deep learning models (LSTM, Autoencoder) require sufficient RAM","title":"Common Issues"},{"location":"models/#performance-optimization","text":"Use appropriate predict_window sizes to balance visualization and performance Configure retrain_window based on data velocity and model stability Monitor MLflow for model performance metrics and storage usage Use batch processing for multiple model training This comprehensive guide provides the foundation for implementing robust monitoring solutions with ObServML across various industrial applications and use cases.","title":"Performance Optimization"},{"location":"plugin_system/","text":"ExperimentHub Plugin System The ExperimentHub plugin system provides a flexible and extensible architecture for integrating various services and functionalities into the ExperimentHub. This document provides a comprehensive overview of the plugin system, including its architecture, configuration, and usage. System Architecture graph TD A[ExperimentHub] --> B[Plugin System] B --> C[MLOps Plugin] B --> D[DataStream Plugin] A --> F[Experiment Management] F --> G[Training] F --> H[Prediction] F --> I[Retraining] A --> J[Configuration System] J --> K[Plugin Configuration] J --> L[Experiment Configuration] C --> M[MLflowPlugin] D --> N[RabbitMQPlugin] The ExperimentHub plugin system consists of three main components: Plugin Interfaces : Define the contract that plugin implementations must adhere to Plugin Implementations : Concrete implementations of the plugin interfaces Plugin Registry : Manages plugin registration and retrieval Plugin Types The ExperimentHub supports two main types of plugins: 1. MLOps Plugin The MLOps plugin handles model tracking, versioning, and artifact storage. It provides methods for: Creating experiments Starting runs Logging artifacts Retrieving runs and experiments Default implementation: MLflowPlugin 2. DataStream Plugin The DataStream plugin handles data streaming and messaging. It provides methods for: Creating queues Sending and receiving data Flushing queues Default implementation: RabbitMQPlugin 3. TaskQueue Plugin (REMOVED) Note : The TaskQueue plugin functionality has been removed from ObservML. Training and prediction operations are now handled synchronously within the ExperimentHub. This simplifies the architecture and reduces dependencies while maintaining performance for most use cases. Plugin Interfaces All plugins must implement the Plugin Protocol defined in framework/plugins/base.py : class Plugin(Protocol): \"\"\"Base interface for all plugins\"\"\" plugin_type: str def __init__(self, **kwargs) -> None: \"\"\"Initialize the plugin with configuration\"\"\" ... def initialize(self) -> None: \"\"\"Initialize the plugin\"\"\" ... def shutdown(self) -> None: \"\"\"Clean up resources when shutting down\"\"\" ... def health_check(self) -> Tuple[bool, Dict[str, Any]]: \"\"\"Check if the plugin is working correctly Returns: Tuple containing: - Boolean indicating if the plugin is healthy - Dictionary with additional health information and metrics \"\"\" ... Each plugin type then extends this base interface with additional methods specific to its functionality. MLOps Plugin Interface class MLOpsPlugin(Plugin, Protocol): \"\"\"Interface for MLOps plugins\"\"\" plugin_type = \"mlops\" def set_tracking_uri(self, uri: str) -> None: \"\"\"Set tracking URI\"\"\" ... def create_experiment(self, name: str) -> str: \"\"\"Create a new experiment\"\"\" ... def start_run(self, experiment_id: str) -> str: \"\"\"Start a new run\"\"\" ... def log_artifact(self, path: str, artifact_path: str) -> None: \"\"\"Log an artifact\"\"\" ... def get_run(self, run_id: str) -> Any: \"\"\"Get a run by ID\"\"\" ... def get_experiment_by_name(self, name: str) -> Any: \"\"\"Get an experiment by name\"\"\" ... DataStream Plugin Interface class DataStreamPlugin(Plugin, Protocol): \"\"\"Interface for data stream plugins\"\"\" plugin_type = \"datastream\" def connect(self, **kwargs) -> None: \"\"\"Connect to the data stream service\"\"\" ... def disconnect(self) -> None: \"\"\"Disconnect from the data stream service\"\"\" ... def create_queue(self, queue_name: str) -> None: \"\"\"Create a queue\"\"\" ... def pull_data(self, queue: str) -> Any: \"\"\"Pull data from a queue\"\"\" ... def flush_queue(self, queue: str) -> None: \"\"\"Flush a queue\"\"\" ... TaskQueue Plugin Interface (REMOVED) Note : The TaskQueue plugin interface has been removed from ObservML. This interface is no longer supported as task queue functionality has been replaced with synchronous processing. Plugin Implementations MLflowPlugin The MLflowPlugin is an implementation of the MLOpsPlugin interface that uses MLflow for model tracking and versioning. class MLflowPlugin(MLOpsPlugin): \"\"\"MLflow implementation of MLOpsPlugin\"\"\" plugin_type = \"mlops\" def __init__(self, mlflow_uri: str, **kwargs): self.mlflow_uri = mlflow_uri self.client = None def initialize(self) -> None: \"\"\"Initialize the plugin\"\"\" self.set_tracking_uri(self.mlflow_uri) self.client = MlflowClient(self.mlflow_uri) def health_check(self) -> Tuple[bool, Dict[str, Any]]: \"\"\"Check if MLflow plugin is working correctly\"\"\" try: # Try to connect to MLflow server self.client.list_experiments() return True, {\"status\": \"connected\", \"uri\": self.mlflow_uri} except Exception as e: return False, {\"status\": \"error\", \"message\": str(e), \"uri\": self.mlflow_uri} # ... other methods ... RabbitMQPlugin The RabbitMQPlugin is an implementation of the DataStreamPlugin interface that uses RabbitMQ for data streaming and messaging. class RabbitMQPlugin(DataStreamPlugin): \"\"\"RabbitMQ implementation of DataStreamPlugin\"\"\" plugin_type = \"datastream\" def __init__(self, host: str, port: str|int, username: str, password: str, **kwargs): self.host = host self.port = port self.username = username self.password = password self.connection = None self.channel = None def initialize(self) -> None: \"\"\"Initialize the plugin\"\"\" self.connect() def health_check(self) -> Tuple[bool, Dict[str, Any]]: \"\"\"Check if RabbitMQ plugin is working correctly\"\"\" try: # Check if connection is open if self.connection is None or self.connection.is_closed: self.connect() # Try to create a test queue test_queue = f\"health_check_{int(time.time())}\" self.channel.queue_declare(queue=test_queue, durable=False) self.channel.queue_delete(queue=test_queue) return True, { \"status\": \"connected\", \"host\": self.host, \"port\": self.port } except Exception as e: return False, { \"status\": \"error\", \"message\": str(e), \"host\": self.host, \"port\": self.port } # ... other methods ... CeleryPlugin (REMOVED) Note : The CeleryPlugin implementation has been removed from ObservML as task queue functionality is no longer supported. The system now processes training and prediction operations synchronously for improved simplicity and reliability. Plugin Registration and Retrieval Plugins are registered with the ExperimentHub using the register_plugin method: def register_plugin(self, plugin: Plugin) -> None: \"\"\"Register a plugin. Args: plugin: The plugin to register. \"\"\" self.plugins[plugin.plugin_type] = plugin plugin.initialize() Plugins can be retrieved using the get_plugin method: def get_plugin(self, plugin_type: str) -> Optional[Plugin]: \"\"\"Get a plugin by type. Args: plugin_type: The type of plugin to get. Returns: The plugin, or None if not found. \"\"\" return self.plugins.get(plugin_type) Plugin Health Checks All plugins must implement a health_check method that verifies if the plugin is working correctly. This method returns a tuple containing: A boolean indicating if the plugin is healthy A dictionary with additional health information and metrics The ExperimentHub provides a check_plugin_health method that checks the health of all registered plugins: def check_plugin_health(self) -> Dict[str, Dict[str, Any]]: \"\"\"Check the health of all registered plugins Returns: Dictionary with plugin health information \"\"\" health_info = {} for plugin_type, plugin in self.plugins.items(): is_healthy, details = plugin.health_check() health_info[plugin_type] = { \"healthy\": is_healthy, \"details\": details } return health_info Plugin Configuration Plugins are configured using the hub_config.yaml file: # Plugin configurations plugins: # MLOps plugin configuration mlops: enabled: true type: \"mlflow\" # Which implementation to use config: mlflow_uri: \"http://localhost:5000\" # Data stream plugin configuration datastream: enabled: true type: \"rabbitmq\" config: host: \"localhost\" port: 5672 username: \"guest\" password: \"guest\" # Task queue plugin configuration - REMOVED # The task queue functionality has been removed from ObservML # Training and prediction operations are now handled synchronously The ExperimentHub initializes plugins based on this configuration: def _init_plugins_from_config(self, plugin_config: dict) -> None: \"\"\"Initialize plugins from configuration Args: plugin_config: Plugin configuration dictionary \"\"\" # Initialize MLOps plugin if plugin_config.get(\"mlops\", {}).get(\"enabled\", False): mlops_config = plugin_config[\"mlops\"] if mlops_config[\"type\"] == \"mlflow\": from framework.plugins.mlflow_plugin import MLflowPlugin mlops_plugin = MLflowPlugin(**mlops_config[\"config\"]) self.register_plugin(mlops_plugin) # Initialize DataStream plugin if plugin_config.get(\"datastream\", {}).get(\"enabled\", False): datastream_config = plugin_config[\"datastream\"] if datastream_config[\"type\"] == \"rabbitmq\": from framework.plugins.rabbitmq_plugin import RabbitMQPlugin datastream_plugin = RabbitMQPlugin(**datastream_config[\"config\"]) self.register_plugin(datastream_plugin) # TaskQueue plugin removed - no longer supported pass Creating Custom Plugins You can create custom plugins by implementing the appropriate plugin interface. Here's an example of creating a custom MLOps plugin: from framework.plugins.mlops import MLOpsPlugin from typing import Any, Dict, Tuple class CustomMLOpsPlugin(MLOpsPlugin): \"\"\"Custom implementation of MLOpsPlugin\"\"\" plugin_type = \"mlops\" def __init__(self, custom_param: str, **kwargs): self.custom_param = custom_param def initialize(self) -> None: \"\"\"Initialize the plugin\"\"\" # Custom initialization logic pass def shutdown(self) -> None: \"\"\"Clean up resources\"\"\" # Custom cleanup logic pass def health_check(self) -> Tuple[bool, Dict[str, Any]]: \"\"\"Check if plugin is working correctly\"\"\" try: # Custom health check logic return True, {\"status\": \"connected\", \"custom_param\": self.custom_param} except Exception as e: return False, {\"status\": \"error\", \"message\": str(e)} # Implement other MLOpsPlugin methods # ... To use your custom plugin, you can register it with the ExperimentHub: from framework.ExperimentHub import ExperimentHub # Create ExperimentHub hub = ExperimentHub() # Create and register custom plugin custom_plugin = CustomMLOpsPlugin(custom_param=\"value\") hub.register_plugin(custom_plugin) Plugin Lifecycle Plugins go through the following lifecycle: Initialization : The plugin is created with configuration parameters Registration : The plugin is registered with the ExperimentHub Initialization : The initialize method is called to set up the plugin Usage : The plugin is used by the ExperimentHub Shutdown : The shutdown method is called to clean up resources Best Practices When working with plugins, follow these best practices: Error Handling Plugins should handle errors gracefully and provide meaningful error messages: def health_check(self) -> Tuple[bool, Dict[str, Any]]: \"\"\"Check if plugin is working correctly\"\"\" try: # Health check logic return True, {\"status\": \"connected\"} except ConnectionError as e: return False, {\"status\": \"error\", \"message\": f\"Connection error: {str(e)}\"} except TimeoutError as e: return False, {\"status\": \"error\", \"message\": f\"Timeout error: {str(e)}\"} except Exception as e: return False, {\"status\": \"error\", \"message\": f\"Unexpected error: {str(e)}\"} Resource Management Plugins should clean up resources in the shutdown method: def shutdown(self) -> None: \"\"\"Clean up resources\"\"\" if self.connection is not None: try: self.connection.close() except Exception as e: logging.error(f\"Error closing connection: {e}\") finally: self.connection = None Configuration Plugins should accept configuration parameters in the constructor: def __init__(self, host: str, port: int, username: str, password: str, **kwargs): self.host = host self.port = port self.username = username self.password = password self.timeout = kwargs.get(\"timeout\", 30) self.connection = None Health Checks Plugins should implement thorough health checks: def health_check(self) -> Tuple[bool, Dict[str, Any]]: \"\"\"Check if plugin is working correctly\"\"\" try: # Check connection if self.connection is None or not self.connection.is_connected(): self.connect() # Check functionality self.ping() # Return health information return True, { \"status\": \"connected\", \"host\": self.host, \"port\": self.port, \"connection_id\": self.connection.id } except Exception as e: return False, { \"status\": \"error\", \"message\": str(e), \"host\": self.host, \"port\": self.port } Documentation Document the purpose, configuration, and usage of your plugins: class CustomPlugin(Plugin): \"\"\"Custom plugin for integrating with Example Service. This plugin provides integration with Example Service, allowing the ExperimentHub to use Example Service for specific functionality. Configuration: api_key: API key for authenticating with Example Service host: Hostname of the Example Service API port: Port of the Example Service API timeout: Timeout for API requests (default: 30 seconds) Usage: plugin = CustomPlugin( api_key=\"your-api-key\", host=\"api.example.com\", port=443, timeout=60 ) hub.register_plugin(plugin) \"\"\" # ... Troubleshooting Plugin Initialization Errors If a plugin fails to initialize, check the plugin configuration and make sure all required parameters are provided. Plugin Health Check Failures If a plugin health check fails, check the health information for details on what went wrong. Missing Plugins If a plugin is missing, check the configuration file and make sure the plugin is enabled and configured correctly.","title":"Plugin System"},{"location":"plugin_system/#experimenthub-plugin-system","text":"The ExperimentHub plugin system provides a flexible and extensible architecture for integrating various services and functionalities into the ExperimentHub. This document provides a comprehensive overview of the plugin system, including its architecture, configuration, and usage.","title":"ExperimentHub Plugin System"},{"location":"plugin_system/#system-architecture","text":"graph TD A[ExperimentHub] --> B[Plugin System] B --> C[MLOps Plugin] B --> D[DataStream Plugin] A --> F[Experiment Management] F --> G[Training] F --> H[Prediction] F --> I[Retraining] A --> J[Configuration System] J --> K[Plugin Configuration] J --> L[Experiment Configuration] C --> M[MLflowPlugin] D --> N[RabbitMQPlugin] The ExperimentHub plugin system consists of three main components: Plugin Interfaces : Define the contract that plugin implementations must adhere to Plugin Implementations : Concrete implementations of the plugin interfaces Plugin Registry : Manages plugin registration and retrieval","title":"System Architecture"},{"location":"plugin_system/#plugin-types","text":"The ExperimentHub supports two main types of plugins:","title":"Plugin Types"},{"location":"plugin_system/#1-mlops-plugin","text":"The MLOps plugin handles model tracking, versioning, and artifact storage. It provides methods for: Creating experiments Starting runs Logging artifacts Retrieving runs and experiments Default implementation: MLflowPlugin","title":"1. MLOps Plugin"},{"location":"plugin_system/#2-datastream-plugin","text":"The DataStream plugin handles data streaming and messaging. It provides methods for: Creating queues Sending and receiving data Flushing queues Default implementation: RabbitMQPlugin","title":"2. DataStream Plugin"},{"location":"plugin_system/#3-taskqueue-plugin-removed","text":"Note : The TaskQueue plugin functionality has been removed from ObservML. Training and prediction operations are now handled synchronously within the ExperimentHub. This simplifies the architecture and reduces dependencies while maintaining performance for most use cases.","title":"3. TaskQueue Plugin (REMOVED)"},{"location":"plugin_system/#plugin-interfaces","text":"All plugins must implement the Plugin Protocol defined in framework/plugins/base.py : class Plugin(Protocol): \"\"\"Base interface for all plugins\"\"\" plugin_type: str def __init__(self, **kwargs) -> None: \"\"\"Initialize the plugin with configuration\"\"\" ... def initialize(self) -> None: \"\"\"Initialize the plugin\"\"\" ... def shutdown(self) -> None: \"\"\"Clean up resources when shutting down\"\"\" ... def health_check(self) -> Tuple[bool, Dict[str, Any]]: \"\"\"Check if the plugin is working correctly Returns: Tuple containing: - Boolean indicating if the plugin is healthy - Dictionary with additional health information and metrics \"\"\" ... Each plugin type then extends this base interface with additional methods specific to its functionality.","title":"Plugin Interfaces"},{"location":"plugin_system/#mlops-plugin-interface","text":"class MLOpsPlugin(Plugin, Protocol): \"\"\"Interface for MLOps plugins\"\"\" plugin_type = \"mlops\" def set_tracking_uri(self, uri: str) -> None: \"\"\"Set tracking URI\"\"\" ... def create_experiment(self, name: str) -> str: \"\"\"Create a new experiment\"\"\" ... def start_run(self, experiment_id: str) -> str: \"\"\"Start a new run\"\"\" ... def log_artifact(self, path: str, artifact_path: str) -> None: \"\"\"Log an artifact\"\"\" ... def get_run(self, run_id: str) -> Any: \"\"\"Get a run by ID\"\"\" ... def get_experiment_by_name(self, name: str) -> Any: \"\"\"Get an experiment by name\"\"\" ...","title":"MLOps Plugin Interface"},{"location":"plugin_system/#datastream-plugin-interface","text":"class DataStreamPlugin(Plugin, Protocol): \"\"\"Interface for data stream plugins\"\"\" plugin_type = \"datastream\" def connect(self, **kwargs) -> None: \"\"\"Connect to the data stream service\"\"\" ... def disconnect(self) -> None: \"\"\"Disconnect from the data stream service\"\"\" ... def create_queue(self, queue_name: str) -> None: \"\"\"Create a queue\"\"\" ... def pull_data(self, queue: str) -> Any: \"\"\"Pull data from a queue\"\"\" ... def flush_queue(self, queue: str) -> None: \"\"\"Flush a queue\"\"\" ...","title":"DataStream Plugin Interface"},{"location":"plugin_system/#taskqueue-plugin-interface-removed","text":"Note : The TaskQueue plugin interface has been removed from ObservML. This interface is no longer supported as task queue functionality has been replaced with synchronous processing.","title":"TaskQueue Plugin Interface (REMOVED)"},{"location":"plugin_system/#plugin-implementations","text":"","title":"Plugin Implementations"},{"location":"plugin_system/#mlflowplugin","text":"The MLflowPlugin is an implementation of the MLOpsPlugin interface that uses MLflow for model tracking and versioning. class MLflowPlugin(MLOpsPlugin): \"\"\"MLflow implementation of MLOpsPlugin\"\"\" plugin_type = \"mlops\" def __init__(self, mlflow_uri: str, **kwargs): self.mlflow_uri = mlflow_uri self.client = None def initialize(self) -> None: \"\"\"Initialize the plugin\"\"\" self.set_tracking_uri(self.mlflow_uri) self.client = MlflowClient(self.mlflow_uri) def health_check(self) -> Tuple[bool, Dict[str, Any]]: \"\"\"Check if MLflow plugin is working correctly\"\"\" try: # Try to connect to MLflow server self.client.list_experiments() return True, {\"status\": \"connected\", \"uri\": self.mlflow_uri} except Exception as e: return False, {\"status\": \"error\", \"message\": str(e), \"uri\": self.mlflow_uri} # ... other methods ...","title":"MLflowPlugin"},{"location":"plugin_system/#rabbitmqplugin","text":"The RabbitMQPlugin is an implementation of the DataStreamPlugin interface that uses RabbitMQ for data streaming and messaging. class RabbitMQPlugin(DataStreamPlugin): \"\"\"RabbitMQ implementation of DataStreamPlugin\"\"\" plugin_type = \"datastream\" def __init__(self, host: str, port: str|int, username: str, password: str, **kwargs): self.host = host self.port = port self.username = username self.password = password self.connection = None self.channel = None def initialize(self) -> None: \"\"\"Initialize the plugin\"\"\" self.connect() def health_check(self) -> Tuple[bool, Dict[str, Any]]: \"\"\"Check if RabbitMQ plugin is working correctly\"\"\" try: # Check if connection is open if self.connection is None or self.connection.is_closed: self.connect() # Try to create a test queue test_queue = f\"health_check_{int(time.time())}\" self.channel.queue_declare(queue=test_queue, durable=False) self.channel.queue_delete(queue=test_queue) return True, { \"status\": \"connected\", \"host\": self.host, \"port\": self.port } except Exception as e: return False, { \"status\": \"error\", \"message\": str(e), \"host\": self.host, \"port\": self.port } # ... other methods ...","title":"RabbitMQPlugin"},{"location":"plugin_system/#celeryplugin-removed","text":"Note : The CeleryPlugin implementation has been removed from ObservML as task queue functionality is no longer supported. The system now processes training and prediction operations synchronously for improved simplicity and reliability.","title":"CeleryPlugin (REMOVED)"},{"location":"plugin_system/#plugin-registration-and-retrieval","text":"Plugins are registered with the ExperimentHub using the register_plugin method: def register_plugin(self, plugin: Plugin) -> None: \"\"\"Register a plugin. Args: plugin: The plugin to register. \"\"\" self.plugins[plugin.plugin_type] = plugin plugin.initialize() Plugins can be retrieved using the get_plugin method: def get_plugin(self, plugin_type: str) -> Optional[Plugin]: \"\"\"Get a plugin by type. Args: plugin_type: The type of plugin to get. Returns: The plugin, or None if not found. \"\"\" return self.plugins.get(plugin_type)","title":"Plugin Registration and Retrieval"},{"location":"plugin_system/#plugin-health-checks","text":"All plugins must implement a health_check method that verifies if the plugin is working correctly. This method returns a tuple containing: A boolean indicating if the plugin is healthy A dictionary with additional health information and metrics The ExperimentHub provides a check_plugin_health method that checks the health of all registered plugins: def check_plugin_health(self) -> Dict[str, Dict[str, Any]]: \"\"\"Check the health of all registered plugins Returns: Dictionary with plugin health information \"\"\" health_info = {} for plugin_type, plugin in self.plugins.items(): is_healthy, details = plugin.health_check() health_info[plugin_type] = { \"healthy\": is_healthy, \"details\": details } return health_info","title":"Plugin Health Checks"},{"location":"plugin_system/#plugin-configuration","text":"Plugins are configured using the hub_config.yaml file: # Plugin configurations plugins: # MLOps plugin configuration mlops: enabled: true type: \"mlflow\" # Which implementation to use config: mlflow_uri: \"http://localhost:5000\" # Data stream plugin configuration datastream: enabled: true type: \"rabbitmq\" config: host: \"localhost\" port: 5672 username: \"guest\" password: \"guest\" # Task queue plugin configuration - REMOVED # The task queue functionality has been removed from ObservML # Training and prediction operations are now handled synchronously The ExperimentHub initializes plugins based on this configuration: def _init_plugins_from_config(self, plugin_config: dict) -> None: \"\"\"Initialize plugins from configuration Args: plugin_config: Plugin configuration dictionary \"\"\" # Initialize MLOps plugin if plugin_config.get(\"mlops\", {}).get(\"enabled\", False): mlops_config = plugin_config[\"mlops\"] if mlops_config[\"type\"] == \"mlflow\": from framework.plugins.mlflow_plugin import MLflowPlugin mlops_plugin = MLflowPlugin(**mlops_config[\"config\"]) self.register_plugin(mlops_plugin) # Initialize DataStream plugin if plugin_config.get(\"datastream\", {}).get(\"enabled\", False): datastream_config = plugin_config[\"datastream\"] if datastream_config[\"type\"] == \"rabbitmq\": from framework.plugins.rabbitmq_plugin import RabbitMQPlugin datastream_plugin = RabbitMQPlugin(**datastream_config[\"config\"]) self.register_plugin(datastream_plugin) # TaskQueue plugin removed - no longer supported pass","title":"Plugin Configuration"},{"location":"plugin_system/#creating-custom-plugins","text":"You can create custom plugins by implementing the appropriate plugin interface. Here's an example of creating a custom MLOps plugin: from framework.plugins.mlops import MLOpsPlugin from typing import Any, Dict, Tuple class CustomMLOpsPlugin(MLOpsPlugin): \"\"\"Custom implementation of MLOpsPlugin\"\"\" plugin_type = \"mlops\" def __init__(self, custom_param: str, **kwargs): self.custom_param = custom_param def initialize(self) -> None: \"\"\"Initialize the plugin\"\"\" # Custom initialization logic pass def shutdown(self) -> None: \"\"\"Clean up resources\"\"\" # Custom cleanup logic pass def health_check(self) -> Tuple[bool, Dict[str, Any]]: \"\"\"Check if plugin is working correctly\"\"\" try: # Custom health check logic return True, {\"status\": \"connected\", \"custom_param\": self.custom_param} except Exception as e: return False, {\"status\": \"error\", \"message\": str(e)} # Implement other MLOpsPlugin methods # ... To use your custom plugin, you can register it with the ExperimentHub: from framework.ExperimentHub import ExperimentHub # Create ExperimentHub hub = ExperimentHub() # Create and register custom plugin custom_plugin = CustomMLOpsPlugin(custom_param=\"value\") hub.register_plugin(custom_plugin)","title":"Creating Custom Plugins"},{"location":"plugin_system/#plugin-lifecycle","text":"Plugins go through the following lifecycle: Initialization : The plugin is created with configuration parameters Registration : The plugin is registered with the ExperimentHub Initialization : The initialize method is called to set up the plugin Usage : The plugin is used by the ExperimentHub Shutdown : The shutdown method is called to clean up resources","title":"Plugin Lifecycle"},{"location":"plugin_system/#best-practices","text":"When working with plugins, follow these best practices:","title":"Best Practices"},{"location":"plugin_system/#error-handling","text":"Plugins should handle errors gracefully and provide meaningful error messages: def health_check(self) -> Tuple[bool, Dict[str, Any]]: \"\"\"Check if plugin is working correctly\"\"\" try: # Health check logic return True, {\"status\": \"connected\"} except ConnectionError as e: return False, {\"status\": \"error\", \"message\": f\"Connection error: {str(e)}\"} except TimeoutError as e: return False, {\"status\": \"error\", \"message\": f\"Timeout error: {str(e)}\"} except Exception as e: return False, {\"status\": \"error\", \"message\": f\"Unexpected error: {str(e)}\"}","title":"Error Handling"},{"location":"plugin_system/#resource-management","text":"Plugins should clean up resources in the shutdown method: def shutdown(self) -> None: \"\"\"Clean up resources\"\"\" if self.connection is not None: try: self.connection.close() except Exception as e: logging.error(f\"Error closing connection: {e}\") finally: self.connection = None","title":"Resource Management"},{"location":"plugin_system/#configuration","text":"Plugins should accept configuration parameters in the constructor: def __init__(self, host: str, port: int, username: str, password: str, **kwargs): self.host = host self.port = port self.username = username self.password = password self.timeout = kwargs.get(\"timeout\", 30) self.connection = None","title":"Configuration"},{"location":"plugin_system/#health-checks","text":"Plugins should implement thorough health checks: def health_check(self) -> Tuple[bool, Dict[str, Any]]: \"\"\"Check if plugin is working correctly\"\"\" try: # Check connection if self.connection is None or not self.connection.is_connected(): self.connect() # Check functionality self.ping() # Return health information return True, { \"status\": \"connected\", \"host\": self.host, \"port\": self.port, \"connection_id\": self.connection.id } except Exception as e: return False, { \"status\": \"error\", \"message\": str(e), \"host\": self.host, \"port\": self.port }","title":"Health Checks"},{"location":"plugin_system/#documentation","text":"Document the purpose, configuration, and usage of your plugins: class CustomPlugin(Plugin): \"\"\"Custom plugin for integrating with Example Service. This plugin provides integration with Example Service, allowing the ExperimentHub to use Example Service for specific functionality. Configuration: api_key: API key for authenticating with Example Service host: Hostname of the Example Service API port: Port of the Example Service API timeout: Timeout for API requests (default: 30 seconds) Usage: plugin = CustomPlugin( api_key=\"your-api-key\", host=\"api.example.com\", port=443, timeout=60 ) hub.register_plugin(plugin) \"\"\" # ...","title":"Documentation"},{"location":"plugin_system/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"plugin_system/#plugin-initialization-errors","text":"If a plugin fails to initialize, check the plugin configuration and make sure all required parameters are provided.","title":"Plugin Initialization Errors"},{"location":"plugin_system/#plugin-health-check-failures","text":"If a plugin health check fails, check the health information for details on what went wrong.","title":"Plugin Health Check Failures"},{"location":"plugin_system/#missing-plugins","text":"If a plugin is missing, check the configuration file and make sure the plugin is enabled and configured correctly.","title":"Missing Plugins"},{"location":"plugins/","text":"ExperimentHub Plugin System The ExperimentHub plugin system provides a flexible and extensible architecture for integrating various services and functionalities into the ExperimentHub. This document explains the plugin architecture, how to use existing plugins, and how to create custom plugins. Plugin Architecture The plugin system is based on a Protocol-based architecture, where each plugin type defines an interface that implementations must adhere to. The main components of the plugin system are: graph TD A[ExperimentHub] --> B[Plugin System] B --> C[MLOps Plugin] B --> D[DataStream Plugin] B --> E[TaskQueue Plugin] C --> F[MLflowPlugin] D --> G[RabbitMQPlugin] E --> H[CeleryPlugin] Plugin Types The ExperimentHub supports three main types of plugins: MLOps Plugin : Handles model tracking, versioning, and artifact storage Default implementation: MLflowPlugin DataStream Plugin : Handles data streaming and messaging Default implementation: RabbitMQPlugin TaskQueue Plugin : Handles asynchronous task processing Default implementation: CeleryPlugin Plugin Interface All plugins must implement the Plugin Protocol defined in framework/plugins/base.py : class Plugin(Protocol): \"\"\"Base interface for all plugins\"\"\" plugin_type: str def __init__(self, **kwargs) -> None: \"\"\"Initialize the plugin with configuration\"\"\" ... def initialize(self) -> None: \"\"\"Initialize the plugin\"\"\" ... def shutdown(self) -> None: \"\"\"Clean up resources when shutting down\"\"\" ... def health_check(self) -> Tuple[bool, Dict[str, Any]]: \"\"\"Check if the plugin is working correctly Returns: Tuple containing: - Boolean indicating if the plugin is healthy - Dictionary with additional health information and metrics \"\"\" ... Each plugin type then extends this base interface with additional methods specific to its functionality. Configuration System The ExperimentHub plugin system is configured using a YAML configuration file ( hub_config.yaml ). This file specifies which plugins to use, their configurations, and available experiment types. Configuration File Structure # hub_config.yaml version: \"1.0\" # Global settings global: log_level: \"info\" environment: \"development\" # Plugin configurations plugins: # MLOps plugin configuration mlops: enabled: true type: \"mlflow\" # Which implementation to use config: mlflow_uri: \"http://localhost:5000\" # Data stream plugin configuration datastream: enabled: true type: \"rabbitmq\" config: host: \"localhost\" port: 5672 username: \"guest\" password: \"guest\" # Task queue plugin configuration taskqueue: enabled: false # Disabled by default, enable when needed type: \"celery\" config: broker_url: \"amqp://guest:guest@localhost:5672//\" backend_url: \"redis://localhost:6379/0\" app_name: \"experiment_hub\" # Available experiment types experiments: - name: \"time_series\" module: \"framework.TimeSeriesAnalysis\" class: \"TimeSeriesExperiment\" enabled: true - name: \"fault_detection\" module: \"framework.FaultDetection\" class: \"FaultDetectionExperiment\" enabled: true Loading Configuration The ExperimentHub can be initialized from a configuration file using the from_config class method: from framework.ExperimentHub import ExperimentHub # Initialize from configuration file hub = ExperimentHub.from_config(\"hub_config.yaml\") Using Plugins Once the ExperimentHub is initialized with plugins, you can use them through the ExperimentHub API: # Get a plugin by type mlops_plugin = hub.get_plugin(\"mlops\") datastream_plugin = hub.get_plugin(\"datastream\") taskqueue_plugin = hub.get_plugin(\"taskqueue\") # Check plugin health health_info = hub.check_plugin_health() Creating Custom Plugins You can create custom plugins by implementing the appropriate plugin interface. Here's an example of creating a custom MLOps plugin: from framework.plugins.mlops import MLOpsPlugin from typing import Any, Dict, Tuple class CustomMLOpsPlugin(MLOpsPlugin): \"\"\"Custom implementation of MLOpsPlugin\"\"\" plugin_type = \"mlops\" def __init__(self, custom_param: str, **kwargs): self.custom_param = custom_param def initialize(self) -> None: \"\"\"Initialize the plugin\"\"\" # Custom initialization logic pass def shutdown(self) -> None: \"\"\"Clean up resources\"\"\" # Custom cleanup logic pass def health_check(self) -> Tuple[bool, Dict[str, Any]]: \"\"\"Check if plugin is working correctly\"\"\" try: # Custom health check logic return True, {\"status\": \"connected\", \"custom_param\": self.custom_param} except Exception as e: return False, {\"status\": \"error\", \"message\": str(e)} # Implement other MLOpsPlugin methods # ... To use your custom plugin, you can register it with the ExperimentHub: from framework.ExperimentHub import ExperimentHub # Create ExperimentHub hub = ExperimentHub() # Create and register custom plugin custom_plugin = CustomMLOpsPlugin(custom_param=\"value\") hub.register_plugin(custom_plugin) Plugin Health Checks All plugins must implement a health_check method that verifies if the plugin is working correctly. This method returns a tuple containing: A boolean indicating if the plugin is healthy A dictionary with additional health information and metrics You can check the health of all registered plugins using the check_plugin_health method of the ExperimentHub: health_info = hub.check_plugin_health() for plugin_type, info in health_info.items(): status = \"HEALTHY\" if info[\"healthy\"] else \"UNHEALTHY\" print(f\"{plugin_type}: {status}\") print(f\" Details: {info['details']}\") Plugin Lifecycle Plugins go through the following lifecycle: Initialization : The plugin is created with configuration parameters Registration : The plugin is registered with the ExperimentHub Initialization : The initialize method is called to set up the plugin Usage : The plugin is used by the ExperimentHub Shutdown : The shutdown method is called to clean up resources Best Practices When working with plugins, follow these best practices: Error Handling : Plugins should handle errors gracefully and provide meaningful error messages Resource Management : Plugins should clean up resources in the shutdown method Configuration : Plugins should accept configuration parameters in the constructor Health Checks : Plugins should implement thorough health checks Documentation : Document the purpose, configuration, and usage of your plugins","title":"Plugins"},{"location":"plugins/#experimenthub-plugin-system","text":"The ExperimentHub plugin system provides a flexible and extensible architecture for integrating various services and functionalities into the ExperimentHub. This document explains the plugin architecture, how to use existing plugins, and how to create custom plugins.","title":"ExperimentHub Plugin System"},{"location":"plugins/#plugin-architecture","text":"The plugin system is based on a Protocol-based architecture, where each plugin type defines an interface that implementations must adhere to. The main components of the plugin system are: graph TD A[ExperimentHub] --> B[Plugin System] B --> C[MLOps Plugin] B --> D[DataStream Plugin] B --> E[TaskQueue Plugin] C --> F[MLflowPlugin] D --> G[RabbitMQPlugin] E --> H[CeleryPlugin]","title":"Plugin Architecture"},{"location":"plugins/#plugin-types","text":"The ExperimentHub supports three main types of plugins: MLOps Plugin : Handles model tracking, versioning, and artifact storage Default implementation: MLflowPlugin DataStream Plugin : Handles data streaming and messaging Default implementation: RabbitMQPlugin TaskQueue Plugin : Handles asynchronous task processing Default implementation: CeleryPlugin","title":"Plugin Types"},{"location":"plugins/#plugin-interface","text":"All plugins must implement the Plugin Protocol defined in framework/plugins/base.py : class Plugin(Protocol): \"\"\"Base interface for all plugins\"\"\" plugin_type: str def __init__(self, **kwargs) -> None: \"\"\"Initialize the plugin with configuration\"\"\" ... def initialize(self) -> None: \"\"\"Initialize the plugin\"\"\" ... def shutdown(self) -> None: \"\"\"Clean up resources when shutting down\"\"\" ... def health_check(self) -> Tuple[bool, Dict[str, Any]]: \"\"\"Check if the plugin is working correctly Returns: Tuple containing: - Boolean indicating if the plugin is healthy - Dictionary with additional health information and metrics \"\"\" ... Each plugin type then extends this base interface with additional methods specific to its functionality.","title":"Plugin Interface"},{"location":"plugins/#configuration-system","text":"The ExperimentHub plugin system is configured using a YAML configuration file ( hub_config.yaml ). This file specifies which plugins to use, their configurations, and available experiment types.","title":"Configuration System"},{"location":"plugins/#configuration-file-structure","text":"# hub_config.yaml version: \"1.0\" # Global settings global: log_level: \"info\" environment: \"development\" # Plugin configurations plugins: # MLOps plugin configuration mlops: enabled: true type: \"mlflow\" # Which implementation to use config: mlflow_uri: \"http://localhost:5000\" # Data stream plugin configuration datastream: enabled: true type: \"rabbitmq\" config: host: \"localhost\" port: 5672 username: \"guest\" password: \"guest\" # Task queue plugin configuration taskqueue: enabled: false # Disabled by default, enable when needed type: \"celery\" config: broker_url: \"amqp://guest:guest@localhost:5672//\" backend_url: \"redis://localhost:6379/0\" app_name: \"experiment_hub\" # Available experiment types experiments: - name: \"time_series\" module: \"framework.TimeSeriesAnalysis\" class: \"TimeSeriesExperiment\" enabled: true - name: \"fault_detection\" module: \"framework.FaultDetection\" class: \"FaultDetectionExperiment\" enabled: true","title":"Configuration File Structure"},{"location":"plugins/#loading-configuration","text":"The ExperimentHub can be initialized from a configuration file using the from_config class method: from framework.ExperimentHub import ExperimentHub # Initialize from configuration file hub = ExperimentHub.from_config(\"hub_config.yaml\")","title":"Loading Configuration"},{"location":"plugins/#using-plugins","text":"Once the ExperimentHub is initialized with plugins, you can use them through the ExperimentHub API: # Get a plugin by type mlops_plugin = hub.get_plugin(\"mlops\") datastream_plugin = hub.get_plugin(\"datastream\") taskqueue_plugin = hub.get_plugin(\"taskqueue\") # Check plugin health health_info = hub.check_plugin_health()","title":"Using Plugins"},{"location":"plugins/#creating-custom-plugins","text":"You can create custom plugins by implementing the appropriate plugin interface. Here's an example of creating a custom MLOps plugin: from framework.plugins.mlops import MLOpsPlugin from typing import Any, Dict, Tuple class CustomMLOpsPlugin(MLOpsPlugin): \"\"\"Custom implementation of MLOpsPlugin\"\"\" plugin_type = \"mlops\" def __init__(self, custom_param: str, **kwargs): self.custom_param = custom_param def initialize(self) -> None: \"\"\"Initialize the plugin\"\"\" # Custom initialization logic pass def shutdown(self) -> None: \"\"\"Clean up resources\"\"\" # Custom cleanup logic pass def health_check(self) -> Tuple[bool, Dict[str, Any]]: \"\"\"Check if plugin is working correctly\"\"\" try: # Custom health check logic return True, {\"status\": \"connected\", \"custom_param\": self.custom_param} except Exception as e: return False, {\"status\": \"error\", \"message\": str(e)} # Implement other MLOpsPlugin methods # ... To use your custom plugin, you can register it with the ExperimentHub: from framework.ExperimentHub import ExperimentHub # Create ExperimentHub hub = ExperimentHub() # Create and register custom plugin custom_plugin = CustomMLOpsPlugin(custom_param=\"value\") hub.register_plugin(custom_plugin)","title":"Creating Custom Plugins"},{"location":"plugins/#plugin-health-checks","text":"All plugins must implement a health_check method that verifies if the plugin is working correctly. This method returns a tuple containing: A boolean indicating if the plugin is healthy A dictionary with additional health information and metrics You can check the health of all registered plugins using the check_plugin_health method of the ExperimentHub: health_info = hub.check_plugin_health() for plugin_type, info in health_info.items(): status = \"HEALTHY\" if info[\"healthy\"] else \"UNHEALTHY\" print(f\"{plugin_type}: {status}\") print(f\" Details: {info['details']}\")","title":"Plugin Health Checks"},{"location":"plugins/#plugin-lifecycle","text":"Plugins go through the following lifecycle: Initialization : The plugin is created with configuration parameters Registration : The plugin is registered with the ExperimentHub Initialization : The initialize method is called to set up the plugin Usage : The plugin is used by the ExperimentHub Shutdown : The shutdown method is called to clean up resources","title":"Plugin Lifecycle"},{"location":"plugins/#best-practices","text":"When working with plugins, follow these best practices: Error Handling : Plugins should handle errors gracefully and provide meaningful error messages Resource Management : Plugins should clean up resources in the shutdown method Configuration : Plugins should accept configuration parameters in the constructor Health Checks : Plugins should implement thorough health checks Documentation : Document the purpose, configuration, and usage of your plugins","title":"Best Practices"},{"location":"rest_api/","text":"ObServML REST API Reference This comprehensive guide covers the ObServML REST API, framework architecture, and the dynamic configuration-to-function mapping system that powers the platform. Table of Contents API Overview Complete REST API Reference Framework Architecture Function-Config Mapping Configuration-Driven Workflows Advanced Usage Patterns Error Handling Client Libraries API Overview The ObServML REST API provides a comprehensive interface for managing machine learning experiments through a microservices architecture. Built with FastAPI, it enables: Experiment Lifecycle Management : Create, train, save, load, and delete experiments Real-time Predictions : Make predictions on streaming data via RabbitMQ Visualization : Access plots and EDA figures in JSON format MLOps Integration : Automatic model versioning and tracking with MLflow Configuration-Driven Operations : Dynamic function execution based on YAML configurations Base URL http://localhost:8010 Architecture Components ExperimentHub : Central orchestrator managing all experiments RabbitMQ : Message broker for data streaming and communication MLflow : Model registry and experiment tracking FastAPI : REST API framework with automatic documentation Complete REST API Reference The following table presents all available REST API endpoints as documented in the research paper and current implementation: Endpoint HTTP Method Description / GET Default path. Returns a \"Hello World\" message /health GET Check the health of all plugins (MLflow, RabbitMQ) /available_experiments GET Get available experiment types and their configurations /flush/{queue} POST Removes all data from the specified RabbitMQ queue /create_experiment/{name}/{experiment_type} POST Create a new experiment of the specified type /{name}/train POST Starts training for the specified experiment, setting up the training configuration. Requires data in the Rabbit queue and a configuration file /{name}/load POST Loads an experiment by its name /{name}/load/{run_id} POST Loads an experiment using the specified run ID from MLflow /{name}/save POST Saves the current experiment to the MLflow server /{name}/predict POST Initiates a prediction call for the specified experiment. Requires a request and data in the RabbitMQ queue {name} /{name}/retrain POST Starts retraining an experiment as requested /{name}/stop_training POST Stop the training process for an experiment without removing it /{name}/delete POST Remove an experiment from memory /{name}/plot/{plot_name} GET Returns a plot (figure) for the specified experiment and plot name in JSON /{name}/plot_eda/{plot_name} GET Returns the EDA plot (figure) for the specified experiment and plot name in JSON /{name}/train_data GET Returns training data for the specified experiment in JSON /{name}/cfg GET Returns the configuration details of the specified experiment /experiments GET Lists all experiment names and available figure names /{name}/run_id GET Returns the run ID (MLflow) of the specified experiment /{name}/exp_id GET Returns the experiment ID (MLflow) of the specified experiment Framework Architecture ObServML uses a configuration-driven architecture where YAML configuration files dynamically control experiment behavior. The core principle is: Each YAML configuration key corresponds to a method in the Experiment class Core Components 1. Experiment Class (Abstract Base) The Experiment class serves as the foundation for all experiment types: class Experiment(Protocol): \"\"\"Abstract base class for all experiments\"\"\" # Core attributes model: any = None data: pd.DataFrame = None cfg: dict = None _model_registry: dict[str, type] = dict() _report_registry: dict[str, go.Figure] = dict() _eda_registry: dict[str, go.Figure] = dict() # MLflow integration run_id: str = None experiment_id: str = None mlflow_uri: str = None 2. Dynamic Function Execution The run() method implements the core configuration-to-function mapping: def run(self, data: pd.DataFrame) -> None: \"\"\"Dynamically execute functions based on configuration keys\"\"\" # Extract function names from config (excluding load_object) funcs = list(self.cfg.keys()) funcs.remove(\"load_object\") self.data = data for k in funcs: if k == \"setup\": self.setup(data) else: # Dynamic function call using getattr getattr(self, k)() 3. Experiment Types ObServML provides four main experiment types, each inheriting from the base Experiment class: TimeSeriesAnalysis : For forecasting and temporal anomaly detection FaultDetection : For unsupervised anomaly detection FaultIsolation : For supervised classification and root cause analysis ProcessMining : For workflow and sequence analysis Function-Config Mapping The configuration-to-function mapping system enables dynamic experiment execution without code changes. Here's how it works: Core Framework Functions Required Functions (Must be implemented by experiment types) Function Config Key Purpose Parameters setup() setup Data preparation and experiment initialization data: pd.DataFrame, **kwargs create_model() create_model Model training and configuration **kwargs predict() predict Prediction on new data data: pd.DataFrame, **kwargs _score() N/A Calculate model metrics y, y_hat Optional Functions (Can be included in config) Function Config Key Purpose Parameters eda() eda Exploratory data analysis None retrain() retrain Model retraining logic None format_data() format Custom data formatting data, format: dict Built-in Functions (Always available) Function Purpose Usage save() Save experiment to MLflow Called via API endpoint load() Load experiment from MLflow Called via API endpoint plot_model() Retrieve model visualizations Called via API endpoint export() Export reports as HTML Called internally join_data() Combine training and new data Called during retraining Configuration Structure Every experiment configuration follows this structure: load_object: module: framework.{ExperimentType} # Python module path name: {ExperimentType}Experiment # Class name setup: # Maps to setup() function datetime_column: \"timestamp\" target: \"target_variable\" # Additional setup parameters... eda: # Maps to eda() function # EDA configuration (empty = use defaults) create_model: # Maps to create_model() function model: \"model_name\" params: # Model-specific parameters # Additional custom functions can be added custom_preprocessing: # Maps to custom_preprocessing() function param1: value1 param2: value2 Parameter Passing Parameters from the configuration are passed to functions using Python's **kwargs mechanism: # Configuration setup: datetime_column: \"timestamp\" target: \"value\" predict_window: 1000 # Function call (automatically generated) self.setup(data, datetime_column=\"timestamp\", target=\"value\", predict_window=1000) Configuration-Driven Workflows 1. Training Workflow graph TD A[POST /{name}/train] --> B[Parse Configuration] B --> C[Create Experiment Instance] C --> D[Load Data from RabbitMQ] D --> E[Execute run() method] E --> F[Dynamic Function Calls] F --> G[setup()] F --> H[eda()] F --> I[create_model()] I --> J[Save to MLflow] J --> K[Return Response] 2. Prediction Workflow graph TD A[POST /{name}/predict] --> B[Load Experiment] B --> C[Get Data from RabbitMQ] C --> D[Call predict() method] D --> E[Update Visualizations] E --> F[Return Results] 3. Configuration Processing When a training request is received: Configuration Parsing : YAML config is loaded and validated Dynamic Import : Experiment class is loaded using load_object specification Instance Creation : Experiment instance is created with config Function Mapping : Config keys are mapped to class methods Sequential Execution : Functions are called in order based on config structure Advanced Usage Patterns 1. Multi-Model Training NOTE: YOu must have sent a data already on rabbit before the Curl can properly start the training process. Train multiple models with different configurations: # Train Isolation Forest curl -X POST \"http://localhost:8010/pump_iforest/train\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"load_object\": { \"module\": \"framework.FaultDetection\", \"name\": \"FaultDetectionExperiment\" }, \"setup\": {\"datetime_column\": \"ds\"}, \"eda\": {}, \"create_model\": { \"model\": \"iforest\", \"params\": {\"n_estimators\": 100, \"contamination\": \"auto\"} } }' # Train PCA model curl -X POST \"http://localhost:8010/pump_pca/train\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"load_object\": { \"module\": \"framework.FaultDetection\", \"name\": \"FaultDetectionExperiment\" }, \"setup\": {\"datetime_column\": \"ds\"}, \"create_model\": { \"model\": \"pca\", \"params\": {\"n_components\": 0.95, \"alpha\": 0.05} } }' 2. Automated Retraining Pipeline Configure automatic retraining based on performance metrics: setup: datetime_column: \"ds\" target: \"fault_type\" retrain: retrain_window: 5000 metric: \"Accuracy\" metric_threshold: 0.85 higher_better: true create_model: model: \"dt\" params: {} 3. Custom Function Integration Add custom preprocessing functions: load_object: module: framework.FaultIsolation name: FaultIsolationExperiment setup: datetime_column: \"ds\" target: \"output\" custom_preprocessing: # Custom function normalize: true remove_outliers: true outlier_threshold: 3 eda: create_model: model: \"rf\" params: n_estimators: 100 4. Batch Processing Process multiple experiments in sequence: #!/bin/bash # Batch training script experiments=(\"pump_iforest\" \"pump_pca\" \"pump_dbscan\") models=(\"iforest\" \"pca\" \"dbscan\") for i in \"${!experiments[@]}\"; do echo \"Training ${experiments[$i]} with ${models[$i]}\" curl -X POST \"http://localhost:8010/${experiments[$i]}/train\" \\ -H \"Content-Type: application/json\" \\ -d @configs/pump/${models[$i]}.yaml # Wait for training to complete sleep 30 # Make initial prediction curl -X POST \"http://localhost:8010/${experiments[$i]}/predict\" done Common Error Scenarios 1. Configuration Errors { \"detail\": \"Configuration validation failed: missing required field 'target'\" } Solution : Ensure all required configuration fields are present. 2. Experiment Not Found { \"detail\": \"Experiment 'nonexistent_model' not found\" } Solution : Check experiment name and ensure it has been created/loaded. 3. Model Training Failures { \"detail\": \"Model training failed: insufficient data\" } Solution : Verify data is available in RabbitMQ queue and meets model requirements. 4. Plugin Health Issues { \"mlops\": { \"healthy\": false, \"details\": { \"error\": \"Connection refused to MLflow server\" } } } Solution : Check MLflow server status and connectivity. Error Recovery Patterns def robust_api_call(url, max_retries=3, backoff_factor=2): \"\"\"Make API calls with retry logic\"\"\" for attempt in range(max_retries): try: response = requests.post(url, timeout=30) response.raise_for_status() return response except requests.exceptions.RequestException as e: if attempt == max_retries - 1: raise e wait_time = backoff_factor ** attempt print(f\"Attempt {attempt + 1} failed, retrying in {wait_time}s...\") time.sleep(wait_time) Client Libraries ObServML Python Client The ObServML Python Client is available as observml_client.py in the project root directory. This comprehensive client combines the functionality from api_commands.py with enhanced error handling, type hints, and modern Python practices. Key Features: - Enhanced Error Handling : Automatic retry logic with exponential backoff - Type Hints : Full type annotations for better IDE support - RabbitMQ Integration : Automatic data posting to message queues - File Format Support : Load data from .xlsx, .csv, .json, .pkl files - Backward Compatibility : Legacy functions for existing code - Comprehensive Logging : Built-in logging for debugging - Monitoring Support : Continuous experiment monitoring capabilities Installation: # Import the client from observml_client import ObServMLClient # Or use legacy functions for backward compatibility from observml_client import train, predict, load_experiment Python Client Usage import requests import json from typing import Dict, Any, Optional class ObServMLClient: \"\"\"Comprehensive Python client for ObServML API\"\"\" def __init__(self, base_url: str = \"http://localhost:8010\"): self.base_url = base_url.rstrip('/') def health_check(self) -> Dict[str, Any]: \"\"\"Check system health\"\"\" response = requests.get(f\"{self.base_url}/health\") response.raise_for_status() return response.json() def get_available_experiments(self) -> Dict[str, Any]: \"\"\"Get available experiment types\"\"\" response = requests.get(f\"{self.base_url}/available_experiments\") response.raise_for_status() return response.json() def create_experiment(self, name: str, experiment_type: str, config: Dict[str, Any]) -> Dict[str, Any]: \"\"\"Create a new experiment\"\"\" response = requests.post( f\"{self.base_url}/create_experiment/{name}/{experiment_type}\", json=config ) response.raise_for_status() return response.json() def train_experiment(self, name: str, config: Dict[str, Any]) -> str: \"\"\"Train an experiment\"\"\" response = requests.post(f\"{self.base_url}/{name}/train\", json=config) response.raise_for_status() return response.text def predict(self, name: str) -> str: \"\"\"Make predictions\"\"\" response = requests.post(f\"{self.base_url}/{name}/predict\") response.raise_for_status() return response.text def load_experiment(self, name: str, run_id: Optional[str] = None) -> str: \"\"\"Load an experiment\"\"\" if run_id: url = f\"{self.base_url}/{name}/load/{run_id}\" else: url = f\"{self.base_url}/{name}/load\" response = requests.post(url) response.raise_for_status() return response.text def save_experiment(self, name: str) -> str: \"\"\"Save an experiment\"\"\" response = requests.post(f\"{self.base_url}/{name}/save\") response.raise_for_status() return response.text def get_plot(self, name: str, plot_name: str) -> Dict[str, Any]: \"\"\"Get a plot from an experiment\"\"\" response = requests.get(f\"{self.base_url}/{name}/plot/{plot_name}\") response.raise_for_status() return response.json() def get_eda_plot(self, name: str, plot_name: str) -> Dict[str, Any]: \"\"\"Get an EDA plot from an experiment\"\"\" response = requests.get(f\"{self.base_url}/{name}/plot_eda/{plot_name}\") response.raise_for_status() return response.json() def get_configuration(self, name: str) -> Dict[str, Any]: \"\"\"Get experiment configuration\"\"\" response = requests.get(f\"{self.base_url}/{name}/cfg\") response.raise_for_status() return response.json() def get_experiments(self) -> Dict[str, Any]: \"\"\"Get all experiments and their available plots\"\"\" response = requests.get(f\"{self.base_url}/experiments\") response.raise_for_status() return response.json() def flush_queue(self, queue_name: str) -> str: \"\"\"Flush a RabbitMQ queue\"\"\" response = requests.post(f\"{self.base_url}/flush/{queue_name}\") response.raise_for_status() return response.text def retrain_experiment(self, name: str) -> str: \"\"\"Retrain an experiment\"\"\" response = requests.post(f\"{self.base_url}/{name}/retrain\") response.raise_for_status() return response.text def delete_experiment(self, name: str) -> Dict[str, Any]: \"\"\"Delete an experiment from memory\"\"\" response = requests.post(f\"{self.base_url}/{name}/delete\") response.raise_for_status() return response.json() # Usage example client = ObServMLClient() # Check system health health = client.health_check() print(f\"System health: {json.dumps(health, indent=2)}\") # Create and train a fault detection experiment config = { \"load_object\": { \"module\": \"framework.FaultDetection\", \"name\": \"FaultDetectionExperiment\" }, \"setup\": { \"datetime_column\": \"ds\" }, \"eda\": {}, \"create_model\": { \"model\": \"iforest\", \"params\": { \"n_estimators\": 100, \"contamination\": \"auto\" } } } result = client.train_experiment(\"pump_anomaly\", config) print(f\"Training result: {result}\") # Make predictions prediction = client.predict(\"pump_anomaly\") print(f\"Prediction result: {prediction}\") # Get anomaly plot plot = client.get_plot(\"pump_anomaly\", \"outliers\") print(f\"Plot keys: {list(plot.keys())}\")","title":"REST API Reference"},{"location":"rest_api/#observml-rest-api-reference","text":"This comprehensive guide covers the ObServML REST API, framework architecture, and the dynamic configuration-to-function mapping system that powers the platform.","title":"ObServML REST API Reference"},{"location":"rest_api/#table-of-contents","text":"API Overview Complete REST API Reference Framework Architecture Function-Config Mapping Configuration-Driven Workflows Advanced Usage Patterns Error Handling Client Libraries","title":"Table of Contents"},{"location":"rest_api/#api-overview","text":"The ObServML REST API provides a comprehensive interface for managing machine learning experiments through a microservices architecture. Built with FastAPI, it enables: Experiment Lifecycle Management : Create, train, save, load, and delete experiments Real-time Predictions : Make predictions on streaming data via RabbitMQ Visualization : Access plots and EDA figures in JSON format MLOps Integration : Automatic model versioning and tracking with MLflow Configuration-Driven Operations : Dynamic function execution based on YAML configurations","title":"API Overview"},{"location":"rest_api/#base-url","text":"http://localhost:8010","title":"Base URL"},{"location":"rest_api/#architecture-components","text":"ExperimentHub : Central orchestrator managing all experiments RabbitMQ : Message broker for data streaming and communication MLflow : Model registry and experiment tracking FastAPI : REST API framework with automatic documentation","title":"Architecture Components"},{"location":"rest_api/#complete-rest-api-reference","text":"The following table presents all available REST API endpoints as documented in the research paper and current implementation: Endpoint HTTP Method Description / GET Default path. Returns a \"Hello World\" message /health GET Check the health of all plugins (MLflow, RabbitMQ) /available_experiments GET Get available experiment types and their configurations /flush/{queue} POST Removes all data from the specified RabbitMQ queue /create_experiment/{name}/{experiment_type} POST Create a new experiment of the specified type /{name}/train POST Starts training for the specified experiment, setting up the training configuration. Requires data in the Rabbit queue and a configuration file /{name}/load POST Loads an experiment by its name /{name}/load/{run_id} POST Loads an experiment using the specified run ID from MLflow /{name}/save POST Saves the current experiment to the MLflow server /{name}/predict POST Initiates a prediction call for the specified experiment. Requires a request and data in the RabbitMQ queue {name} /{name}/retrain POST Starts retraining an experiment as requested /{name}/stop_training POST Stop the training process for an experiment without removing it /{name}/delete POST Remove an experiment from memory /{name}/plot/{plot_name} GET Returns a plot (figure) for the specified experiment and plot name in JSON /{name}/plot_eda/{plot_name} GET Returns the EDA plot (figure) for the specified experiment and plot name in JSON /{name}/train_data GET Returns training data for the specified experiment in JSON /{name}/cfg GET Returns the configuration details of the specified experiment /experiments GET Lists all experiment names and available figure names /{name}/run_id GET Returns the run ID (MLflow) of the specified experiment /{name}/exp_id GET Returns the experiment ID (MLflow) of the specified experiment","title":"Complete REST API Reference"},{"location":"rest_api/#framework-architecture","text":"ObServML uses a configuration-driven architecture where YAML configuration files dynamically control experiment behavior. The core principle is: Each YAML configuration key corresponds to a method in the Experiment class","title":"Framework Architecture"},{"location":"rest_api/#core-components","text":"","title":"Core Components"},{"location":"rest_api/#1-experiment-class-abstract-base","text":"The Experiment class serves as the foundation for all experiment types: class Experiment(Protocol): \"\"\"Abstract base class for all experiments\"\"\" # Core attributes model: any = None data: pd.DataFrame = None cfg: dict = None _model_registry: dict[str, type] = dict() _report_registry: dict[str, go.Figure] = dict() _eda_registry: dict[str, go.Figure] = dict() # MLflow integration run_id: str = None experiment_id: str = None mlflow_uri: str = None","title":"1. Experiment Class (Abstract Base)"},{"location":"rest_api/#2-dynamic-function-execution","text":"The run() method implements the core configuration-to-function mapping: def run(self, data: pd.DataFrame) -> None: \"\"\"Dynamically execute functions based on configuration keys\"\"\" # Extract function names from config (excluding load_object) funcs = list(self.cfg.keys()) funcs.remove(\"load_object\") self.data = data for k in funcs: if k == \"setup\": self.setup(data) else: # Dynamic function call using getattr getattr(self, k)()","title":"2. Dynamic Function Execution"},{"location":"rest_api/#3-experiment-types","text":"ObServML provides four main experiment types, each inheriting from the base Experiment class: TimeSeriesAnalysis : For forecasting and temporal anomaly detection FaultDetection : For unsupervised anomaly detection FaultIsolation : For supervised classification and root cause analysis ProcessMining : For workflow and sequence analysis","title":"3. Experiment Types"},{"location":"rest_api/#function-config-mapping","text":"The configuration-to-function mapping system enables dynamic experiment execution without code changes. Here's how it works:","title":"Function-Config Mapping"},{"location":"rest_api/#core-framework-functions","text":"","title":"Core Framework Functions"},{"location":"rest_api/#required-functions-must-be-implemented-by-experiment-types","text":"Function Config Key Purpose Parameters setup() setup Data preparation and experiment initialization data: pd.DataFrame, **kwargs create_model() create_model Model training and configuration **kwargs predict() predict Prediction on new data data: pd.DataFrame, **kwargs _score() N/A Calculate model metrics y, y_hat","title":"Required Functions (Must be implemented by experiment types)"},{"location":"rest_api/#optional-functions-can-be-included-in-config","text":"Function Config Key Purpose Parameters eda() eda Exploratory data analysis None retrain() retrain Model retraining logic None format_data() format Custom data formatting data, format: dict","title":"Optional Functions (Can be included in config)"},{"location":"rest_api/#built-in-functions-always-available","text":"Function Purpose Usage save() Save experiment to MLflow Called via API endpoint load() Load experiment from MLflow Called via API endpoint plot_model() Retrieve model visualizations Called via API endpoint export() Export reports as HTML Called internally join_data() Combine training and new data Called during retraining","title":"Built-in Functions (Always available)"},{"location":"rest_api/#configuration-structure","text":"Every experiment configuration follows this structure: load_object: module: framework.{ExperimentType} # Python module path name: {ExperimentType}Experiment # Class name setup: # Maps to setup() function datetime_column: \"timestamp\" target: \"target_variable\" # Additional setup parameters... eda: # Maps to eda() function # EDA configuration (empty = use defaults) create_model: # Maps to create_model() function model: \"model_name\" params: # Model-specific parameters # Additional custom functions can be added custom_preprocessing: # Maps to custom_preprocessing() function param1: value1 param2: value2","title":"Configuration Structure"},{"location":"rest_api/#parameter-passing","text":"Parameters from the configuration are passed to functions using Python's **kwargs mechanism: # Configuration setup: datetime_column: \"timestamp\" target: \"value\" predict_window: 1000 # Function call (automatically generated) self.setup(data, datetime_column=\"timestamp\", target=\"value\", predict_window=1000)","title":"Parameter Passing"},{"location":"rest_api/#configuration-driven-workflows","text":"","title":"Configuration-Driven Workflows"},{"location":"rest_api/#1-training-workflow","text":"graph TD A[POST /{name}/train] --> B[Parse Configuration] B --> C[Create Experiment Instance] C --> D[Load Data from RabbitMQ] D --> E[Execute run() method] E --> F[Dynamic Function Calls] F --> G[setup()] F --> H[eda()] F --> I[create_model()] I --> J[Save to MLflow] J --> K[Return Response]","title":"1. Training Workflow"},{"location":"rest_api/#2-prediction-workflow","text":"graph TD A[POST /{name}/predict] --> B[Load Experiment] B --> C[Get Data from RabbitMQ] C --> D[Call predict() method] D --> E[Update Visualizations] E --> F[Return Results]","title":"2. Prediction Workflow"},{"location":"rest_api/#3-configuration-processing","text":"When a training request is received: Configuration Parsing : YAML config is loaded and validated Dynamic Import : Experiment class is loaded using load_object specification Instance Creation : Experiment instance is created with config Function Mapping : Config keys are mapped to class methods Sequential Execution : Functions are called in order based on config structure","title":"3. Configuration Processing"},{"location":"rest_api/#advanced-usage-patterns","text":"","title":"Advanced Usage Patterns"},{"location":"rest_api/#1-multi-model-training","text":"NOTE: YOu must have sent a data already on rabbit before the Curl can properly start the training process. Train multiple models with different configurations: # Train Isolation Forest curl -X POST \"http://localhost:8010/pump_iforest/train\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"load_object\": { \"module\": \"framework.FaultDetection\", \"name\": \"FaultDetectionExperiment\" }, \"setup\": {\"datetime_column\": \"ds\"}, \"eda\": {}, \"create_model\": { \"model\": \"iforest\", \"params\": {\"n_estimators\": 100, \"contamination\": \"auto\"} } }' # Train PCA model curl -X POST \"http://localhost:8010/pump_pca/train\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"load_object\": { \"module\": \"framework.FaultDetection\", \"name\": \"FaultDetectionExperiment\" }, \"setup\": {\"datetime_column\": \"ds\"}, \"create_model\": { \"model\": \"pca\", \"params\": {\"n_components\": 0.95, \"alpha\": 0.05} } }'","title":"1. Multi-Model Training"},{"location":"rest_api/#2-automated-retraining-pipeline","text":"Configure automatic retraining based on performance metrics: setup: datetime_column: \"ds\" target: \"fault_type\" retrain: retrain_window: 5000 metric: \"Accuracy\" metric_threshold: 0.85 higher_better: true create_model: model: \"dt\" params: {}","title":"2. Automated Retraining Pipeline"},{"location":"rest_api/#3-custom-function-integration","text":"Add custom preprocessing functions: load_object: module: framework.FaultIsolation name: FaultIsolationExperiment setup: datetime_column: \"ds\" target: \"output\" custom_preprocessing: # Custom function normalize: true remove_outliers: true outlier_threshold: 3 eda: create_model: model: \"rf\" params: n_estimators: 100","title":"3. Custom Function Integration"},{"location":"rest_api/#4-batch-processing","text":"Process multiple experiments in sequence: #!/bin/bash # Batch training script experiments=(\"pump_iforest\" \"pump_pca\" \"pump_dbscan\") models=(\"iforest\" \"pca\" \"dbscan\") for i in \"${!experiments[@]}\"; do echo \"Training ${experiments[$i]} with ${models[$i]}\" curl -X POST \"http://localhost:8010/${experiments[$i]}/train\" \\ -H \"Content-Type: application/json\" \\ -d @configs/pump/${models[$i]}.yaml # Wait for training to complete sleep 30 # Make initial prediction curl -X POST \"http://localhost:8010/${experiments[$i]}/predict\" done","title":"4. Batch Processing"},{"location":"rest_api/#common-error-scenarios","text":"","title":"Common Error Scenarios"},{"location":"rest_api/#1-configuration-errors","text":"{ \"detail\": \"Configuration validation failed: missing required field 'target'\" } Solution : Ensure all required configuration fields are present.","title":"1. Configuration Errors"},{"location":"rest_api/#2-experiment-not-found","text":"{ \"detail\": \"Experiment 'nonexistent_model' not found\" } Solution : Check experiment name and ensure it has been created/loaded.","title":"2. Experiment Not Found"},{"location":"rest_api/#3-model-training-failures","text":"{ \"detail\": \"Model training failed: insufficient data\" } Solution : Verify data is available in RabbitMQ queue and meets model requirements.","title":"3. Model Training Failures"},{"location":"rest_api/#4-plugin-health-issues","text":"{ \"mlops\": { \"healthy\": false, \"details\": { \"error\": \"Connection refused to MLflow server\" } } } Solution : Check MLflow server status and connectivity.","title":"4. Plugin Health Issues"},{"location":"rest_api/#error-recovery-patterns","text":"def robust_api_call(url, max_retries=3, backoff_factor=2): \"\"\"Make API calls with retry logic\"\"\" for attempt in range(max_retries): try: response = requests.post(url, timeout=30) response.raise_for_status() return response except requests.exceptions.RequestException as e: if attempt == max_retries - 1: raise e wait_time = backoff_factor ** attempt print(f\"Attempt {attempt + 1} failed, retrying in {wait_time}s...\") time.sleep(wait_time)","title":"Error Recovery Patterns"},{"location":"rest_api/#client-libraries","text":"","title":"Client Libraries"},{"location":"rest_api/#observml-python-client","text":"The ObServML Python Client is available as observml_client.py in the project root directory. This comprehensive client combines the functionality from api_commands.py with enhanced error handling, type hints, and modern Python practices. Key Features: - Enhanced Error Handling : Automatic retry logic with exponential backoff - Type Hints : Full type annotations for better IDE support - RabbitMQ Integration : Automatic data posting to message queues - File Format Support : Load data from .xlsx, .csv, .json, .pkl files - Backward Compatibility : Legacy functions for existing code - Comprehensive Logging : Built-in logging for debugging - Monitoring Support : Continuous experiment monitoring capabilities Installation: # Import the client from observml_client import ObServMLClient # Or use legacy functions for backward compatibility from observml_client import train, predict, load_experiment","title":"ObServML Python Client"},{"location":"rest_api/#python-client-usage","text":"import requests import json from typing import Dict, Any, Optional class ObServMLClient: \"\"\"Comprehensive Python client for ObServML API\"\"\" def __init__(self, base_url: str = \"http://localhost:8010\"): self.base_url = base_url.rstrip('/') def health_check(self) -> Dict[str, Any]: \"\"\"Check system health\"\"\" response = requests.get(f\"{self.base_url}/health\") response.raise_for_status() return response.json() def get_available_experiments(self) -> Dict[str, Any]: \"\"\"Get available experiment types\"\"\" response = requests.get(f\"{self.base_url}/available_experiments\") response.raise_for_status() return response.json() def create_experiment(self, name: str, experiment_type: str, config: Dict[str, Any]) -> Dict[str, Any]: \"\"\"Create a new experiment\"\"\" response = requests.post( f\"{self.base_url}/create_experiment/{name}/{experiment_type}\", json=config ) response.raise_for_status() return response.json() def train_experiment(self, name: str, config: Dict[str, Any]) -> str: \"\"\"Train an experiment\"\"\" response = requests.post(f\"{self.base_url}/{name}/train\", json=config) response.raise_for_status() return response.text def predict(self, name: str) -> str: \"\"\"Make predictions\"\"\" response = requests.post(f\"{self.base_url}/{name}/predict\") response.raise_for_status() return response.text def load_experiment(self, name: str, run_id: Optional[str] = None) -> str: \"\"\"Load an experiment\"\"\" if run_id: url = f\"{self.base_url}/{name}/load/{run_id}\" else: url = f\"{self.base_url}/{name}/load\" response = requests.post(url) response.raise_for_status() return response.text def save_experiment(self, name: str) -> str: \"\"\"Save an experiment\"\"\" response = requests.post(f\"{self.base_url}/{name}/save\") response.raise_for_status() return response.text def get_plot(self, name: str, plot_name: str) -> Dict[str, Any]: \"\"\"Get a plot from an experiment\"\"\" response = requests.get(f\"{self.base_url}/{name}/plot/{plot_name}\") response.raise_for_status() return response.json() def get_eda_plot(self, name: str, plot_name: str) -> Dict[str, Any]: \"\"\"Get an EDA plot from an experiment\"\"\" response = requests.get(f\"{self.base_url}/{name}/plot_eda/{plot_name}\") response.raise_for_status() return response.json() def get_configuration(self, name: str) -> Dict[str, Any]: \"\"\"Get experiment configuration\"\"\" response = requests.get(f\"{self.base_url}/{name}/cfg\") response.raise_for_status() return response.json() def get_experiments(self) -> Dict[str, Any]: \"\"\"Get all experiments and their available plots\"\"\" response = requests.get(f\"{self.base_url}/experiments\") response.raise_for_status() return response.json() def flush_queue(self, queue_name: str) -> str: \"\"\"Flush a RabbitMQ queue\"\"\" response = requests.post(f\"{self.base_url}/flush/{queue_name}\") response.raise_for_status() return response.text def retrain_experiment(self, name: str) -> str: \"\"\"Retrain an experiment\"\"\" response = requests.post(f\"{self.base_url}/{name}/retrain\") response.raise_for_status() return response.text def delete_experiment(self, name: str) -> Dict[str, Any]: \"\"\"Delete an experiment from memory\"\"\" response = requests.post(f\"{self.base_url}/{name}/delete\") response.raise_for_status() return response.json() # Usage example client = ObServMLClient() # Check system health health = client.health_check() print(f\"System health: {json.dumps(health, indent=2)}\") # Create and train a fault detection experiment config = { \"load_object\": { \"module\": \"framework.FaultDetection\", \"name\": \"FaultDetectionExperiment\" }, \"setup\": { \"datetime_column\": \"ds\" }, \"eda\": {}, \"create_model\": { \"model\": \"iforest\", \"params\": { \"n_estimators\": 100, \"contamination\": \"auto\" } } } result = client.train_experiment(\"pump_anomaly\", config) print(f\"Training result: {result}\") # Make predictions prediction = client.predict(\"pump_anomaly\") print(f\"Prediction result: {prediction}\") # Get anomaly plot plot = client.get_plot(\"pump_anomaly\", \"outliers\") print(f\"Plot keys: {list(plot.keys())}\")","title":"Python Client Usage"},{"location":"roadmap/","text":"Roadmap for further improvemnts Introducing uv for dependency handling and Docker generation Python Package Click-based Command line Interface for ease of use Authorization (OAuth2, Secure IoT) Logging and testing environments Experiment specific improvements, see ... Automatic retraining Webhooks (sending alarms through email) Git integration for extending the deployed service through repositories. Adding celery for faster concurrency Adding Mlflow registry (ExperimentHub ID - name of model in registry) More figures on frontend (select relevant figures) Postgre-dv Data query for pandas datasets. ObServML package","title":"Roadmap"},{"location":"roadmap/#roadmap-for-further-improvemnts","text":"Introducing uv for dependency handling and Docker generation Python Package Click-based Command line Interface for ease of use Authorization (OAuth2, Secure IoT) Logging and testing environments Experiment specific improvements, see ... Automatic retraining Webhooks (sending alarms through email) Git integration for extending the deployed service through repositories. Adding celery for faster concurrency Adding Mlflow registry (ExperimentHub ID - name of model in registry) More figures on frontend (select relevant figures) Postgre-dv Data query for pandas datasets.","title":"Roadmap for further improvemnts"},{"location":"roadmap/#observml-package","text":"","title":"ObServML package"},{"location":"serve/","text":"Containerization of ObServML -- Docker This section will tell the reader how to use the ObServML in production. For starters, you can find .dockerfiles (for mlflow, backend) in the repository, which have to be built first to be used. Docker compose will start the ObServML by launching interconnected microservices with the help of docker compose. The example project contains the docker-compose.yaml and the configuration files we require. Project template Pull \"main\" branch to obtain the code to communicate with the backend. You can use the project template's mindeps.txt for minimal dependency: pip install -r mindeps.txt NOTE: Minimal dependencies will make you unable to run locally. It will only allow for scripts to run. This way, the size of the virtual environment is reduced significantly. RabbitMQ This image is relatively easy to get. It should be pulled from this Docker repository. Docker is used in terminal, so either cmd, or a python IDE can accomodate. Walkthrough for: - Linux - Windows - VSCode - Pycharm In this version of ObServML, we tested with rabbitmq:3.12.14-management-alpine, but can be used with any version that is less than 2.0.0. So type into the terminal: docker pull rabbitmq:3.12.14-management-alpine ## pull image_name:image_tag If you use Docker Desktop, and everything went fine, then the image will appear: Building images In case of not being able to access to the prebuild Docker images, we also provide a tutorial on creating these images. DOCKER ENGINE IS REQUIRED! First you have to pull the git repository, and then navigate to the \"main\" branch: git pull ... git checkout main NOTE: BUILDING THE IMAGES DO NOT REQUIRE A WORKING VIRTUAL ENVIRONMENT! Building requires the following Docker command: docker build . -t {name}:{tag} -f {filename.dockerfile, preferably in root directory} If you want to build the images locally: docker build . -t mmlw:mlflow -f mlflow.dockerfile docker build . -t mmlw:frontend -f streamlit_frontend.dockerfile docker build . -t mmlw:backend -f service.dockerfile For the first run with build, the backend may take up to 1000s to build this image, which is reduced to ~600s after caching. Please do not modify the branch (DO NOT push to branch) -- open a new branch instead, if you modify the code: git branch [--branch_name] NOTE: The Rabbit image must be pulled for successful start. Running Docker images with compose Docker compose [3] starts the application from images and creates a network, through which the applications can communicate. It requires a docker-compose.yaml file that specifies the behaviour of the stack. The structure is as follows: services: service_A : # service name image : A:latest # image-name:tag ports: - 5100:5672 ## will expose local port # 5672 to global 5100, outside is only reachable from 5100 environment: HUB_URL : \"http://service_B:8010\" ## OS environmental variables # defined in dockerfile ## can refer to service_B IP address as service_B networks: - common_network service_B : image : B:latest ports: - 8010:8010 # CAN be the same networks: - common_network networks: # define existing/nonexisit networks to be used # can also be an outside network... common_network: ## name driver: bridge name: common_network # we recommend this to be the same as the key A docker-compose file is already provided in the GitHub repository (mmlw_template_project branch), and therefore will not be necessary to write out own. For further understanding, please see [5] . Its contents are the following: services: rabbit: image: rabbitmq:3.12.14-management-alpine ports: - 5100:5672 - 15100:15672 networks: - mml_network restart: on-failure mlflow: image: mmlw:mlflow ports: - 5000:5000 networks: - mml_network restart: on-failure backend: image: mmlw:backend ports : - 8010:8010 depends_on: rabbit : condition: service_started mlflow : condition: service_started environment: MLFLOW_URI : \"http://mlflow:5000\" RABBIT_HOST : \"rabbit\" RABBIT_PORT : \"5672\" RABBIT_USER : \"guest\" RABBIT_PASSWORD : \"guest\" restart: on-failure networks: - mml_network frontend: image: mmlw:frontend ports: - 8501:8501 depends_on: backend: condition: service_started environment: HUB_URL : \"http://backend:8010\" restart: on-failure networks: - mml_network networks: mml_network: driver: bridge name: mml_network To run the set of applications, type in: docker compose up What you will see in the Desktop app is: Interfaces If the compose file has not been changed, then the mlflow client can be opened thought a webbrowser: http://localhost:5000 The management version enables monitoring though a TCP ip. It is currently set on 15100. The starting username and password is always guest. Open though webbrowser: http://localhost:15100 The backend and frontend is also viewable: Backend: http://localhost:8010/docs Frontend: http://localhost:8105 The frontend will show error if no models are found. DO NOT PANIC! It will work after the first training/loading session. If everything opened, then congratulations, you have set up the application with Docker!","title":"Docker Deployment"},{"location":"serve/#containerization-of-observml-docker","text":"This section will tell the reader how to use the ObServML in production. For starters, you can find .dockerfiles (for mlflow, backend) in the repository, which have to be built first to be used. Docker compose will start the ObServML by launching interconnected microservices with the help of docker compose. The example project contains the docker-compose.yaml and the configuration files we require.","title":"Containerization of ObServML -- Docker"},{"location":"serve/#project-template","text":"Pull \"main\" branch to obtain the code to communicate with the backend. You can use the project template's mindeps.txt for minimal dependency: pip install -r mindeps.txt NOTE: Minimal dependencies will make you unable to run locally. It will only allow for scripts to run. This way, the size of the virtual environment is reduced significantly.","title":"Project template"},{"location":"serve/#rabbitmq","text":"This image is relatively easy to get. It should be pulled from this Docker repository. Docker is used in terminal, so either cmd, or a python IDE can accomodate. Walkthrough for: - Linux - Windows - VSCode - Pycharm In this version of ObServML, we tested with rabbitmq:3.12.14-management-alpine, but can be used with any version that is less than 2.0.0. So type into the terminal: docker pull rabbitmq:3.12.14-management-alpine ## pull image_name:image_tag If you use Docker Desktop, and everything went fine, then the image will appear:","title":"RabbitMQ"},{"location":"serve/#building-images","text":"In case of not being able to access to the prebuild Docker images, we also provide a tutorial on creating these images. DOCKER ENGINE IS REQUIRED! First you have to pull the git repository, and then navigate to the \"main\" branch: git pull ... git checkout main NOTE: BUILDING THE IMAGES DO NOT REQUIRE A WORKING VIRTUAL ENVIRONMENT! Building requires the following Docker command: docker build . -t {name}:{tag} -f {filename.dockerfile, preferably in root directory} If you want to build the images locally: docker build . -t mmlw:mlflow -f mlflow.dockerfile docker build . -t mmlw:frontend -f streamlit_frontend.dockerfile docker build . -t mmlw:backend -f service.dockerfile For the first run with build, the backend may take up to 1000s to build this image, which is reduced to ~600s after caching. Please do not modify the branch (DO NOT push to branch) -- open a new branch instead, if you modify the code: git branch [--branch_name] NOTE: The Rabbit image must be pulled for successful start.","title":"Building images"},{"location":"serve/#running-docker-images-with-compose","text":"Docker compose [3] starts the application from images and creates a network, through which the applications can communicate. It requires a docker-compose.yaml file that specifies the behaviour of the stack. The structure is as follows: services: service_A : # service name image : A:latest # image-name:tag ports: - 5100:5672 ## will expose local port # 5672 to global 5100, outside is only reachable from 5100 environment: HUB_URL : \"http://service_B:8010\" ## OS environmental variables # defined in dockerfile ## can refer to service_B IP address as service_B networks: - common_network service_B : image : B:latest ports: - 8010:8010 # CAN be the same networks: - common_network networks: # define existing/nonexisit networks to be used # can also be an outside network... common_network: ## name driver: bridge name: common_network # we recommend this to be the same as the key A docker-compose file is already provided in the GitHub repository (mmlw_template_project branch), and therefore will not be necessary to write out own. For further understanding, please see [5] . Its contents are the following: services: rabbit: image: rabbitmq:3.12.14-management-alpine ports: - 5100:5672 - 15100:15672 networks: - mml_network restart: on-failure mlflow: image: mmlw:mlflow ports: - 5000:5000 networks: - mml_network restart: on-failure backend: image: mmlw:backend ports : - 8010:8010 depends_on: rabbit : condition: service_started mlflow : condition: service_started environment: MLFLOW_URI : \"http://mlflow:5000\" RABBIT_HOST : \"rabbit\" RABBIT_PORT : \"5672\" RABBIT_USER : \"guest\" RABBIT_PASSWORD : \"guest\" restart: on-failure networks: - mml_network frontend: image: mmlw:frontend ports: - 8501:8501 depends_on: backend: condition: service_started environment: HUB_URL : \"http://backend:8010\" restart: on-failure networks: - mml_network networks: mml_network: driver: bridge name: mml_network To run the set of applications, type in: docker compose up What you will see in the Desktop app is:","title":"Running Docker images with compose"},{"location":"serve/#interfaces","text":"If the compose file has not been changed, then the mlflow client can be opened thought a webbrowser: http://localhost:5000 The management version enables monitoring though a TCP ip. It is currently set on 15100. The starting username and password is always guest. Open though webbrowser: http://localhost:15100 The backend and frontend is also viewable: Backend: http://localhost:8010/docs Frontend: http://localhost:8105 The frontend will show error if no models are found. DO NOT PANIC! It will work after the first training/loading session. If everything opened, then congratulations, you have set up the application with Docker!","title":"Interfaces"},{"location":"test_results/","text":"Results for Experiments This section provides the results for specific experiment cases for datasets. In the example version of the projects, several templates are provided: Fault isolation on detect_dataset [digital output signal for circuit, with 3 alterante voltages and currents] -- Decision Tree Fault isolation on hmm data - Hidden Markov Model Time series analysis on time_series_analysis - LSTM Electricity data - Prophet (Multivariate) Fault Detection on Pump data (50 sensors) - PCA Tenesse-Eastman reactor data for fault detection - PCA Process-mining - TopKRules - on operator data of cable head maker machines (specifically machine 27). (cable_head_mach_27) Process-mining - heuristic miner on machine state data (cable head machines) (traces_csoft_oper) The config.yaml looks the following: defaults: ## Fault isolation test - detect : DecisionTree.yaml - hmm : HMM.yaml ## Time series test #- time_series : LSTM.yaml #- electric : Prophet.yaml ## Fault detection test #- pump : pca.yaml #- ten_detect : iforest.yaml ## Process mining test #- oper : HeuristicsMiner.yaml #- process_mining : TopKRules.yaml We encourage you to try them out. Comment in/out any key/values pairs. Please Note that sending requests/data in quick succession may freeze the backend. Please note that there is a timer between sending the request in the script, which can be adjust as seen fit. Fault Isolation Fault isolation experiment is used during the identification of states and specific labels. The most trivial use is to uncover anomalies by providing the \"anomaly\" label. In the case study we use the data from an electric circuit, where digital output is labeled as an anomaly if it is high (1). Inputs are alternating voltage and current, which is often problematic with linear models. Zeroth step is always to ensure that the app is running: docker compose up For training the model, we define a decision tree model. First step is to modify the project template to include only the \"detect\" dataset. config.yaml defaults: - detect : DecisionTree.yaml - hmm : HMM.yaml To do a train request: python train_script_local.py The EDA results is the following for the detect dataset: A set of histograms for the variables: A correlation heatmap, categorical data provides complications, and therefore is omitted at this stage: If we switch for hmm dataset, we will see the following for the variables: Feature importance for decision trees: Prediction result: Time Series Analysis - Multiple models For training the model, we define a LSTM and a prophet model. First step is to modify the project template to include only the \"time_series\" and \"electric\" dataset. config.yaml defaults: - time_series : LSTM.yaml - electric : Prophet.yaml Outliers for time series dataset: Outliers for electric dataset: Prediction for time series dataset requires the following code: if __name__ == \"__main__\": from api_commands import predict import pandas as pd for i in range(8): data = pd.read_excel(f\"./data/time_series_test_{i}.xlsx\") data.reset_index(inplace=True, drop=True) data = data.to_json() predict(url=\"http://localhost:8010\", model_name = \"time_series, data = data, rhost = \"localhost\", rport = \"5100\") Metrics for predictions: Fault Detection - Parameterizing models config.yaml defaults: - pump : pca.yaml - ten_detect : iforest.yaml PCA performance on pump data: 2PC outlier detection for 50 variables: To use test_script.py with the pump dataset, use the following code: if __name__ == \"__main__\": from api_commands import predict import pandas as pd for i in range(8): data = pd.read_excel(f\"./data/pump_test_{i}.xlsx\") data.reset_index(inplace=True, drop=True) data = data.to_json() predict(url=\"http://localhost:8010\", model_name = \"pump\", data = data, rhost = \"localhost\", rport = \"5100\") The outliers figure changes with the addition of new data: This is an exception, each other model will modify the predict figure. Isolation Forest performance on Tenesse Eastman data: the parameters have been set to: params: n_estimators : 100 contamination : \"auto\" random_state : 0 We can check the outliers for flow feed to reactor A: Text Mining For heuristics miner, we get the following results if we use the oper data: Here the resulting network provides information on state transitions between machines. This can be used to find errors in process sequences. We also applied association rules on machining data: Where numbers represent operator IDs: @ITEM=1=ID0998 @ITEM=2=ID4385 @ITEM=3=ID4355 @ITEM=4=ID4163 @ITEM=5=ID4872 @ITEM=6=ID4820 @ITEM=7=ID4445 @ITEM=8=ID4882 @ITEM=9=ID4528 @ITEM=10=ID4493 @ITEM=11=ID0937 @ITEM=12=ID4932 @ITEM=13=ID4429 @ITEM=14=ID4167 @ITEM=15=ID4529 @ITEM=16=ID4641 @ITEM=17=ID0420 @ITEM=18=ID4491 @ITEM=19=ID4132 @ITEM=20=ID4287 @ITEM=21=ID4718 @ITEM=22=ID4442 @ITEM=23=ID3846 @ITEM=24=ID4291 @ITEM=25=ID4794 @ITEM=26=ID0997 NOTE: IT IS NOT GOOD PRACTICE TO USE PREDICT WITH THESE MODELS","title":"Test Results"},{"location":"test_results/#results-for-experiments","text":"This section provides the results for specific experiment cases for datasets. In the example version of the projects, several templates are provided: Fault isolation on detect_dataset [digital output signal for circuit, with 3 alterante voltages and currents] -- Decision Tree Fault isolation on hmm data - Hidden Markov Model Time series analysis on time_series_analysis - LSTM Electricity data - Prophet (Multivariate) Fault Detection on Pump data (50 sensors) - PCA Tenesse-Eastman reactor data for fault detection - PCA Process-mining - TopKRules - on operator data of cable head maker machines (specifically machine 27). (cable_head_mach_27) Process-mining - heuristic miner on machine state data (cable head machines) (traces_csoft_oper) The config.yaml looks the following: defaults: ## Fault isolation test - detect : DecisionTree.yaml - hmm : HMM.yaml ## Time series test #- time_series : LSTM.yaml #- electric : Prophet.yaml ## Fault detection test #- pump : pca.yaml #- ten_detect : iforest.yaml ## Process mining test #- oper : HeuristicsMiner.yaml #- process_mining : TopKRules.yaml We encourage you to try them out. Comment in/out any key/values pairs. Please Note that sending requests/data in quick succession may freeze the backend. Please note that there is a timer between sending the request in the script, which can be adjust as seen fit.","title":"Results for Experiments"},{"location":"test_results/#fault-isolation","text":"Fault isolation experiment is used during the identification of states and specific labels. The most trivial use is to uncover anomalies by providing the \"anomaly\" label. In the case study we use the data from an electric circuit, where digital output is labeled as an anomaly if it is high (1). Inputs are alternating voltage and current, which is often problematic with linear models. Zeroth step is always to ensure that the app is running: docker compose up For training the model, we define a decision tree model. First step is to modify the project template to include only the \"detect\" dataset. config.yaml defaults: - detect : DecisionTree.yaml - hmm : HMM.yaml To do a train request: python train_script_local.py The EDA results is the following for the detect dataset: A set of histograms for the variables: A correlation heatmap, categorical data provides complications, and therefore is omitted at this stage: If we switch for hmm dataset, we will see the following for the variables: Feature importance for decision trees: Prediction result:","title":"Fault Isolation"},{"location":"test_results/#time-series-analysis-multiple-models","text":"For training the model, we define a LSTM and a prophet model. First step is to modify the project template to include only the \"time_series\" and \"electric\" dataset. config.yaml defaults: - time_series : LSTM.yaml - electric : Prophet.yaml Outliers for time series dataset: Outliers for electric dataset: Prediction for time series dataset requires the following code: if __name__ == \"__main__\": from api_commands import predict import pandas as pd for i in range(8): data = pd.read_excel(f\"./data/time_series_test_{i}.xlsx\") data.reset_index(inplace=True, drop=True) data = data.to_json() predict(url=\"http://localhost:8010\", model_name = \"time_series, data = data, rhost = \"localhost\", rport = \"5100\") Metrics for predictions:","title":"Time Series Analysis - Multiple models"},{"location":"test_results/#fault-detection-parameterizing-models","text":"config.yaml defaults: - pump : pca.yaml - ten_detect : iforest.yaml PCA performance on pump data: 2PC outlier detection for 50 variables: To use test_script.py with the pump dataset, use the following code: if __name__ == \"__main__\": from api_commands import predict import pandas as pd for i in range(8): data = pd.read_excel(f\"./data/pump_test_{i}.xlsx\") data.reset_index(inplace=True, drop=True) data = data.to_json() predict(url=\"http://localhost:8010\", model_name = \"pump\", data = data, rhost = \"localhost\", rport = \"5100\") The outliers figure changes with the addition of new data: This is an exception, each other model will modify the predict figure. Isolation Forest performance on Tenesse Eastman data: the parameters have been set to: params: n_estimators : 100 contamination : \"auto\" random_state : 0 We can check the outliers for flow feed to reactor A:","title":"Fault Detection - Parameterizing models"},{"location":"test_results/#text-mining","text":"For heuristics miner, we get the following results if we use the oper data: Here the resulting network provides information on state transitions between machines. This can be used to find errors in process sequences. We also applied association rules on machining data: Where numbers represent operator IDs: @ITEM=1=ID0998 @ITEM=2=ID4385 @ITEM=3=ID4355 @ITEM=4=ID4163 @ITEM=5=ID4872 @ITEM=6=ID4820 @ITEM=7=ID4445 @ITEM=8=ID4882 @ITEM=9=ID4528 @ITEM=10=ID4493 @ITEM=11=ID0937 @ITEM=12=ID4932 @ITEM=13=ID4429 @ITEM=14=ID4167 @ITEM=15=ID4529 @ITEM=16=ID4641 @ITEM=17=ID0420 @ITEM=18=ID4491 @ITEM=19=ID4132 @ITEM=20=ID4287 @ITEM=21=ID4718 @ITEM=22=ID4442 @ITEM=23=ID3846 @ITEM=24=ID4291 @ITEM=25=ID4794 @ITEM=26=ID0997 NOTE: IT IS NOT GOOD PRACTICE TO USE PREDICT WITH THESE MODELS","title":"Text Mining"}]}